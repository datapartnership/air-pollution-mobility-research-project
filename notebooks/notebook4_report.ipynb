{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Report - Notebook 4 \n",
    "\n",
    "## 1 Methodology\n",
    "\n",
    "### 1.1 NO₂ Explanatory Model Architecture Selection\n",
    "\n",
    "Given (1) the large sample size (near million scale), and (2) the non-linearity relationship among features, **machine learning models are appropriate choices for this task.**\n",
    "\n",
    "Among them, **tree-based algorithms** are selected based on the combination of literature insights and empirical suitability , as they offer a good trade-off between **accuracy, interpretability, and computational efficiency.**\n",
    "We restricted the modelling shortlist to two proven ensemble-tree families:\n",
    "\n",
    "| Algorithm          | Architecture | Selection Rationale | Key Advantages |\n",
    "|-----------         |--------------|------------------|-----------------------------|\n",
    "| **Random Forest**  | *Parallel / bagging* &nbsp;→ each tree is trained on a bootstrap sample and votes independently.       | Fast to train, low variance, and effective at capturing non-linear feature interactions with minimal tuning.      | Provides a robust baseline and generates clear SHAP-based explanations for stakeholders. |\n",
    "| **XGBoost**        | *Sequential / boosting* &nbsp;→ each new tree learns the residual errors of the current ensemble.            | Delivers state-of-the-art accuracy on structured data; built-in regularization helps prevent overfitting.         | Achieves lower RMSE; alignment with RF improves interpretability and model trust. |\n",
    "\n",
    "*Note – Deep learning architectures were deliberately excluded at this stage due to time and hardware constraints.*  \n",
    "\n",
    "> This dual-algorithm strategy enables a direct comparison between a **variance-reduction approach** (Random Forest) and a **bias-reduction approach** (XGBoost), allowing us to select the best-performing model for each city.\n",
    "\n",
    "\n",
    "### 1.2 Tree-based Model Fine-tuning\n",
    "\n",
    "Tree-based models consist of multiple decision trees and are well-suited for capturing nonlinear relationships and complex feature interactions. \n",
    "However, their performance is highly sensitive to structural hyperparameters, such as tree depth, learning rate, and the number of estimators. \n",
    "In this project, we focus on fine-tuning these models with the following goals:\n",
    "\n",
    "- Improving generalization\n",
    "\n",
    "- Reducing computational cost\n",
    "\n",
    "- Maintaining interpretability\n",
    "\n",
    "To achieve these objectives, we employed **Randomized Search** as the hyperparameter optimization strategy. \n",
    "Unlike exhaustive methods such as grid search, Randomized Search efficiently explores the hyperparameter space by randomly sampling parameter combinations. \n",
    "This approach is especially useful when computational resources are limited, as it can yield good results more quickly.\n",
    "\n",
    "#### 1.3 Shapley Additive Explanations, SHAP\n",
    "\n",
    "Due to the black-box nature of many machine learning algorithm, it is often difficult to interpret the importance of the input features. \n",
    "**Shapley Additive Explanations, SHAP** provides a principled and theoretically sound way to attribute model output to individual input features, improving model interpretability. \n",
    "It is based on the concept of **Shapley values** from cooperative game theory, which fairly distributes the \"payout\" (i.e., model prediction) among all the \"players\" (i.e., features) that contribute to it.\n",
    "\n",
    "One common way to visualize SHAP values is through a violin-like scatter plot (see figure below). \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/demo-data/Baghdad - SHAP Feature Impact - Random Forest Model.png\" alt=\"SHAP Feature Impact\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <em>SHAP Feature Impact Random Forest Results for Baghdad</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "The interpretation of the above plot components are summarized as the following table:\n",
    "\n",
    "| **Plot element**             | **Meaning**        |\n",
    "| ------------------------- | --------       |\n",
    "| **Dot**                   | One single datapoint, in this case, one grid-cell on one day.                                                                                      |\n",
    "| **X-position**            | How much this feature increases (► right) or decreases (◄ left) the predicted output compared to the average output. |\n",
    "| **Colour**                | The **raw feature value** for that dot (sample): <br> blue = low <br>red/pink = high.                                      |\n",
    "| **Row order**             | Features are sorted from **most to least influential** by their average absolute SHAP value.                     |\n",
    "| **Width of the \"violin\"** | **Spread of impacts**: <br>  wide = feature effect varies a lot across space/time; <br>narrow = stable effect.  \n",
    "\n",
    "## 2 Results\n",
    "### 2.1 Best Model Configuration\n",
    "\n",
    "After optimizing both the Random Forest and XGBoost models, we identified their optimal hyperparameter configurations. The corresponding performance metrics—Root Mean Squared Error (RMSE) and R-squared (R²)—are presented in the table below.\n",
    "\n",
    "**Model for Addis Ababa**\n",
    "\n",
    "| Model Type      | If Scale      | RMSE              | R²        | Best Parameters |\n",
    "|-----------------|----------     |----------------   |-----------|-----------------|\n",
    "| Random Forest   | Unscaled      | 1.84221e-05       | 0.21495   |'n_estimators': 200, 'max_depth': 15, <br> 'max_features': 0.5, 'min_samples_leaf': 4|\n",
    "| XGBoost         | Unscaled      | 1.85828e-05       | 0.20120   |'subsample': 1.0, 'min_child_weight': 3, <br> 'max_depth': 12, 'eta': 0.2, <br> 'colsample_bytree': 1.0|\n",
    "| XGBoost*        | Scale X & y   | 1.84037e-05       | 0.21652   |'subsample': 0.7, 'min_child_weight': 1, <br> 'max_depth': 8, 'eta': 0.01, <br> 'colsample_bytree': 1.0|\n",
    "| XGBoost         | Only Scale X  | 1.85835e-05       | 0.20114   |'subsample': 1.0, 'min_child_weight': 3, <br> 'max_depth': 12, 'eta': 0.2, <br> 'colsample_bytree': 0.7|\n",
    "\n",
    "*Note: the Model with * is the final best model for NO₂ Concentration Explanation*\n",
    "\n",
    "**Model for Baghdad**\n",
    "\n",
    "Due to the large dataset size in Baghdad (over four million samples) and limited computational resources, we reduced the complexity of the hyperparameter search. The resulting model performance is summarized below.\n",
    "\n",
    "| Model Type      | If Scale      | RMSE              | R²        | Best Parameters |\n",
    "|-----------------|----------     |----------------   |-----------|-----------------|\n",
    "| Random Forest   | Unscaled      | 1.32521e-4       | 0.09575   |'n_estimators': 50, 'max_depth': 10, <br> 'max_features': 0.5, 'min_samples_leaf': 500\n",
    "| XGBoost         | Unscaled      | 1.31575e-4       | 0.10857   |'subsample': 0.7, 'min_child_weight': 5, <br> 'max_depth': 12, 'eta': 0.01, <br> 'colsample_bytree': 0.7|\n",
    "| XGBoost*        | Scale X & y   | 1.31435e-4       | 0.11045   |'subsample': 0.7, 'min_child_weight': 5, <br> 'max_depth': 12, 'eta': 0.01, <br> 'colsample_bytree': 0.7|\n",
    "| XGBoost         | Only Scale X  | 1.31583e-4       | 0.10845   |'subsample': 0.7, 'min_child_weight': 5, <br> 'max_depth': 12, 'eta': 0.01, <br> 'colsample_bytree': 0.7|\n",
    "\n",
    "*Note: the Model with * is the final best model for NO₂ Concentration Explanation*\n",
    "\n",
    "### 2.2 NO₂ Concentration Drivers\n",
    "\n",
    "To gain a deeper understanding of the key drivers influencing NO₂ concentration dynamics in the interested region, we performed NO₂ level explanatory analysis using Random Forest (RF) and XGBoost (XGB) models. \n",
    "Model interpretation was conducted via SHAP (SHapley Additive exPlanations) values. \n",
    "\n",
    "The target variable is grid-level NO₂ concentration, with values on the order of 10⁻⁵. \n",
    "For feature preprocessing, the RF model utilized raw input data, while all features in the XGB model were normalized to the [0, 1] range. \n",
    "This decision was based on comparative performance evaluation across different preprocessing strategies, including unscaled inputs, scaling only the features, and scaling both features and the target variable.\n",
    "\n",
    "#### Addis Ababa\n",
    "\n",
    "The SHAP value violin plot of the best two models are shown below.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/demo-data/Addis Ababa - SHAP Feature Impact - Random Forest Model.png\" alt=\"Addis Ababa - SHAP Feature Impact\" width=\"900\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <em>Addis Ababa - SHAP Feature Impact - Random Forest Model</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/demo-data/Addis Ababa - SHAP Feature Impact - XGBoost (Scaled).png\" alt=\"Addis Ababa - SHAP Feature Impact\" width=\"900\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <em>Addis Ababa - SHAP Feature Impact - XGBoost (Scaled)</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "Both models consistently identified the lagged NO₂ concentration in neighboring grids (`no2_neighbor_lag1`) as the most influential predictor, highlighting the significant role of spatial diffusion in pollutant dynamics. \n",
    "Additionally, features reflecting human activity levels - such as `cloud_category`, `pop_sum_m`, and `NTL_mean` - ranked highly in both models, underscoring the non-negligible contribution of anthropogenic factors to air pollution levels.\n",
    "\n",
    "Of particular interest is the strong influence of nighttime light intensity (`NTL_mean`) observed in both models. \n",
    "This variable commonly serves as a proxy for night-time economic activity and population aggregation, capturing composite effects of commercial vibrancy, traffic density, and industrial lighting. \n",
    "SHAP analysis reveals that **areas with higher NTL values (the reddish regions in the SHAP plots) are positively associated with elevated NO₂ concentrations**, indicating that mobile pollution sources such as night-time traffic and industrial emissions may play a critical role in the spatio-temporal distribution of NO₂.\n",
    "\n",
    "When it comes to meteorological factors, the average temperature (`temp_mean`) stands out as an important variable in the XGB model, but it shows little influence in the RF model. \n",
    "This difference may be due to the effect of feature normalization in XGB, which can make small changes in temperature appear more impactful. \n",
    "It may also reflect the more complex and nonlinear role temperature plays in NO₂ behaviour. \n",
    "On the one hand, higher temperatures can boost air circulation and speed up chemical reactions that involve NO₂. \n",
    "On the other hand, in some situations, high temperatures can increase the formation of ground-level ozone, which may reduce NO₂ levels. \n",
    "This kind of two-sided effect might be better captured by the XGB model, which is more sensitive to subtle patterns in the data.\n",
    "\n",
    "In addition, road-related variables closely linked to transportation activity (e.g., `road_residential_len`, `road_len`, `road_primary_len`) demonstrate medium-to-high importance in both models. \n",
    "Road length not only reflects the density of transportation infrastructure but also indirectly indicates the frequency of vehicular movement and emission sources. \n",
    "Particularly in the RF model - where no normalization was applied - road-related features with larger value scales exhibit stronger SHAP responses, suggesting a stable contribution to NO₂ levels.\n",
    "\n",
    "**In summary, both models reveal the multifactorial drivers of NO₂ concentration variability, including spatial lag effects, night-time economic activity, meteorological conditions, and transportation infrastructure.** \n",
    "While the exact rankings of feature importance differ slightly between models, the core influential variables remain consistent. \n",
    "These findings offer actionable insights for urban air pollution mitigation, suggesting that policy efforts should focus on controlling emissions in high-NTL areas, regulating night-time economic activities, and fostering regional coordination in response to spatial diffusion of pollutants.\n",
    "\n",
    "#### Baghdad\n",
    "\n",
    "The SHAP value violin plot of the best two models are shown below.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/demo-data/Baghdad - SHAP Feature Impact - Random Forest Model.png\" alt=\"Addis Ababa - SHAP Feature Impact\" width=\"900\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <em>Baghdad - SHAP Feature Impact - Random Forest Model</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../data/demo-data/Baghdad - SHAP Feature Impact - XGBoost (Scaled).png\" alt=\"Addis Ababa - SHAP Feature Impact\" width=\"900\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <em>Baghdad - SHAP Feature Impact - XGBoost (Scaled)</em>\n",
    "</p>\n",
    "\n",
    "Similarly to Addis Ababa, both models consistently highlight spatial lag effects, confirming the critical role of regional pollutant spillover. \n",
    "Indicators of human activity, such as nighttime light intensity (NTL_mean), also show strong and consistent influence across models, reflecting the contribution of nocturnal economic and transportation activity to urban NO₂ emissions. \n",
    "These shared findings underscore a common underlying structure in both models, where spatial dependence and anthropogenic factors are central to explaining pollutant variation.\n",
    "\n",
    "In the case of Baghdad, **the XGBoost model further emphasizes the role of dynamic environmental factors**. \n",
    "In particular, mean temperature (`temp_mean`) emerges as a highly influential feature, likely due to its role in modulating vertical mixing, atmospheric stability, and the photochemical transformation of NO₂. \n",
    "Elevated temperatures can reduce pollutant dispersion and intensify local accumulation of NO₂, especially under stagnant meteorological conditions. \n",
    "The model also ranks the Traffic Congestion Index (`TCI`) among the top predictors, capturing the real-time impact of mobility bottlenecks on transport emissions. \n",
    "These results suggest that the XGBoost model is especially sensitive to temporally variable features, which aligns with its ability to model complex nonlinear interactions when inputs are normalized.\n",
    "\n",
    "By contrast, **the Random Forest model shows a tendency to prioritize structural and infrastructural features** - such as `road_residential_len` and total road length `road_len` - over meteorological or dynamic urban variables. \n",
    "This is likely influenced by the use of raw feature scales, which may bias the model toward variables with inherently larger numeric scales. \n",
    "As a result, Random Forest may underrepresent the relative impact of high-frequency or small-scale fluctuations (e.g., TCI or temperature), and instead overemphasize more stable, cumulative spatial attributes. \n",
    "Nevertheless, RF still captures the importance of key variables such as `NTL_mean` and `no2_neighbor_lag1`, suggesting broad alignment in the most essential predictors.\n",
    "\n",
    "The divergence in feature prioritization between the two models reveals their complementary strengths: \n",
    "**XGBoost appears better suited for capturing short-term, high-variability drivers of NO₂ (e.g., meteorology and traffic dynamics), while Random Forest offers a more stable representation of long-term or structural determinants (e.g., built environment and infrastructure).**\n",
    "These differences are not only methodological - influenced by model architecture and preprocessing pipelines - but also conceptual, highlighting how different modeling approaches may uncover distinct but meaningful layers of insight in urban air pollution dynamics.\n",
    "\n",
    "**Overall, the results from both models illustrate a multifactorial landscape driving NO₂ variability in Baghdad, shaped by spatial spillover, human mobility, and atmospheric regulation.** \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
