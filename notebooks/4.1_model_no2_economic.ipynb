{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bf8542",
   "metadata": {},
   "source": [
    "# NO2 and economic activity model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9a1c3",
   "metadata": {},
   "source": [
    "## Addis-Ababa Random Forest + SHAP\n",
    "Here we load **all 730** daily meshes for Addis, build lag & neighbor features, train a global RF, and visualize SHAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420065d",
   "metadata": {},
   "source": [
    "#### Imports, constants & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf53fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# bring src/ into path\n",
    "CURR_PATH = Path().resolve()\n",
    "REPO_PATH = CURR_PATH.parent\n",
    "sys.path.append(str(REPO_PATH / \"src\"))\n",
    "\n",
    "# our feature-engineering helpers\n",
    "from feature_engineering import load_mesh_series, make_lag_features, NeighborAggregator\n",
    "\n",
    "# Constants\n",
    "ADDIS_FOLDER = Path(\n",
    "    r\"C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\"\n",
    "    r\"\\air-pollution-mobility-research-project\\data\"\n",
    "    r\"\\Populated meshes\\addis-mesh-data\"\n",
    ")\n",
    "NLAGS   = 7     # cut in half for speed, can tune\n",
    "K_NEIGH = 8     # number of neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae0f85",
   "metadata": {},
   "source": [
    "#### Load & build lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e519a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 399126\n"
     ]
    }
   ],
   "source": [
    "# load all daily meshes\n",
    "gdf = load_mesh_series(ADDIS_FOLDER)\n",
    "print(\"Total rows:\", len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0c12f",
   "metadata": {},
   "source": [
    "### Create autoregressive lags 1…14 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ab0f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after lags: (341292, 11)\n"
     ]
    }
   ],
   "source": [
    "df = make_lag_features(gdf, nlags=NLAGS)\n",
    "df = df.dropna(subset=[f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)])\n",
    "print(\"Shape after lags:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04f16c",
   "metadata": {},
   "source": [
    "#### Vectorised neighbour aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0bd7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape with neighbour feats: (341292, 18)\n"
     ]
    }
   ],
   "source": [
    "# static geometry (center points)\n",
    "static = gdf.drop_duplicates([\"geom_id\"])[[\"geom_id\",\"geometry\"]]\n",
    "\n",
    "# fit neighbour index\n",
    "neighborer = NeighborAggregator(k=K_NEIGH, id_col=\"geom_id\")\n",
    "neighborer.fit(static.reset_index(), None)\n",
    "\n",
    "# 1) unique (geom_id, date) pairs\n",
    "df_center = df[[\"geom_id\",\"date\"]].drop_duplicates()\n",
    "\n",
    "# 2) build edge list (center → neighbour)\n",
    "edges = pd.DataFrame({\n",
    "    \"geom_id\": np.repeat(neighborer.ids_, K_NEIGH),\n",
    "    \"neigh_id\": neighborer.ids_[neighborer.neigh_idx.ravel()]\n",
    "})\n",
    "\n",
    "# 3) cross-join to assign dates to each edge\n",
    "edges_date = df_center.merge(edges, on=\"geom_id\")\n",
    "\n",
    "# 4) join lag columns for each neighbour\n",
    "lag_cols = [f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)]\n",
    "df_lags  = df[[\"geom_id\",\"date\"] + lag_cols]\n",
    "\n",
    "# this merge will create geom_id_x (center) and geom_id_y (neigh)\n",
    "df_nei   = edges_date.merge(\n",
    "    df_lags,\n",
    "    left_on=[\"neigh_id\",\"date\"],\n",
    "    right_on=[\"geom_id\",\"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5) rename & drop so we group by the *center* geom_id\n",
    "df_nei = (\n",
    "    df_nei\n",
    "    .rename(columns={\"geom_id_x\": \"geom_id\"})   # center id\n",
    "    .drop(columns=[\"geom_id_y\", \"neigh_id\"])    # drop the neighbour id & duplicate\n",
    ")\n",
    "\n",
    "# 6) aggregate the neighbour lags\n",
    "neigh_feats = (\n",
    "    df_nei\n",
    "    .groupby([\"geom_id\",\"date\"])[lag_cols]\n",
    "    .mean()\n",
    "    .rename(columns=lambda c: f\"neigh_{c}\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) merge back onto the full df\n",
    "df_full = df.merge(neigh_feats, on=[\"geom_id\",\"date\"], how=\"left\")\n",
    "print(\"Shape with neighbour feats:\", df_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dc791",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2930c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 15\n",
      "float64    14\n",
      "int64       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_full[\"year\"] = df_full[\"date\"].dt.year\n",
    "\n",
    "train = df_full[df_full[\"year\"] < 2024].dropna(subset=[\"no2_mean\"])\n",
    "test  = df_full[df_full[\"year\"] >= 2024].dropna(subset=[\"no2_mean\"])\n",
    "\n",
    "# drop columns not used as features\n",
    "drop_cols = [\"no2_mean\",\"geometry\",\"date\",\"year\"]\n",
    "X_cols = [c for c in train.columns if c not in drop_cols]\n",
    "\n",
    "X_train, y_train = train[X_cols], train[\"no2_mean\"]\n",
    "X_test,  y_test  = test[X_cols],  test[\"no2_mean\"]\n",
    "\n",
    "print(\"Num features:\", len(X_cols))\n",
    "print(X_train.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287dd67",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c98d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R²: 0.72995726580713\n",
      "Test R²: 0.17453496300736593\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        oob_score=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"OOB R²:\", pipeline.named_steps[\"rf\"].oob_score_)\n",
    "print(\"Test R²:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99863934",
   "metadata": {},
   "source": [
    "### Fast SHAP on a sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(pipeline.named_steps[\"rf\"])\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# bar plot\n",
    "shap.plots.bar(explainer, max_display=15)\n",
    "# beeswarm\n",
    "shap.summary_plot(shap_values, X_test, max_display=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347e0aa",
   "metadata": {},
   "source": [
    "### Dependence plot for top-2 features, and mapping one SHAP feature back to space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick background & explain sets\n",
    "X_bg   = X_train.sample(1000, random_state=0)\n",
    "X_expl = X_test.sample(800,  random_state=1)\n",
    "\n",
    "explainer = shap.TreeExplainer(\n",
    "    pipeline.named_steps[\"rf\"],\n",
    "    data=X_bg,\n",
    "    feature_perturbation=\"interventional\"   # fast, path-dependent\n",
    ")\n",
    "\n",
    "shap_vals = explainer.shap_values(X_expl)\n",
    "\n",
    "# global plots\n",
    "shap.plots.bar(shap_vals, max_display=15)\n",
    "shap.plots.beeswarm(shap_vals, max_display=15)\n",
    "\n",
    "# dependence for top-2\n",
    "mean_abs = np.abs(shap_vals).mean(0)\n",
    "top2 = np.array(X_expl.columns)[np.argsort(mean_abs)[-2:]]\n",
    "for feat in top2:\n",
    "    shap.dependence_plot(feat, shap_vals, X_expl)\n",
    "\n",
    "# map mean SHAP of the top feature\n",
    "shap_df = pd.DataFrame(shap_vals, columns=X_expl.columns, index=X_expl.index)\n",
    "shap_df[\"geom_id\"] = test.loc[X_expl.index, \"geom_id\"]\n",
    "mean_shap = shap_df.groupby(\"geom_id\")[top2[-1]].mean().reset_index()\n",
    "\n",
    "map_gdf = static.merge(mean_shap, on=\"geom_id\")\n",
    "map_gdf.plot(column=top2[-1], legend=True, cmap=\"plasma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e986807",
   "metadata": {},
   "source": [
    "### Approximate elasticities from SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute elasticities on the SAME 800‐row subsample we explained\n",
    "X_sub = X_expl.copy()\n",
    "y_pred = pipeline.predict(X_sub)\n",
    "dxs    = X_sub.quantile(0.75) - X_sub.quantile(0.25)\n",
    "\n",
    "# vectorised: \n",
    "#   shap_vals has shape (800, n_features)\n",
    "#   y_pred is (800,)\n",
    "#   X_sub is (800, n_features)\n",
    "rels = (shap_vals / y_pred[:, None]) * (X_sub / dxs[None, :])\n",
    "\n",
    "elasticities = pd.DataFrame({\n",
    "    \"feature\": X_sub.columns,\n",
    "    \"median\": np.median(rels, axis=0),\n",
    "    \"p10\":     np.percentile(rels, 10, axis=0),\n",
    "    \"p90\":     np.percentile(rels, 90, axis=0)\n",
    "})\n",
    "elasticities.sort_values(\"median\", ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
