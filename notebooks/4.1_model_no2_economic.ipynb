{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bf8542",
   "metadata": {},
   "source": [
    "# NO2 and economic activity model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9a1c3",
   "metadata": {},
   "source": [
    "## Addis-Ababa Random Forest + SHAP\n",
    "Here we load **all 730** daily meshes for Addis, build lag & neighbor features, train a global RF, and visualize SHAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420065d",
   "metadata": {},
   "source": [
    "#### Imports, constants & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf53fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# bring src/ into path\n",
    "CURR_PATH = Path().resolve()\n",
    "REPO_PATH = CURR_PATH.parent\n",
    "sys.path.append(str(REPO_PATH / \"src\"))\n",
    "\n",
    "# our feature-engineering helpers\n",
    "from feature_engineering import load_mesh_series, make_lag_features, NeighborAggregator\n",
    "from feature_engineering import (\n",
    "    train_rf_pipeline,\n",
    "    evaluate_model,\n",
    "    explain_shap,\n",
    "    plot_shap_dependence,\n",
    "    compute_elasticities_shap\n",
    ")\n",
    "\n",
    "# define the extra numeric features you want:\n",
    "FEATURE_COLS = [\n",
    "    \"pop_sum_m\", \"NTL_mean\", \"road_len\", \"poi_count\",\n",
    "    \"lu_industrial_area\", \"lu_commercial_area\",\n",
    "    \"lu_residential_area\",\"lu_retail_area\",\n",
    "    \"lu_farmland_area\",  \"lu_farmyard_area\",\n",
    "]\n",
    "\n",
    "# Constants\n",
    "ADDIS_FOLDER = Path(\n",
    "    r\"C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\"\n",
    "    r\"\\air-pollution-mobility-research-project\\data\"\n",
    "    r\"\\Populated meshes\\addis-mesh-data\"\n",
    ")\n",
    "NLAGS   = 7     # cut in half for speed, can tune\n",
    "K_NEIGH = 8     # number of neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae0f85",
   "metadata": {},
   "source": [
    "#### Load & build lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e519a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(records, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 399126\n"
     ]
    }
   ],
   "source": [
    "# load all daily meshes\n",
    "gdf = load_mesh_series(ADDIS_FOLDER, features=FEATURE_COLS)\n",
    "print(\"Total rows:\", len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0c12f",
   "metadata": {},
   "source": [
    "### Create autoregressive lags 1…14 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ab0f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after lags: (341292, 21)\n"
     ]
    }
   ],
   "source": [
    "df = make_lag_features(gdf, nlags=NLAGS)\n",
    "df = df.dropna(subset=[f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)])\n",
    "print(\"Shape after lags:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04f16c",
   "metadata": {},
   "source": [
    "#### Vectorised neighbour aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0bd7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape with neighbour feats: (341292, 28)\n"
     ]
    }
   ],
   "source": [
    "# static geometry (center points)\n",
    "static = gdf.drop_duplicates([\"geom_id\"])[[\"geom_id\",\"geometry\"]]\n",
    "\n",
    "# fit neighbour index\n",
    "neighborer = NeighborAggregator(k=K_NEIGH, id_col=\"geom_id\")\n",
    "neighborer.fit(static.reset_index(), None)\n",
    "\n",
    "# 1) unique (geom_id, date) pairs\n",
    "df_center = df[[\"geom_id\",\"date\"]].drop_duplicates()\n",
    "\n",
    "# 2) build edge list (center → neighbour)\n",
    "edges = pd.DataFrame({\n",
    "    \"geom_id\": np.repeat(neighborer.ids_, K_NEIGH),\n",
    "    \"neigh_id\": neighborer.ids_[neighborer.neigh_idx.ravel()]\n",
    "})\n",
    "\n",
    "# 3) cross-join to assign dates to each edge\n",
    "edges_date = df_center.merge(edges, on=\"geom_id\")\n",
    "\n",
    "# 4) join lag columns for each neighbour\n",
    "lag_cols = [f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)]\n",
    "df_lags  = df[[\"geom_id\",\"date\"] + lag_cols]\n",
    "\n",
    "# this merge will create geom_id_x (center) and geom_id_y (neigh)\n",
    "df_nei   = edges_date.merge(\n",
    "    df_lags,\n",
    "    left_on=[\"neigh_id\",\"date\"],\n",
    "    right_on=[\"geom_id\",\"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5) rename & drop so we group by the *center* geom_id\n",
    "df_nei = (\n",
    "    df_nei\n",
    "    .rename(columns={\"geom_id_x\": \"geom_id\"})   # center id\n",
    "    .drop(columns=[\"geom_id_y\", \"neigh_id\"])    # drop the neighbour id & duplicate\n",
    ")\n",
    "\n",
    "# 6) aggregate the neighbour lags\n",
    "neigh_feats = (\n",
    "    df_nei\n",
    "    .groupby([\"geom_id\",\"date\"])[lag_cols]\n",
    "    .mean()\n",
    "    .rename(columns=lambda c: f\"neigh_{c}\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) merge back onto the full df\n",
    "df_full = df.merge(neigh_feats, on=[\"geom_id\",\"date\"], how=\"left\")\n",
    "print(\"Shape with neighbour feats:\", df_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dc791",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2930c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 171486 rows; evaluating on 163254 rows\n",
      "Num features: 25\n"
     ]
    }
   ],
   "source": [
    "# ─── 1) DEFINE RANDOM-SAMPLING TEST SPLIT ─────────────────────────────────────\n",
    "# all unique dates in the second year\n",
    "all_2024 = (\n",
    "    df_full.loc[df_full[\"date\"].dt.year == 2024, \"date\"]\n",
    "           .dt.normalize()\n",
    "           .unique()\n",
    ")\n",
    "# sample 10% for test\n",
    "rng = np.random.default_rng(42)\n",
    "n_test = max(1, int(len(all_2024) * 0.10))\n",
    "test_dates = rng.choice(all_2024, size=n_test, replace=False)\n",
    "\n",
    "# tag rows\n",
    "df_full[\"is_test\"] = df_full[\"date\"].isin(test_dates)\n",
    "\n",
    "# build actual splits\n",
    "train = (\n",
    "    df_full\n",
    "    .loc[~df_full[\"is_test\"]]\n",
    "    .dropna(subset=[\"no2_mean\"])\n",
    ")\n",
    "test  = (\n",
    "    df_full\n",
    "    .loc[ df_full[\"is_test\"]]\n",
    "    .dropna(subset=[\"no2_mean\"])\n",
    ")\n",
    "\n",
    "print(f\"Train days: {len(df_full.loc[~df_full['is_test'], 'date'].dt.normalize().unique())}\")\n",
    "print(f\" Test days: {len(test_dates)}\")\n",
    "print(f\" Rows → train: {len(train)}, test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"no2_mean\",\"geometry\",\"date\",\"is_test\"]\n",
    "X_cols    = [c for c in train.columns if c not in drop_cols]\n",
    "X_train, y_train = train[X_cols], train[\"no2_mean\"]\n",
    "X_test,  y_test  = test [X_cols],  test [\"no2_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287dd67",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R²:    0.73713995869615\n",
      "Test R²:   0.18441785334598337\n",
      "Test RMSE: 1.9305755239440867e-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 2) define pipeline (no hard hyperparams here)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        oob_score=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 3) lean search space\n",
    "param_dist = {\n",
    "    \"rf__n_estimators\":      [100, 150, 200],\n",
    "    \"rf__max_depth\":         [10, 15, 20],\n",
    "    \"rf__min_samples_leaf\":  [1, 2, 4],\n",
    "    \"rf__max_features\":      [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "print(\"🔍 Best hyper-parameters:\", search.best_params_)\n",
    "\n",
    "# 4) evaluate\n",
    "metrics = evaluate_model(best_model, X_test, y_test)\n",
    "print(\"Test RMSE:\", metrics[\"test_rmse\"])\n",
    "print(\"Test R²:  \", metrics[\"test_r2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99863934",
   "metadata": {},
   "source": [
    "### Fast SHAP on a sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3cb16",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The shap_values argument must be an Explanation object, Cohorts object, or dictionary of Explanation objects!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m X_bg   \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m X_expl \u001b[38;5;241m=\u001b[39m X_test \u001b[38;5;241m.\u001b[39msample( \u001b[38;5;241m800\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m explainer, shap_vals \u001b[38;5;241m=\u001b[39m \u001b[43mexplain_shap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_bg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_expl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py:170\u001b[0m, in \u001b[0;36mexplain_shap\u001b[1;34m(pipeline, X_bg, X_eval, max_display)\u001b[0m\n\u001b[0;32m    168\u001b[0m shap_vals \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_eval)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# summary visuals\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_display\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m shap\u001b[38;5;241m.\u001b[39mplots\u001b[38;5;241m.\u001b[39mbeeswarm(shap_vals, max_display\u001b[38;5;241m=\u001b[39mmax_display)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m explainer, shap_vals\n",
      "File \u001b[1;32mc:\\Users\\Luis.ParraMorales\\AppData\\Local\\miniforge3\\Lib\\site-packages\\shap\\plots\\_bar.py:91\u001b[0m, in \u001b[0;36mbar\u001b[1;34m(shap_values, max_display, order, clustering, clustering_cutoff, show_data, ax, show)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     emsg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shap_values argument must be an Explanation object, Cohorts \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject, or dictionary of Explanation objects!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(emsg)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# unpack our list of Explanation objects we need to plot\u001b[39;00m\n\u001b[0;32m     94\u001b[0m cohort_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cohorts\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mTypeError\u001b[0m: The shap_values argument must be an Explanation object, Cohorts object, or dictionary of Explanation objects!"
     ]
    }
   ],
   "source": [
    "X_bg   = X_train.sample(1000, random_state=0)\n",
    "# evaluate SHAP on your RANDOM-SAMPLED test days\n",
    "explainer, shap_exp = explain_shap(best_model, X_bg, X_test, max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e986807",
   "metadata": {},
   "source": [
    "### Approximate elasticities from SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build full feature matrix (drop objs)\n",
    "full_X = df_full.dropna(subset=[\"no2_mean\"])[X_cols]\n",
    "\n",
    "# get a fresh explainer if you like, or re-use the one above\n",
    "_, full_shap = explain_shap(best_model, X_bg, full_X, max_display=0)  \n",
    "# max_display=0 suppresses extra plots\n",
    "\n",
    "elas_df = compute_elasticities_shap(best_model, full_shap)\n",
    "display(elas_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
