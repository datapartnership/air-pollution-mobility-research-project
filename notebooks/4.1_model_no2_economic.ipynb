{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bf8542",
   "metadata": {},
   "source": [
    "# NO2 and economic activity model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9a1c3",
   "metadata": {},
   "source": [
    "## Addis-Ababa Random Forest + SHAP\n",
    "Here we load **all 730** daily meshes for Addis, build lag & neighbor features, train a global RF, and visualize SHAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420065d",
   "metadata": {},
   "source": [
    "#### Imports, constants & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cf53fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# bring src/ into path\n",
    "CURR_PATH = Path().resolve()\n",
    "REPO_PATH = CURR_PATH.parent\n",
    "sys.path.append(str(REPO_PATH / \"src\"))\n",
    "\n",
    "# our feature-engineering helpers\n",
    "from feature_engineering import load_mesh_series, make_lag_features, NeighborAggregator\n",
    "from feature_engineering import (\n",
    "    train_rf_pipeline,\n",
    "    evaluate_model,\n",
    "    explain_shap,\n",
    "    plot_shap_dependence,\n",
    "    compute_elasticities_shap\n",
    ")\n",
    "\n",
    "# define the extra numeric features you want:\n",
    "FEATURE_COLS = [\n",
    "    \"pop_sum_m\", \"NTL_mean\", \"road_len\", \"poi_count\",\n",
    "    \"lu_industrial_area\", \"lu_commercial_area\",\n",
    "    \"lu_residential_area\",\"lu_retail_area\",\n",
    "    \"lu_farmland_area\",  \"lu_farmyard_area\",\n",
    "]\n",
    "\n",
    "# Constants\n",
    "ADDIS_FOLDER = Path(\n",
    "    r\"C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\"\n",
    "    r\"\\air-pollution-mobility-research-project\\data\"\n",
    "    r\"\\Populated meshes\\addis-mesh-data\"\n",
    ")\n",
    "NLAGS   = 7     # cut in half for speed, can tune\n",
    "K_NEIGH = 8     # number of neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae0f85",
   "metadata": {},
   "source": [
    "#### Load & build lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e519a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(records, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 399126\n"
     ]
    }
   ],
   "source": [
    "# load all daily meshes\n",
    "gdf = load_mesh_series(ADDIS_FOLDER, features=FEATURE_COLS)\n",
    "print(\"Total rows:\", len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0c12f",
   "metadata": {},
   "source": [
    "### Create autoregressive lags 1‚Ä¶14 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69ab0f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after lags: (341292, 21)\n"
     ]
    }
   ],
   "source": [
    "df = make_lag_features(gdf, nlags=NLAGS)\n",
    "df = df.dropna(subset=[f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)])\n",
    "print(\"Shape after lags:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04f16c",
   "metadata": {},
   "source": [
    "#### Vectorised neighbour aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0bd7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape with neighbour feats: (341292, 28)\n"
     ]
    }
   ],
   "source": [
    "# static geometry (center points)\n",
    "static = gdf.drop_duplicates([\"geom_id\"])[[\"geom_id\",\"geometry\"]]\n",
    "\n",
    "# fit neighbour index\n",
    "neighborer = NeighborAggregator(k=K_NEIGH, id_col=\"geom_id\")\n",
    "neighborer.fit(static.reset_index(), None)\n",
    "\n",
    "# 1) unique (geom_id, date) pairs\n",
    "df_center = df[[\"geom_id\",\"date\"]].drop_duplicates()\n",
    "\n",
    "# 2) build edge list (center ‚Üí neighbour)\n",
    "edges = pd.DataFrame({\n",
    "    \"geom_id\": np.repeat(neighborer.ids_, K_NEIGH),\n",
    "    \"neigh_id\": neighborer.ids_[neighborer.neigh_idx.ravel()]\n",
    "})\n",
    "\n",
    "# 3) cross-join to assign dates to each edge\n",
    "edges_date = df_center.merge(edges, on=\"geom_id\")\n",
    "\n",
    "# 4) join lag columns for each neighbour\n",
    "lag_cols = [f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)]\n",
    "df_lags  = df[[\"geom_id\",\"date\"] + lag_cols]\n",
    "\n",
    "# this merge will create geom_id_x (center) and geom_id_y (neigh)\n",
    "df_nei   = edges_date.merge(\n",
    "    df_lags,\n",
    "    left_on=[\"neigh_id\",\"date\"],\n",
    "    right_on=[\"geom_id\",\"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5) rename & drop so we group by the *center* geom_id\n",
    "df_nei = (\n",
    "    df_nei\n",
    "    .rename(columns={\"geom_id_x\": \"geom_id\"})   # center id\n",
    "    .drop(columns=[\"geom_id_y\", \"neigh_id\"])    # drop the neighbour id & duplicate\n",
    ")\n",
    "\n",
    "# 6) aggregate the neighbour lags\n",
    "neigh_feats = (\n",
    "    df_nei\n",
    "    .groupby([\"geom_id\",\"date\"])[lag_cols]\n",
    "    .mean()\n",
    "    .rename(columns=lambda c: f\"neigh_{c}\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) merge back onto the full df\n",
    "df_full = df.merge(neigh_feats, on=[\"geom_id\",\"date\"], how=\"left\")\n",
    "print(\"Shape with neighbour feats:\", df_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dc791",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2930c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train days: 598\n",
      " Test days: 30\n",
      " Rows ‚Üí train: 318906, test: 15834\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 1) DEFINE RANDOM-SAMPLING TEST SPLIT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# all unique dates in the second year\n",
    "all_2024 = (\n",
    "    df_full.loc[df_full[\"date\"].dt.year == 2024, \"date\"]\n",
    "           .dt.normalize()\n",
    "           .unique()\n",
    ")\n",
    "# sample 10% for test\n",
    "rng = np.random.default_rng(42)\n",
    "n_test = max(1, int(len(all_2024) * 0.10))\n",
    "test_dates = rng.choice(all_2024, size=n_test, replace=False)\n",
    "\n",
    "# tag rows\n",
    "df_full[\"is_test\"] = df_full[\"date\"].isin(test_dates)\n",
    "\n",
    "# build actual splits\n",
    "train = (\n",
    "    df_full\n",
    "    .loc[~df_full[\"is_test\"]]\n",
    "    .dropna(subset=[\"no2_mean\"])\n",
    ")\n",
    "test  = (\n",
    "    df_full\n",
    "    .loc[ df_full[\"is_test\"]]\n",
    "    .dropna(subset=[\"no2_mean\"])\n",
    ")\n",
    "\n",
    "print(f\"Train days: {len(df_full.loc[~df_full['is_test'], 'date'].dt.normalize().unique())}\")\n",
    "print(f\" Test days: {len(test_dates)}\")\n",
    "print(f\" Rows ‚Üí train: {len(train)}, test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5a3ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"no2_mean\",\"geometry\",\"date\",\"is_test\"]\n",
    "X_cols    = [c for c in train.columns if c not in drop_cols]\n",
    "X_train, y_train = train[X_cols], train[\"no2_mean\"]\n",
    "X_test,  y_test  = test [X_cols],  test [\"no2_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287dd67",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "üîç Best hyper-parameters: {'rf__n_estimators': 150, 'rf__min_samples_split': 2, 'rf__min_samples_leaf': 1, 'rf__max_features': 'sqrt', 'rf__max_depth': 20}\n",
      "Test RMSE: 1.585629573518724e-05\n",
      "Test R¬≤:   0.37039859808189457\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 2) define pipeline (no hard hyperparams here)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    #(\"scaler\", StandardScaler()),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        oob_score=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 3) lean search space\n",
    "param_dist = {\n",
    "    \"rf__n_estimators\":      [100, 150],\n",
    "    \"rf__max_depth\":         [10, 15],\n",
    "    \"rf__min_samples_leaf\":  [1, 2, 4],\n",
    "    \"rf__max_features\":      [\"sqrt\", \"log2\"],\n",
    "    \"rf__min_samples_split\": [2, 5, 10],      \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "print(\"üîç Best hyper-parameters:\", search.best_params_)\n",
    "\n",
    "# 4) evaluate\n",
    "metrics = evaluate_model(best_model, X_test, y_test)\n",
    "print(\"Test RMSE:\", metrics[\"test_rmse\"])\n",
    "print(\"Test R¬≤:  \", metrics[\"test_r2\"])\n",
    "\n",
    "import gc, joblib\n",
    "# free up ~2‚Äì4 GB kept by the 60 fitted estimators inside the CV object\n",
    "del search\n",
    "joblib.parallel.get_active_backend().abort_everything()  # safety: kill joblib workers\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99863934",
   "metadata": {},
   "source": [
    "### Fast SHAP on a sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bg = X_train.sample(min(1000, len(X_train)), random_state=0)\n",
    "# evaluate SHAP on your RANDOM-SAMPLED test days\n",
    "explainer, shap_exp = explain_shap(best_model, X_bg, X_test, max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e986807",
   "metadata": {},
   "source": [
    "### Approximate elasticities from SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build full feature matrix (drop objs)\n",
    "full_X = df_full.dropna(subset=[\"no2_mean\"])[X_cols]\n",
    "\n",
    "# get a fresh explainer if you like, or re-use the one above\n",
    "_, full_shap = explain_shap(best_model, X_bg, full_X, max_display=0)  \n",
    "# max_display=0 suppresses extra plots\n",
    "\n",
    "elas_df = compute_elasticities_shap(best_model, full_shap)\n",
    "display(elas_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
