{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bf8542",
   "metadata": {},
   "source": [
    "# NO2 and economic activity model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9a1c3",
   "metadata": {},
   "source": [
    "## Addis-Ababa Random Forest + SHAP\n",
    "Here we load **all 730** daily meshes for Addis, build lag & neighbor features, train a global RF, and visualize SHAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420065d",
   "metadata": {},
   "source": [
    "#### Imports, constants & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf53fa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'train_rf_pipeline' from 'feature_engineering' (C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# our feature-engineering helpers\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_engineering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_mesh_series, make_lag_features, NeighborAggregator\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_engineering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     train_rf_pipeline,\n\u001b[0;32m     22\u001b[0m     evaluate_model,\n\u001b[0;32m     23\u001b[0m     explain_shap,\n\u001b[0;32m     24\u001b[0m     plot_shap_dependence,\n\u001b[0;32m     25\u001b[0m     compute_elasticities_shap\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m FEATURE_COLS \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpop_sum_m\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNTL_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlu_farmyard_area\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     39\u001b[0m ]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Constants\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'train_rf_pipeline' from 'feature_engineering' (C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# bring src/ into path\n",
    "CURR_PATH = Path().resolve()\n",
    "REPO_PATH = CURR_PATH.parent\n",
    "sys.path.append(str(REPO_PATH / \"src\"))\n",
    "\n",
    "# our feature-engineering helpers\n",
    "from feature_engineering import load_mesh_series, make_lag_features, NeighborAggregator\n",
    "from feature_engineering import (\n",
    "    train_rf_pipeline,\n",
    "    evaluate_model,\n",
    "    explain_shap,\n",
    "    plot_shap_dependence,\n",
    "    compute_elasticities_shap\n",
    ")\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"pop_sum_m\",\n",
    "    \"NTL_mean\",\n",
    "    \"road_len\",\n",
    "    \"poi_count\",\n",
    "    \"lu_industrial_area\",\n",
    "    \"lu_commercial_area\",\n",
    "    \"lu_residential_area\",\n",
    "    \"lu_retail_area\",\n",
    "    \"lu_farmland_area\",\n",
    "    \"lu_farmyard_area\",\n",
    "]\n",
    "\n",
    "# Constants\n",
    "ADDIS_FOLDER = Path(\n",
    "    r\"C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\"\n",
    "    r\"\\air-pollution-mobility-research-project\\data\"\n",
    "    r\"\\Populated meshes\\addis-mesh-data\"\n",
    ")\n",
    "NLAGS   = 7     # cut in half for speed, can tune\n",
    "K_NEIGH = 8     # number of neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae0f85",
   "metadata": {},
   "source": [
    "#### Load & build lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e519a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 399126\n"
     ]
    }
   ],
   "source": [
    "# load all daily meshes\n",
    "gdf = load_mesh_series(ADDIS_FOLDER, features=FEATURE_COLS)\n",
    "print(\"Total rows:\", len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0c12f",
   "metadata": {},
   "source": [
    "### Create autoregressive lags 1…14 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ab0f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after lags: (341292, 21)\n"
     ]
    }
   ],
   "source": [
    "df = make_lag_features(gdf, nlags=NLAGS)\n",
    "df = df.dropna(subset=[f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)])\n",
    "print(\"Shape after lags:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04f16c",
   "metadata": {},
   "source": [
    "#### Vectorised neighbour aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0bd7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape with neighbour feats: (341292, 28)\n"
     ]
    }
   ],
   "source": [
    "# static geometry (center points)\n",
    "static = gdf.drop_duplicates([\"geom_id\"])[[\"geom_id\",\"geometry\"]]\n",
    "\n",
    "# fit neighbour index\n",
    "neighborer = NeighborAggregator(k=K_NEIGH, id_col=\"geom_id\")\n",
    "neighborer.fit(static.reset_index(), None)\n",
    "\n",
    "# 1) unique (geom_id, date) pairs\n",
    "df_center = df[[\"geom_id\",\"date\"]].drop_duplicates()\n",
    "\n",
    "# 2) build edge list (center → neighbour)\n",
    "edges = pd.DataFrame({\n",
    "    \"geom_id\": np.repeat(neighborer.ids_, K_NEIGH),\n",
    "    \"neigh_id\": neighborer.ids_[neighborer.neigh_idx.ravel()]\n",
    "})\n",
    "\n",
    "# 3) cross-join to assign dates to each edge\n",
    "edges_date = df_center.merge(edges, on=\"geom_id\")\n",
    "\n",
    "# 4) join lag columns for each neighbour\n",
    "lag_cols = [f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)]\n",
    "df_lags  = df[[\"geom_id\",\"date\"] + lag_cols]\n",
    "\n",
    "# this merge will create geom_id_x (center) and geom_id_y (neigh)\n",
    "df_nei   = edges_date.merge(\n",
    "    df_lags,\n",
    "    left_on=[\"neigh_id\",\"date\"],\n",
    "    right_on=[\"geom_id\",\"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5) rename & drop so we group by the *center* geom_id\n",
    "df_nei = (\n",
    "    df_nei\n",
    "    .rename(columns={\"geom_id_x\": \"geom_id\"})   # center id\n",
    "    .drop(columns=[\"geom_id_y\", \"neigh_id\"])    # drop the neighbour id & duplicate\n",
    ")\n",
    "\n",
    "# 6) aggregate the neighbour lags\n",
    "neigh_feats = (\n",
    "    df_nei\n",
    "    .groupby([\"geom_id\",\"date\"])[lag_cols]\n",
    "    .mean()\n",
    "    .rename(columns=lambda c: f\"neigh_{c}\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) merge back onto the full df\n",
    "df_full = df.merge(neigh_feats, on=[\"geom_id\",\"date\"], how=\"left\")\n",
    "print(\"Shape with neighbour feats:\", df_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dc791",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2930c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 15\n",
      "float64    14\n",
      "int64       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_full[\"year\"] = df_full[\"date\"].dt.year\n",
    "\n",
    "train = df_full[df_full[\"year\"] < 2024].dropna(subset=[\"no2_mean\"])\n",
    "test  = df_full[df_full[\"year\"] >= 2024].dropna(subset=[\"no2_mean\"])\n",
    "\n",
    "# drop columns not used as features\n",
    "drop_cols = [\"no2_mean\",\"geometry\",\"date\",\"year\"]\n",
    "X_cols = [c for c in train.columns if c not in drop_cols]\n",
    "\n",
    "X_train, y_train = train[X_cols], train[\"no2_mean\"]\n",
    "X_test,  y_test  = test[X_cols],  test[\"no2_mean\"]\n",
    "\n",
    "print(\"Num features:\", len(X_cols))\n",
    "print(X_train.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287dd67",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c98d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R²: 0.72995726580713\n",
      "Test R²: 0.17453496300736593\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        oob_score=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"OOB R²:\", pipeline.named_steps[\"rf\"].oob_score_)\n",
    "print(\"Test R²:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99863934",
   "metadata": {},
   "source": [
    "### Fast SHAP on a sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(pipeline.named_steps[\"rf\"])\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# bar plot\n",
    "shap.plots.bar(explainer, max_display=15)\n",
    "# beeswarm\n",
    "shap.summary_plot(shap_values, X_test, max_display=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347e0aa",
   "metadata": {},
   "source": [
    "### Dependence plot for top-2 features, and mapping one SHAP feature back to space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick background & explain sets\n",
    "X_bg   = X_train.sample(1000, random_state=0)\n",
    "X_expl = X_test.sample(800,  random_state=1)\n",
    "\n",
    "explainer = shap.TreeExplainer(\n",
    "    pipeline.named_steps[\"rf\"],\n",
    "    data=X_bg,\n",
    "    feature_perturbation=\"interventional\"   # fast, path-dependent\n",
    ")\n",
    "\n",
    "shap_vals = explainer.shap_values(X_expl)\n",
    "\n",
    "# global plots\n",
    "shap.plots.bar(shap_vals, max_display=15)\n",
    "shap.plots.beeswarm(shap_vals, max_display=15)\n",
    "\n",
    "# dependence for top-2\n",
    "mean_abs = np.abs(shap_vals).mean(0)\n",
    "top2 = np.array(X_expl.columns)[np.argsort(mean_abs)[-2:]]\n",
    "for feat in top2:\n",
    "    shap.dependence_plot(feat, shap_vals, X_expl)\n",
    "\n",
    "# map mean SHAP of the top feature\n",
    "shap_df = pd.DataFrame(shap_vals, columns=X_expl.columns, index=X_expl.index)\n",
    "shap_df[\"geom_id\"] = test.loc[X_expl.index, \"geom_id\"]\n",
    "mean_shap = shap_df.groupby(\"geom_id\")[top2[-1]].mean().reset_index()\n",
    "\n",
    "map_gdf = static.merge(mean_shap, on=\"geom_id\")\n",
    "map_gdf.plot(column=top2[-1], legend=True, cmap=\"plasma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e986807",
   "metadata": {},
   "source": [
    "### Approximate elasticities from SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute elasticities on the SAME 800‐row subsample we explained\n",
    "X_sub = X_expl.copy()\n",
    "y_pred = pipeline.predict(X_sub)\n",
    "dxs    = X_sub.quantile(0.75) - X_sub.quantile(0.25)\n",
    "\n",
    "# vectorised: \n",
    "#   shap_vals has shape (800, n_features)\n",
    "#   y_pred is (800,)\n",
    "#   X_sub is (800, n_features)\n",
    "rels = (shap_vals / y_pred[:, None]) * (X_sub / dxs[None, :])\n",
    "\n",
    "elasticities = pd.DataFrame({\n",
    "    \"feature\": X_sub.columns,\n",
    "    \"median\": np.median(rels, axis=0),\n",
    "    \"p10\":     np.percentile(rels, 10, axis=0),\n",
    "    \"p90\":     np.percentile(rels, 90, axis=0)\n",
    "})\n",
    "elasticities.sort_values(\"median\", ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
