{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bf8542",
   "metadata": {},
   "source": [
    "# NO2 and economic activity model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9a1c3",
   "metadata": {},
   "source": [
    "## Addis-Ababa Random Forest + SHAP\n",
    "Here we load **all 730** daily meshes for Addis, build lag & neighbor features, train a global RF, and visualize SHAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420065d",
   "metadata": {},
   "source": [
    "#### Imports, constants & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf53fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# bring src/ into path\n",
    "CURR_PATH = Path().resolve()\n",
    "REPO_PATH = CURR_PATH.parent\n",
    "sys.path.append(str(REPO_PATH / \"src\"))\n",
    "\n",
    "# our feature-engineering helpers\n",
    "from feature_engineering import load_mesh_series, make_lag_features, NeighborAggregator\n",
    "from feature_engineering import (\n",
    "    train_rf_pipeline,\n",
    "    evaluate_model,\n",
    "    explain_shap,\n",
    "    plot_shap_dependence,\n",
    "    compute_elasticities_shap\n",
    ")\n",
    "\n",
    "# define the extra numeric features you want:\n",
    "FEATURE_COLS = [\n",
    "    \"pop_sum_m\", \"NTL_mean\", \"road_len\", \"poi_count\",\n",
    "    \"lu_industrial_area\", \"lu_commercial_area\",\n",
    "    \"lu_residential_area\",\"lu_retail_area\",\n",
    "    \"lu_farmland_area\",  \"lu_farmyard_area\",\n",
    "]\n",
    "\n",
    "# Constants\n",
    "ADDIS_FOLDER = Path(\n",
    "    r\"C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\"\n",
    "    r\"\\air-pollution-mobility-research-project\\data\"\n",
    "    r\"\\Populated meshes\\addis-mesh-data\"\n",
    ")\n",
    "NLAGS   = 7     # cut in half for speed, can tune\n",
    "K_NEIGH = 8     # number of neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae0f85",
   "metadata": {},
   "source": [
    "#### Load & build lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e519a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis.ParraMorales\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(records, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 399126\n"
     ]
    }
   ],
   "source": [
    "# load all daily meshes\n",
    "gdf = load_mesh_series(ADDIS_FOLDER, features=FEATURE_COLS)\n",
    "print(\"Total rows:\", len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0c12f",
   "metadata": {},
   "source": [
    "### Create autoregressive lags 1…14 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ab0f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after lags: (341292, 21)\n"
     ]
    }
   ],
   "source": [
    "df = make_lag_features(gdf, nlags=NLAGS)\n",
    "df = df.dropna(subset=[f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)])\n",
    "print(\"Shape after lags:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04f16c",
   "metadata": {},
   "source": [
    "#### Vectorised neighbour aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0bd7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape with neighbour feats: (341292, 28)\n"
     ]
    }
   ],
   "source": [
    "# static geometry (center points)\n",
    "static = gdf.drop_duplicates([\"geom_id\"])[[\"geom_id\",\"geometry\"]]\n",
    "\n",
    "# fit neighbour index\n",
    "neighborer = NeighborAggregator(k=K_NEIGH, id_col=\"geom_id\")\n",
    "neighborer.fit(static.reset_index(), None)\n",
    "\n",
    "# 1) unique (geom_id, date) pairs\n",
    "df_center = df[[\"geom_id\",\"date\"]].drop_duplicates()\n",
    "\n",
    "# 2) build edge list (center → neighbour)\n",
    "edges = pd.DataFrame({\n",
    "    \"geom_id\": np.repeat(neighborer.ids_, K_NEIGH),\n",
    "    \"neigh_id\": neighborer.ids_[neighborer.neigh_idx.ravel()]\n",
    "})\n",
    "\n",
    "# 3) cross-join to assign dates to each edge\n",
    "edges_date = df_center.merge(edges, on=\"geom_id\")\n",
    "\n",
    "# 4) join lag columns for each neighbour\n",
    "lag_cols = [f\"no2_mean_lag{i}\" for i in range(1, NLAGS+1)]\n",
    "df_lags  = df[[\"geom_id\",\"date\"] + lag_cols]\n",
    "\n",
    "# this merge will create geom_id_x (center) and geom_id_y (neigh)\n",
    "df_nei   = edges_date.merge(\n",
    "    df_lags,\n",
    "    left_on=[\"neigh_id\",\"date\"],\n",
    "    right_on=[\"geom_id\",\"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5) rename & drop so we group by the *center* geom_id\n",
    "df_nei = (\n",
    "    df_nei\n",
    "    .rename(columns={\"geom_id_x\": \"geom_id\"})   # center id\n",
    "    .drop(columns=[\"geom_id_y\", \"neigh_id\"])    # drop the neighbour id & duplicate\n",
    ")\n",
    "\n",
    "# 6) aggregate the neighbour lags\n",
    "neigh_feats = (\n",
    "    df_nei\n",
    "    .groupby([\"geom_id\",\"date\"])[lag_cols]\n",
    "    .mean()\n",
    "    .rename(columns=lambda c: f\"neigh_{c}\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) merge back onto the full df\n",
    "df_full = df.merge(neigh_feats, on=[\"geom_id\",\"date\"], how=\"left\")\n",
    "print(\"Shape with neighbour feats:\", df_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dc791",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2930c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 171486 rows; evaluating on 163254 rows\n",
      "Num features: 25\n"
     ]
    }
   ],
   "source": [
    "# ─── Train/Test Split ───────────────────────────────────────────────────────\n",
    "df_full[\"year\"] = df_full[\"date\"].dt.year\n",
    "\n",
    "train = df_full[df_full[\"year\"] < 2024].dropna(subset=[\"no2_mean\"])\n",
    "test  = df_full[df_full[\"year\"] >= 2024].dropna(subset=[\"no2_mean\"])\n",
    "\n",
    "drop_cols = [\"no2_mean\",\"geometry\",\"date\",\"year\"]\n",
    "X_cols    = [c for c in train.columns if c not in drop_cols]\n",
    "\n",
    "X_train, y_train = train[X_cols], train[\"no2_mean\"]\n",
    "X_test,  y_test  = test [X_cols],  test [\"no2_mean\"]\n",
    "\n",
    "print(f\"Training on {len(X_train)} rows; evaluating on {len(X_test)} rows\")\n",
    "print(\"Num features:\", len(X_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287dd67",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c98d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R²:    0.73713995869615\n",
      "Test R²:   0.18441785334598337\n",
      "Test RMSE: 1.9305755239440867e-05\n"
     ]
    }
   ],
   "source": [
    "pipeline = train_rf_pipeline(X_train, y_train)\n",
    "metrics  = evaluate_model(pipeline, X_test, y_test)\n",
    "\n",
    "print(\"OOB R²:   \", metrics[\"oob_r2\"])\n",
    "print(\"Test R²:  \", metrics[\"test_r2\"])\n",
    "print(\"Test RMSE:\", metrics[\"test_rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99863934",
   "metadata": {},
   "source": [
    "### Fast SHAP on a sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d3cb16",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The shap_values argument must be an Explanation object, Cohorts object, or dictionary of Explanation objects!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m X_bg   \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m X_expl \u001b[38;5;241m=\u001b[39m X_test \u001b[38;5;241m.\u001b[39msample( \u001b[38;5;241m800\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m explainer, shap_vals \u001b[38;5;241m=\u001b[39m \u001b[43mexplain_shap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_bg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_expl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AirPollution_Analysis\\air-pollution-mobility-research-project\\src\\feature_engineering.py:170\u001b[0m, in \u001b[0;36mexplain_shap\u001b[1;34m(pipeline, X_bg, X_eval, max_display)\u001b[0m\n\u001b[0;32m    168\u001b[0m shap_vals \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_eval)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# summary visuals\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_display\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m shap\u001b[38;5;241m.\u001b[39mplots\u001b[38;5;241m.\u001b[39mbeeswarm(shap_vals, max_display\u001b[38;5;241m=\u001b[39mmax_display)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m explainer, shap_vals\n",
      "File \u001b[1;32mc:\\Users\\Luis.ParraMorales\\AppData\\Local\\miniforge3\\Lib\\site-packages\\shap\\plots\\_bar.py:91\u001b[0m, in \u001b[0;36mbar\u001b[1;34m(shap_values, max_display, order, clustering, clustering_cutoff, show_data, ax, show)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     emsg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shap_values argument must be an Explanation object, Cohorts \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject, or dictionary of Explanation objects!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(emsg)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# unpack our list of Explanation objects we need to plot\u001b[39;00m\n\u001b[0;32m     94\u001b[0m cohort_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cohorts\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mTypeError\u001b[0m: The shap_values argument must be an Explanation object, Cohorts object, or dictionary of Explanation objects!"
     ]
    }
   ],
   "source": [
    "X_bg   = X_train.sample(1000, random_state=0)\n",
    "X_expl = X_test .sample( 800, random_state=1)\n",
    "\n",
    "explainer, shap_vals = explain_shap(pipeline, X_bg, X_expl, max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347e0aa",
   "metadata": {},
   "source": [
    "### Dependence plot for top-2 features, and mapping one SHAP feature back to space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Dependence Plots for Top-2 Features ─────────────────────────────────────\n",
    "plot_shap_dependence(explainer, shap_vals, X_expl, top_k=2)\n",
    "\n",
    "# ─── Map Mean SHAP of Primary Driver ────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pick the single top feature\n",
    "mean_abs = np.abs(shap_vals).mean(0)\n",
    "feat_idx = np.argmax(mean_abs)\n",
    "top_feat = X_expl.columns[feat_idx]\n",
    "\n",
    "shap_df   = pd.DataFrame(shap_vals, columns=X_expl.columns, index=X_expl.index)\n",
    "shap_df[\"geom_id\"] = test.loc[X_expl.index, \"geom_id\"]\n",
    "mean_shap = shap_df.groupby(\"geom_id\")[top_feat].mean().reset_index()\n",
    "\n",
    "map_gdf = static.merge(mean_shap, on=\"geom_id\")\n",
    "map_gdf.plot(column=top_feat, legend=True, cmap=\"plasma\")\n",
    "plt.title(f\"Mean SHAP: {top_feat}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e986807",
   "metadata": {},
   "source": [
    "### Approximate elasticities from SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Approximate Elasticities from SHAP ──────────────────────────────────────\n",
    "elas_df = compute_elasticities_shap(pipeline, X_expl, shap_vals)\n",
    "display(elas_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
