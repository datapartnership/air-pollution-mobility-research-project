{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel the comment to install all the packages and libraries needed.\n",
    "# ! pip install rasterio matplotlib rasterstats ipynbname imageio tqdm rasterstats\n",
    "# ! pip install numpy==1.24.4CURR_PATH\n",
    "# ! pip install libpysal\n",
    "# ! pip install geopandas libpysal esda matplotlib\n",
    "# ! pip install seaborn\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "\n",
    "# Configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "CURR_PATH = Path().resolve()            # current file path\n",
    "REPO_PATH = CURR_PATH.parent            # current repository path\n",
    "DATA_PATH = REPO_PATH / \"data\"          # path for saving the data\n",
    "DEMO_PATH = DATA_PATH / \"demo-data\"     # path for demo purpose \n",
    "\n",
    "SRC_PATH = REPO_PATH / \"src\"    # path for other sources\n",
    "sys.path.append(str(SRC_PATH))  # add src to system path to import custom functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Preparation: Temporal & Spatial Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initial data resolution:\n",
    "    - Time: daily\n",
    "    - Space: cell (mesh grid)\n",
    "\n",
    "- Total four types of data with different aggregation level:\n",
    "    1. Cell level & Daily Data (original parquet file)\n",
    "    2. Cell level & Monthly Data\n",
    "    3. Sub-region level & Daily Data\n",
    "    4. Sub-region level & Monthly Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addis Ababa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Read the Addis Ababa data\n",
    "full_df = pd.read_parquet(DATA_PATH / \"temp\" / \"full_addis_df.parquet\", engine=\"pyarrow\")\n",
    "full_df['month'] = full_df['date'].dt.to_period('M')\n",
    "\n",
    "# Original dataset: daily / cell resolution\n",
    "daily_cell_ori = full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geom_id', 'no2_mean', 'pop_sum_m', 'NTL_mean', 'road_len',\n",
       "       'road_share', 'poi_count', 'poi_share', 'lu_industrial_area',\n",
       "       'lu_industrial_share', 'lu_commercial_area', 'lu_commercial_share',\n",
       "       'lu_residential_area', 'lu_residential_share', 'lu_retail_area',\n",
       "       'lu_retail_share', 'lu_farmland_area', 'lu_farmland_share',\n",
       "       'lu_farmyard_area', 'lu_farmyard_share', 'road_motorway_len',\n",
       "       'road_trunk_len', 'road_primary_len', 'road_secondary_len',\n",
       "       'road_tertiary_len', 'road_residential_len', 'fossil_pp_count',\n",
       "       'geometry_x', 'date', 'no2_lag1', 'no2_neighbor_lag1', 'cloud_category',\n",
       "       'LST_day_mean', 'landcover_2023', 'Shape_Leng', 'Shape_Area', 'ADM3_EN',\n",
       "       'ADM3_PCODE', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(full_df)\n",
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geom_id</th>\n",
       "      <th>no2_mean</th>\n",
       "      <th>pop_sum_m</th>\n",
       "      <th>NTL_mean</th>\n",
       "      <th>road_len</th>\n",
       "      <th>road_share</th>\n",
       "      <th>poi_count</th>\n",
       "      <th>poi_share</th>\n",
       "      <th>lu_industrial_area</th>\n",
       "      <th>lu_industrial_share</th>\n",
       "      <th>...</th>\n",
       "      <th>no2_lag1</th>\n",
       "      <th>no2_neighbor_lag1</th>\n",
       "      <th>cloud_category</th>\n",
       "      <th>LST_day_mean</th>\n",
       "      <th>landcover_2023</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>ADM3_EN</th>\n",
       "      <th>ADM3_PCODE</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>969.68396</td>\n",
       "      <td>7.266073</td>\n",
       "      <td>5860.59401</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.538101</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>Akaki Kality</td>\n",
       "      <td>ET140101</td>\n",
       "      <td>2023-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>969.68396</td>\n",
       "      <td>10.372897</td>\n",
       "      <td>5860.59401</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.130</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.538101</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>Akaki Kality</td>\n",
       "      <td>ET140101</td>\n",
       "      <td>2023-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>969.68396</td>\n",
       "      <td>1.124154</td>\n",
       "      <td>5860.59401</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.588</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.538101</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>Akaki Kality</td>\n",
       "      <td>ET140101</td>\n",
       "      <td>2023-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>969.68396</td>\n",
       "      <td>0.727840</td>\n",
       "      <td>5860.59401</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.670</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.538101</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>Akaki Kality</td>\n",
       "      <td>ET140101</td>\n",
       "      <td>2023-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>969.68396</td>\n",
       "      <td>3.964316</td>\n",
       "      <td>5860.59401</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.990</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.538101</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>Akaki Kality</td>\n",
       "      <td>ET140101</td>\n",
       "      <td>2023-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   geom_id  no2_mean  pop_sum_m   NTL_mean    road_len  road_share  poi_count  \\\n",
       "0        0  0.000051  969.68396   7.266073  5860.59401    0.000745          0   \n",
       "1        0  0.000050  969.68396  10.372897  5860.59401    0.000745          0   \n",
       "2        0  0.000047  969.68396   1.124154  5860.59401    0.000745          0   \n",
       "3        0  0.000047  969.68396   0.727840  5860.59401    0.000745          0   \n",
       "4        0  0.000058  969.68396   3.964316  5860.59401    0.000745          0   \n",
       "\n",
       "   poi_share  lu_industrial_area  lu_industrial_share  ...  no2_lag1  \\\n",
       "0        0.0                 0.0                  0.0  ...       NaN   \n",
       "1        0.0                 0.0                  0.0  ...  0.000051   \n",
       "2        0.0                 0.0                  0.0  ...  0.000050   \n",
       "3        0.0                 0.0                  0.0  ...  0.000047   \n",
       "4        0.0                 0.0                  0.0  ...  0.000047   \n",
       "\n",
       "   no2_neighbor_lag1  cloud_category  LST_day_mean  landcover_2023  \\\n",
       "0                NaN             0.0           NaN            12.0   \n",
       "1           0.000038             0.0        25.130            12.0   \n",
       "2           0.000052             0.0        15.588            12.0   \n",
       "3           0.000050             0.0        30.670            12.0   \n",
       "4           0.000044             0.0        30.990            12.0   \n",
       "\n",
       "   Shape_Leng  Shape_Area       ADM3_EN  ADM3_PCODE    month  \n",
       "0    0.538101    0.010351  Akaki Kality    ET140101  2023-01  \n",
       "1    0.538101    0.010351  Akaki Kality    ET140101  2023-01  \n",
       "2    0.538101    0.010351  Akaki Kality    ET140101  2023-01  \n",
       "3    0.538101    0.010351  Akaki Kality    ET140101  2023-01  \n",
       "4    0.538101    0.010351  Akaki Kality    ET140101  2023-01  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate the cell level, monthly data and save as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features that need mean when aggregate from day to month level\n",
    "time_agg_mean_feature = [\n",
    "        'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "        'NTL_mean',\n",
    "        # 'cloud_category', \n",
    "        'LST_day_mean', \n",
    "        # 'landcover_2023',\n",
    "        'pop_sum_m',  \n",
    "        'road_len', \n",
    "        'poi_count', 'lu_industrial_area',\n",
    "        'lu_commercial_area',  'lu_residential_area', 'lu_retail_area', 'lu_farmland_area', \n",
    "        'lu_farmyard_area', \n",
    "        'road_primary_len',\n",
    "        'road_motorway_len', 'road_trunk_len',  'road_secondary_len', 'road_tertiary_len', 'road_residential_len',\n",
    "         \n",
    "]\n",
    "\n",
    "monthly_cell = daily_cell_ori.groupby(['geom_id', 'month', 'ADM3_EN'])[time_agg_mean_feature].mean().reset_index()\n",
    "monthly_cell.to_csv(DATA_PATH / \"temp\" / 'addis_monthly_cell.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_cell.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, generate the sub-district level, daily data and save as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_agg_sum_feature = [\n",
    "        'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "        'NTL_mean',\n",
    "        'pop_sum_m',  \n",
    "        'road_len', \n",
    "        'poi_count', 'lu_industrial_area',\n",
    "        'lu_commercial_area',  'lu_residential_area', 'lu_retail_area', 'lu_farmland_area', \n",
    "        'lu_farmyard_area', \n",
    "        'road_primary_len',\n",
    "        'road_motorway_len', 'road_trunk_len',  'road_secondary_len', 'road_tertiary_len', 'road_residential_len',\n",
    "         \n",
    "]\n",
    "space_agg_mean_feature = [\n",
    "        # 'cloud_category', \n",
    "        'LST_day_mean', \n",
    "        # 'landcover_2023',       \n",
    "]\n",
    "\n",
    "daily_adm3_sum = daily_cell_ori.groupby(['date', 'ADM3_EN'])[spatial_agg_sum_feature].sum().reset_index()\n",
    "daily_adm3_avg = daily_cell_ori.groupby(['date', 'ADM3_EN'])[space_agg_mean_feature].mean().reset_index()\n",
    "daily_adm3 = daily_adm3_avg.merge(daily_adm3_sum, on=['date', 'ADM3_EN'], how='left')\n",
    "daily_adm3.to_csv(DATA_PATH / \"temp\" / 'addis_daily_adm3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_adm3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, generate the sub-district level, monthly data and save as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_agg_sum_feature = [\n",
    "        'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "        'NTL_mean',\n",
    "        'pop_sum_m',  \n",
    "        'road_len', \n",
    "        'poi_count', 'lu_industrial_area',\n",
    "        'lu_commercial_area',  'lu_residential_area', 'lu_retail_area', 'lu_farmland_area', \n",
    "        'lu_farmyard_area', \n",
    "        'road_primary_len',\n",
    "        'road_motorway_len', 'road_trunk_len',  'road_secondary_len', 'road_tertiary_len', 'road_residential_len',\n",
    "         \n",
    "]\n",
    "\n",
    "space_agg_mean_feature = [\n",
    "        # 'cloud_category', \n",
    "        'LST_day_mean', \n",
    "        # 'landcover_2023',       \n",
    "]\n",
    "\n",
    "monthly_adm3_sum = monthly_cell.groupby(['month', 'ADM3_EN'])[space_agg_sum_feature].sum().reset_index()\n",
    "monthly_adm3_avg = monthly_cell.groupby(['month', 'ADM3_EN'])[space_agg_mean_feature].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_adm3 = monthly_adm3_avg.merge(monthly_adm3_sum, on=['month', 'ADM3_EN'], how='left')\n",
    "monthly_adm3.to_csv(DATA_PATH / \"temp\" / 'addis_monthly_adm3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_adm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baghdad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Read the data\n",
    "full_df = pd.read_parquet(DATA_PATH / \"temp\" / \"full_baghdad_df.parquet\", engine=\"pyarrow\")\n",
    "full_df['month'] = full_df['date'].dt.to_period('M')\n",
    "full_df = full_df.rename(columns={'temp_mean': 'LST_day_mean'})   # unify to LST_day_mean\n",
    "\n",
    "# Original dataset: daily / cell resolution\n",
    "daily_cell_ori = full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geom_id', 'no2_mean', 'pop_sum_m', 'NTL_mean', 'road_len',\n",
       "       'road_share', 'poi_count', 'poi_share', 'lu_industrial_area',\n",
       "       'lu_industrial_share', 'lu_commercial_area', 'lu_commercial_share',\n",
       "       'lu_residential_area', 'lu_residential_share', 'lu_retail_area',\n",
       "       'lu_retail_share', 'lu_farmland_area', 'lu_farmland_share',\n",
       "       'lu_farmyard_area', 'lu_farmyard_share', 'road_motorway_len',\n",
       "       'road_trunk_len', 'road_primary_len', 'road_secondary_len',\n",
       "       'road_tertiary_len', 'road_residential_len', 'fossil_pp_count', 'TCI',\n",
       "       'geometry_x', 'date', 'no2_lag1', 'no2_neighbor_lag1', 'cloud_category',\n",
       "       'LST_day_mean', 'landcover_2023', 'Shape_Leng', 'Shape_Area', 'ADM3_EN',\n",
       "       'ADM3_PCODE', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate the cell level, monthly data and save as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features that need mean when aggregate from day to month level\n",
    "time_agg_mean_feature = [\n",
    "        'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "        'NTL_mean',\n",
    "        'TCI',\n",
    "        # 'cloud_category', \n",
    "        'LST_day_mean', \n",
    "        # 'landcover_2023',\n",
    "        'pop_sum_m',  \n",
    "        'road_len', \n",
    "        'poi_count', 'lu_industrial_area',\n",
    "        'lu_commercial_area',  'lu_residential_area', 'lu_retail_area', 'lu_farmland_area', \n",
    "        'lu_farmyard_area', \n",
    "        'road_primary_len',\n",
    "        'road_motorway_len', 'road_trunk_len',  'road_secondary_len', 'road_tertiary_len', 'road_residential_len',\n",
    "         \n",
    "]\n",
    "\n",
    "monthly_cell = daily_cell_ori.groupby(['geom_id', 'month', 'ADM3_EN'])[time_agg_mean_feature].mean().reset_index()\n",
    "monthly_cell.to_csv(DATA_PATH / \"temp\" / 'baghdad_monthly_cell.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_cell.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, generate the sub-district level, daily data and save as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_agg_sum_feature = [\n",
    "        'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "        'NTL_mean',\n",
    "        'TCI',\n",
    "        'pop_sum_m',  \n",
    "        'road_len', \n",
    "        'poi_count', 'lu_industrial_area',\n",
    "        'lu_commercial_area',  'lu_residential_area', 'lu_retail_area', 'lu_farmland_area', \n",
    "        'lu_farmyard_area', \n",
    "        'road_primary_len',\n",
    "        'road_motorway_len', 'road_trunk_len',  'road_secondary_len', 'road_tertiary_len', 'road_residential_len',\n",
    "         \n",
    "]\n",
    "space_agg_mean_feature = [\n",
    "        # 'cloud_category', \n",
    "        'LST_day_mean', \n",
    "        # 'landcover_2023',       \n",
    "]\n",
    "\n",
    "daily_adm3_sum = daily_cell_ori.groupby(['date', 'ADM3_EN'])[spatial_agg_sum_feature].sum().reset_index()\n",
    "daily_adm3_avg = daily_cell_ori.groupby(['date', 'ADM3_EN'])[space_agg_mean_feature].mean().reset_index()\n",
    "daily_adm3 = daily_adm3_avg.merge(daily_adm3_sum, on=['date', 'ADM3_EN'], how='left')\n",
    "daily_adm3.to_csv(DATA_PATH / \"temp\" / 'baghdad_daily_adm3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ADM3_EN</th>\n",
       "      <th>LST_day_mean</th>\n",
       "      <th>no2_mean</th>\n",
       "      <th>no2_lag1</th>\n",
       "      <th>no2_neighbor_lag1</th>\n",
       "      <th>NTL_mean</th>\n",
       "      <th>TCI</th>\n",
       "      <th>pop_sum_m</th>\n",
       "      <th>road_len</th>\n",
       "      <th>...</th>\n",
       "      <th>lu_residential_area</th>\n",
       "      <th>lu_retail_area</th>\n",
       "      <th>lu_farmland_area</th>\n",
       "      <th>lu_farmyard_area</th>\n",
       "      <th>road_primary_len</th>\n",
       "      <th>road_motorway_len</th>\n",
       "      <th>road_trunk_len</th>\n",
       "      <th>road_secondary_len</th>\n",
       "      <th>road_tertiary_len</th>\n",
       "      <th>road_residential_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Abu Ghraib</td>\n",
       "      <td>12.420119</td>\n",
       "      <td>0.095418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16517.924950</td>\n",
       "      <td>2.796893e+06</td>\n",
       "      <td>5.111909e+05</td>\n",
       "      <td>2.712699e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.436915e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.297929e+07</td>\n",
       "      <td>8734.319149</td>\n",
       "      <td>72805.034518</td>\n",
       "      <td>134296.278767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42553.429081</td>\n",
       "      <td>267884.154210</td>\n",
       "      <td>1.157555e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Al-Fahama</td>\n",
       "      <td>12.771666</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7831.303788</td>\n",
       "      <td>3.661187e+07</td>\n",
       "      <td>1.019493e+06</td>\n",
       "      <td>1.771264e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.925814e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.072887e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9374.771374</td>\n",
       "      <td>23504.337300</td>\n",
       "      <td>31207.293767</td>\n",
       "      <td>4882.810604</td>\n",
       "      <td>165507.886467</td>\n",
       "      <td>1.322054e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Al-Jisr</td>\n",
       "      <td>13.350142</td>\n",
       "      <td>0.066570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6524.999007</td>\n",
       "      <td>5.499618e+06</td>\n",
       "      <td>2.537896e+05</td>\n",
       "      <td>6.784165e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.367158e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.893582e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29253.292783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31548.343818</td>\n",
       "      <td>14049.948014</td>\n",
       "      <td>52184.422521</td>\n",
       "      <td>3.835369e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Al-Karrada Al-Sharqia</td>\n",
       "      <td>13.822724</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10284.304605</td>\n",
       "      <td>2.778583e+07</td>\n",
       "      <td>6.804654e+05</td>\n",
       "      <td>1.657440e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.692580e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.301526e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86620.551798</td>\n",
       "      <td>32598.567966</td>\n",
       "      <td>3730.427599</td>\n",
       "      <td>40736.417691</td>\n",
       "      <td>118433.362443</td>\n",
       "      <td>8.482135e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Al-Latifya</td>\n",
       "      <td>12.399308</td>\n",
       "      <td>0.072134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5038.490275</td>\n",
       "      <td>3.488123e+06</td>\n",
       "      <td>1.598628e+05</td>\n",
       "      <td>8.827485e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.360183e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.155193e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7899.550833</td>\n",
       "      <td>58368.588112</td>\n",
       "      <td>39685.810012</td>\n",
       "      <td>40019.461397</td>\n",
       "      <td>59683.649226</td>\n",
       "      <td>2.846482e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15346</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>Markaz Al-Karkh</td>\n",
       "      <td>13.878697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021264</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>8148.203962</td>\n",
       "      <td>2.332277e+05</td>\n",
       "      <td>1.772346e+05</td>\n",
       "      <td>7.375371e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005766e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71372.128640</td>\n",
       "      <td>14098.977033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50811.351119</td>\n",
       "      <td>31200.153262</td>\n",
       "      <td>2.529159e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15347</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>Markaz Al-Mada'in</td>\n",
       "      <td>13.722056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406985</td>\n",
       "      <td>0.404960</td>\n",
       "      <td>7029.665967</td>\n",
       "      <td>1.069760e+05</td>\n",
       "      <td>1.636998e+05</td>\n",
       "      <td>7.846149e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.549490e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.479552e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22978.465260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42400.937644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42946.439662</td>\n",
       "      <td>4.669922e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>Markaz Al-Mahmoudiya</td>\n",
       "      <td>13.583168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.034775</td>\n",
       "      <td>1850.581081</td>\n",
       "      <td>1.531018e+05</td>\n",
       "      <td>1.072385e+05</td>\n",
       "      <td>4.749349e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>8.522331e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.965280e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9762.885545</td>\n",
       "      <td>7638.868219</td>\n",
       "      <td>9296.516086</td>\n",
       "      <td>2541.127138</td>\n",
       "      <td>57374.549910</td>\n",
       "      <td>2.398848e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>Markaz Al-Thawra</td>\n",
       "      <td>13.638146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046052</td>\n",
       "      <td>0.046124</td>\n",
       "      <td>10100.558853</td>\n",
       "      <td>3.099584e+05</td>\n",
       "      <td>1.156935e+06</td>\n",
       "      <td>1.571223e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.415564e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109349.280838</td>\n",
       "      <td>12861.668398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49075.556344</td>\n",
       "      <td>167318.888287</td>\n",
       "      <td>1.085157e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>That al Salasil</td>\n",
       "      <td>12.653585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.036559</td>\n",
       "      <td>5702.589884</td>\n",
       "      <td>4.826648e+04</td>\n",
       "      <td>2.476259e+05</td>\n",
       "      <td>4.983867e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.333917e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.258954e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3718.209313</td>\n",
       "      <td>14451.666038</td>\n",
       "      <td>2326.787823</td>\n",
       "      <td>18441.066309</td>\n",
       "      <td>3.578889e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15351 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                ADM3_EN  LST_day_mean  no2_mean  no2_lag1  \\\n",
       "0     2023-01-01             Abu Ghraib     12.420119  0.095418  0.000000   \n",
       "1     2023-01-01              Al-Fahama     12.771666  0.037629  0.000000   \n",
       "2     2023-01-01                Al-Jisr     13.350142  0.066570  0.000000   \n",
       "3     2023-01-01  Al-Karrada Al-Sharqia     13.822724  0.043889  0.000000   \n",
       "4     2023-01-01             Al-Latifya     12.399308  0.072134  0.000000   \n",
       "...          ...                    ...           ...       ...       ...   \n",
       "15346 2024-12-31        Markaz Al-Karkh     13.878697  0.000000  0.021264   \n",
       "15347 2024-12-31      Markaz Al-Mada'in     13.722056  0.000000  0.406985   \n",
       "15348 2024-12-31   Markaz Al-Mahmoudiya     13.583168  0.000000  0.034868   \n",
       "15349 2024-12-31       Markaz Al-Thawra     13.638146  0.000000  0.046052   \n",
       "15350 2024-12-31        That al Salasil     12.653585  0.000000  0.036308   \n",
       "\n",
       "       no2_neighbor_lag1      NTL_mean           TCI     pop_sum_m  \\\n",
       "0               0.000000  16517.924950  2.796893e+06  5.111909e+05   \n",
       "1               0.000000   7831.303788  3.661187e+07  1.019493e+06   \n",
       "2               0.000000   6524.999007  5.499618e+06  2.537896e+05   \n",
       "3               0.000000  10284.304605  2.778583e+07  6.804654e+05   \n",
       "4               0.000000   5038.490275  3.488123e+06  1.598628e+05   \n",
       "...                  ...           ...           ...           ...   \n",
       "15346           0.021154   8148.203962  2.332277e+05  1.772346e+05   \n",
       "15347           0.404960   7029.665967  1.069760e+05  1.636998e+05   \n",
       "15348           0.034775   1850.581081  1.531018e+05  1.072385e+05   \n",
       "15349           0.046124  10100.558853  3.099584e+05  1.156935e+06   \n",
       "15350           0.036559   5702.589884  4.826648e+04  2.476259e+05   \n",
       "\n",
       "           road_len  ...  lu_residential_area  lu_retail_area  \\\n",
       "0      2.712699e+06  ...         2.436915e+07             0.0   \n",
       "1      1.771264e+06  ...         3.925814e+07             0.0   \n",
       "2      6.784165e+05  ...         9.367158e+06             0.0   \n",
       "3      1.657440e+06  ...         2.692580e+07             0.0   \n",
       "4      8.827485e+05  ...         2.360183e+06             0.0   \n",
       "...             ...  ...                  ...             ...   \n",
       "15346  7.375371e+05  ...         1.005766e+07             0.0   \n",
       "15347  7.846149e+05  ...         1.549490e+07             0.0   \n",
       "15348  4.749349e+05  ...         8.522331e+03             0.0   \n",
       "15349  1.571223e+06  ...         3.415564e+07             0.0   \n",
       "15350  4.983867e+05  ...         6.333917e+06             0.0   \n",
       "\n",
       "       lu_farmland_area  lu_farmyard_area  road_primary_len  \\\n",
       "0          3.297929e+07       8734.319149      72805.034518   \n",
       "1          1.072887e+06          0.000000       9374.771374   \n",
       "2          1.893582e+07          0.000000      29253.292783   \n",
       "3          6.301526e+06          0.000000      86620.551798   \n",
       "4          2.155193e+08          0.000000       7899.550833   \n",
       "...                 ...               ...               ...   \n",
       "15346      0.000000e+00          0.000000      71372.128640   \n",
       "15347      1.479552e+08          0.000000      22978.465260   \n",
       "15348      3.965280e+07          0.000000       9762.885545   \n",
       "15349      0.000000e+00          0.000000     109349.280838   \n",
       "15350      3.258954e+05          0.000000          0.000000   \n",
       "\n",
       "       road_motorway_len  road_trunk_len  road_secondary_len  \\\n",
       "0          134296.278767        0.000000        42553.429081   \n",
       "1           23504.337300    31207.293767         4882.810604   \n",
       "2               0.000000    31548.343818        14049.948014   \n",
       "3           32598.567966     3730.427599        40736.417691   \n",
       "4           58368.588112    39685.810012        40019.461397   \n",
       "...                  ...             ...                 ...   \n",
       "15346       14098.977033        0.000000        50811.351119   \n",
       "15347           0.000000    42400.937644            0.000000   \n",
       "15348        7638.868219     9296.516086         2541.127138   \n",
       "15349       12861.668398        0.000000        49075.556344   \n",
       "15350        3718.209313    14451.666038         2326.787823   \n",
       "\n",
       "       road_tertiary_len  road_residential_len  \n",
       "0          267884.154210          1.157555e+06  \n",
       "1          165507.886467          1.322054e+06  \n",
       "2           52184.422521          3.835369e+05  \n",
       "3          118433.362443          8.482135e+05  \n",
       "4           59683.649226          2.846482e+05  \n",
       "...                  ...                   ...  \n",
       "15346       31200.153262          2.529159e+05  \n",
       "15347       42946.439662          4.669922e+05  \n",
       "15348       57374.549910          2.398848e+05  \n",
       "15349      167318.888287          1.085157e+06  \n",
       "15350       18441.066309          3.578889e+05  \n",
       "\n",
       "[15351 rows x 23 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_adm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, generate the sub-district level, monthly data and save as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_agg_sum_feature = [\n",
    "        'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "        'NTL_mean',\n",
    "        'TCI',\n",
    "        'pop_sum_m',  \n",
    "        'road_len', \n",
    "        'poi_count', 'lu_industrial_area',\n",
    "        'lu_commercial_area',  'lu_residential_area', 'lu_retail_area', 'lu_farmland_area', \n",
    "        'lu_farmyard_area', \n",
    "        'road_primary_len',\n",
    "        'road_motorway_len', 'road_trunk_len',  'road_secondary_len', 'road_tertiary_len', 'road_residential_len',\n",
    "         \n",
    "]\n",
    "\n",
    "space_agg_mean_feature = [\n",
    "        # 'cloud_category', \n",
    "        'LST_day_mean', \n",
    "        # 'landcover_2023',       \n",
    "]\n",
    "\n",
    "monthly_adm3_sum = monthly_cell.groupby(['month', 'ADM3_EN'])[space_agg_sum_feature].sum().reset_index()\n",
    "monthly_adm3_avg = monthly_cell.groupby(['month', 'ADM3_EN'])[space_agg_mean_feature].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_adm3 = monthly_adm3_avg.merge(monthly_adm3_sum, on=['month', 'ADM3_EN'], how='left')\n",
    "monthly_adm3.to_csv(DATA_PATH / \"temp\" / 'baghdad_monthly_adm3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_adm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Addis Ababa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Baghdad: Mobility Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0) Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "baghdad_df = pd.read_csv(DATA_PATH / \"temp\" / 'baghdad_monthly_adm3.csv')\n",
    "\n",
    "# Features and targets setup\n",
    "features = [\n",
    "    'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "    'NTL_mean', 'pop_sum_m', 'road_len',\n",
    "    'poi_count', 'lu_industrial_area',\n",
    "    'lu_commercial_area', 'lu_residential_area',\n",
    "    'non_built_area'\n",
    "\n",
    "    # ,'LST_day_mean','lu_retail_area',\n",
    "    # 'lu_farmland_area',\n",
    "    #    'lu_farmyard_area', 'road_primary_len', 'road_motorway_len',\n",
    "    #    'road_trunk_len', 'road_secondary_len', 'road_tertiary_len',\n",
    "    #    'road_residential_len', 'grassland_a', 'cropland_a', 'built_up_a',\n",
    "    #    'snow_a', 'water_bod_a', 'wetland_a', 'sparse_veg_a', 'mangroves_a',\n",
    "    #    'moss_a', 'unclassified_a'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1) Models y=TCI/road_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.1) Simple linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple LR (TRAIN): RMSE = 6.5752, R² = 0.5701\n",
      "Simple LR (TEST): RMSE = 7.0133, R² = 0.5787\n"
     ]
    }
   ],
   "source": [
    "# Target definition\n",
    "baghdad_df['y1'] = baghdad_df['TCI'] / baghdad_df['road_len']\n",
    "\n",
    "# Train/test split\n",
    "X = baghdad_df[features]\n",
    "y = baghdad_df['y1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit & predict\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test  = lr.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "for label, y_true, y_pred in [\n",
    "    ('TRAIN', y_train, y_pred_train),\n",
    "    ('TEST',  y_test,  y_pred_test)\n",
    "]:\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    print(f\"Simple LR ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.2) Log-log Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log–Log LR (TRAIN): RMSE = 5.1990, R² = 0.1374\n",
      "Log–Log LR (TEST): RMSE = 5.0356, R² = 0.0052\n"
     ]
    }
   ],
   "source": [
    "# Clone and avoid zeros: common practice is to add a small ε = half the minimum positive value\n",
    "df_ll = baghdad_df.copy()\n",
    "epsilon = df_ll[features + ['TCI', 'road_len']].replace(0, np.nan).min().min() / 2\n",
    "\n",
    "# Log transformation\n",
    "for col in features:\n",
    "    df_ll[col] = np.log(df_ll[col].clip(lower=epsilon))\n",
    "df_ll['y2'] = np.log((df_ll['TCI'] / df_ll['road_len']).clip(lower=epsilon))\n",
    "\n",
    "# Split\n",
    "X = df_ll[features]\n",
    "y = df_ll['y2']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit & evaluate\n",
    "lr_ll = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, lr_ll),\n",
    "    ('TEST',  X_test,  y_test,  lr_ll)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Log–Log LR ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3) Polynomial Dg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly LR (deg 3) (TRAIN): RMSE = 4.3668, R² = 0.8104\n",
      "Poly LR (deg 3) (TEST): RMSE = 6.5631, R² = 0.6310\n"
     ]
    }
   ],
   "source": [
    "# Degree‐3 polynomial expansion (no interactions)\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(baghdad_df[features])\n",
    "\n",
    "# Train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly, baghdad_df['y1'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit & evaluate\n",
    "poly_lr = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, poly_lr),\n",
    "    ('TEST',  X_test,  y_test,  poly_lr)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Poly LR (deg 3) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) Models y=TCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1) Simple linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple LR (TCI) (TRAIN): RMSE = 10025610.9905, R² = 0.6511\n",
      "Simple LR (TCI) (TEST): RMSE = 11332689.0372, R² = 0.7308\n"
     ]
    }
   ],
   "source": [
    "X = baghdad_df[features]\n",
    "y = baghdad_df['TCI']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr2 = LinearRegression().fit(X_train, y_train)\n",
    "for label, y_true, y_pred in [\n",
    "    ('TRAIN', y_train, lr2.predict(X_train)),\n",
    "    ('TEST',  y_test,  lr2.predict(X_test))\n",
    "]:\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    print(f\"Simple LR (TCI) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2) Log-log Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log–Log LR (TCI) (TRAIN): RMSE = 5.9274, R² = 0.1163\n",
      "Log–Log LR (TCI) (TEST): RMSE = 5.7096, R² = -0.0072\n"
     ]
    }
   ],
   "source": [
    "# Add ε and log‐transform\n",
    "df_ll2 = baghdad_df.copy()\n",
    "epsilon = df_ll2[features + ['TCI']].replace(0, np.nan).min().min() / 2\n",
    "for col in features:\n",
    "    df_ll2[col] = np.log(df_ll2[col].clip(lower=epsilon))\n",
    "df_ll2['y5'] = np.log(df_ll2['TCI'].clip(lower=epsilon))\n",
    "\n",
    "X = df_ll2[features]\n",
    "y = df_ll2['y5']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr_ll2 = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, lr_ll2),\n",
    "    ('TEST',  X_test,  y_test,  lr_ll2)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Log–Log LR (TCI) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3) Polynomial Dg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly LR (TCI, deg 3) (TRAIN): RMSE = 6199889.6561, R² = 0.8666\n",
      "Poly LR (TCI, deg 3) (TEST): RMSE = 14277190.1367, R² = 0.5727\n"
     ]
    }
   ],
   "source": [
    "poly2 = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly2 = poly2.fit_transform(baghdad_df[features])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly2, baghdad_df['TCI'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "poly2_lr = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, poly2_lr),\n",
    "    ('TEST',  X_test,  y_test,  poly2_lr)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Poly LR (TCI, deg 3) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics models performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE and R2 for all 6 linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e791a\">\n",
       "  <caption>Baseline Linear-Family Models – Performance KPI Matrix</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e791a_level0_col0\" class=\"col_heading level0 col0\" >RMSE_train</th>\n",
       "      <th id=\"T_e791a_level0_col1\" class=\"col_heading level0 col1\" >R2_train</th>\n",
       "      <th id=\"T_e791a_level0_col2\" class=\"col_heading level0 col2\" >RMSE_test</th>\n",
       "      <th id=\"T_e791a_level0_col3\" class=\"col_heading level0 col3\" >R2_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"index_name level1\" >Target</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e791a_level0_row0\" class=\"row_heading level0 row0\" >LR</th>\n",
       "      <th id=\"T_e791a_level1_row0\" class=\"row_heading level1 row0\" >TCI/road_len</th>\n",
       "      <td id=\"T_e791a_row0_col0\" class=\"data row0 col0\" >6.5752</td>\n",
       "      <td id=\"T_e791a_row0_col1\" class=\"data row0 col1\" >0.5701</td>\n",
       "      <td id=\"T_e791a_row0_col2\" class=\"data row0 col2\" >7.0133</td>\n",
       "      <td id=\"T_e791a_row0_col3\" class=\"data row0 col3\" >0.5787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e791a_level0_row1\" class=\"row_heading level0 row1\" >Log–Log LR</th>\n",
       "      <th id=\"T_e791a_level1_row1\" class=\"row_heading level1 row1\" >TCI/road_len</th>\n",
       "      <td id=\"T_e791a_row1_col0\" class=\"data row1 col0\" >5.1990</td>\n",
       "      <td id=\"T_e791a_row1_col1\" class=\"data row1 col1\" >0.1374</td>\n",
       "      <td id=\"T_e791a_row1_col2\" class=\"data row1 col2\" >5.0356</td>\n",
       "      <td id=\"T_e791a_row1_col3\" class=\"data row1 col3\" >0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e791a_level0_row2\" class=\"row_heading level0 row2\" >Poly LR (d=3)</th>\n",
       "      <th id=\"T_e791a_level1_row2\" class=\"row_heading level1 row2\" >TCI/road_len</th>\n",
       "      <td id=\"T_e791a_row2_col0\" class=\"data row2 col0\" >4.3668</td>\n",
       "      <td id=\"T_e791a_row2_col1\" class=\"data row2 col1\" >0.8104</td>\n",
       "      <td id=\"T_e791a_row2_col2\" class=\"data row2 col2\" >6.5631</td>\n",
       "      <td id=\"T_e791a_row2_col3\" class=\"data row2 col3\" >0.6310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e791a_level0_row3\" class=\"row_heading level0 row3\" >LR</th>\n",
       "      <th id=\"T_e791a_level1_row3\" class=\"row_heading level1 row3\" >TCI</th>\n",
       "      <td id=\"T_e791a_row3_col0\" class=\"data row3 col0\" >10025610.9905</td>\n",
       "      <td id=\"T_e791a_row3_col1\" class=\"data row3 col1\" >0.6511</td>\n",
       "      <td id=\"T_e791a_row3_col2\" class=\"data row3 col2\" >11332689.0372</td>\n",
       "      <td id=\"T_e791a_row3_col3\" class=\"data row3 col3\" >0.7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e791a_level0_row4\" class=\"row_heading level0 row4\" >Log–Log LR</th>\n",
       "      <th id=\"T_e791a_level1_row4\" class=\"row_heading level1 row4\" >TCI</th>\n",
       "      <td id=\"T_e791a_row4_col0\" class=\"data row4 col0\" >5.9274</td>\n",
       "      <td id=\"T_e791a_row4_col1\" class=\"data row4 col1\" >0.1163</td>\n",
       "      <td id=\"T_e791a_row4_col2\" class=\"data row4 col2\" >5.7096</td>\n",
       "      <td id=\"T_e791a_row4_col3\" class=\"data row4 col3\" >-0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e791a_level0_row5\" class=\"row_heading level0 row5\" >Poly LR (d=3)</th>\n",
       "      <th id=\"T_e791a_level1_row5\" class=\"row_heading level1 row5\" >TCI</th>\n",
       "      <td id=\"T_e791a_row5_col0\" class=\"data row5 col0\" >6199889.6561</td>\n",
       "      <td id=\"T_e791a_row5_col1\" class=\"data row5 col1\" >0.8666</td>\n",
       "      <td id=\"T_e791a_row5_col2\" class=\"data row5 col2\" >14277190.1367</td>\n",
       "      <td id=\"T_e791a_row5_col3\" class=\"data row5 col3\" >0.5727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b4e486a6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  PERFORMANCE DASHBOARD – all six baseline models\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Scoring utility\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_model(model, X_tr, X_te, y_tr, y_te):\n",
    "    \"\"\"Return RMSE_train, R2_train, RMSE_test, R2_test.\"\"\"\n",
    "    y_hat_tr = model.predict(X_tr)\n",
    "    y_hat_te = model.predict(X_te)\n",
    "    rmse_tr  = np.sqrt(mean_squared_error(y_tr, y_hat_tr))\n",
    "    rmse_te  = np.sqrt(mean_squared_error(y_te, y_hat_te))\n",
    "    r2_tr    = r2_score(y_tr, y_hat_tr)\n",
    "    r2_te    = r2_score(y_te, y_hat_te)\n",
    "    return rmse_tr, r2_tr, rmse_te, r2_te\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Regenerate splits & capture metrics\n",
    "# ------------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "# --- 1) Simple LR on TCI/road_len ------------------------------------------\n",
    "X = baghdad_df[features]\n",
    "y = baghdad_df['y1']                              # target already computed\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['LR', 'TCI/road_len', *evaluate_model(lr, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 2) Log–Log LR on TCI/road_len -----------------------------------------\n",
    "X = df_ll[features]\n",
    "y = df_ll['y2']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Log–Log LR', 'TCI/road_len', *evaluate_model(lr_ll, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 3) Poly-deg-3 LR on TCI/road_len --------------------------------------\n",
    "poly_tmp = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly   = poly_tmp.fit_transform(baghdad_df[features])\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_poly, baghdad_df['y1'],\n",
    "                                          test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Poly LR (d=3)', 'TCI/road_len', *evaluate_model(poly_lr, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 4) Simple LR on raw TCI ------------------------------------------------\n",
    "X = baghdad_df[features]\n",
    "y = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['LR', 'TCI', *evaluate_model(lr2, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 5) Log–Log LR on raw TCI ----------------------------------------------\n",
    "X = df_ll2[features]\n",
    "y = df_ll2['y5']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Log–Log LR', 'TCI', *evaluate_model(lr_ll2, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 6) Poly-deg-3 LR on raw TCI -------------------------------------------\n",
    "poly_tmp2 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly2   = poly_tmp2.fit_transform(baghdad_df[features])\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_poly2, baghdad_df['TCI'],\n",
    "                                          test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Poly LR (d=3)', 'TCI', *evaluate_model(poly2_lr, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Executive summary table\n",
    "# ------------------------------------------------------------\n",
    "df_perf = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['Model', 'Target', 'RMSE_train', 'R2_train', 'RMSE_test', 'R2_test']\n",
    ").set_index(['Model', 'Target'])\n",
    "\n",
    "display(\n",
    "    df_perf.style.format('{:.4f}')\n",
    "          .set_caption(\"Baseline Linear-Family Models – Performance KPI Matrix\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIC, BIC and adjusted R2 for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_88d6b\">\n",
       "  <caption>Complexity‐Penalized & Out‐of‐Sample Performance</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_88d6b_level0_col0\" class=\"col_heading level0 col0\" >#params</th>\n",
       "      <th id=\"T_88d6b_level0_col1\" class=\"col_heading level0 col1\" >AIC</th>\n",
       "      <th id=\"T_88d6b_level0_col2\" class=\"col_heading level0 col2\" >BIC</th>\n",
       "      <th id=\"T_88d6b_level0_col3\" class=\"col_heading level0 col3\" >R2_train</th>\n",
       "      <th id=\"T_88d6b_level0_col4\" class=\"col_heading level0 col4\" >R2_adj</th>\n",
       "      <th id=\"T_88d6b_level0_col5\" class=\"col_heading level0 col5\" >RMSE_test</th>\n",
       "      <th id=\"T_88d6b_level0_col6\" class=\"col_heading level0 col6\" >R2_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_88d6b_level0_row0\" class=\"row_heading level0 row0\" >OLS (TCI/road_len)</th>\n",
       "      <td id=\"T_88d6b_row0_col0\" class=\"data row0 col0\" >12</td>\n",
       "      <td id=\"T_88d6b_row0_col1\" class=\"data row0 col1\" >1541.94</td>\n",
       "      <td id=\"T_88d6b_row0_col2\" class=\"data row0 col2\" >1589.93</td>\n",
       "      <td id=\"T_88d6b_row0_col3\" class=\"data row0 col3\" >0.5701</td>\n",
       "      <td id=\"T_88d6b_row0_col4\" class=\"data row0 col4\" >0.5568</td>\n",
       "      <td id=\"T_88d6b_row0_col5\" class=\"data row0 col5\" >7.01</td>\n",
       "      <td id=\"T_88d6b_row0_col6\" class=\"data row0 col6\" >0.5787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88d6b_level0_row1\" class=\"row_heading level0 row1\" >LogLog (TCI/road_len)</th>\n",
       "      <td id=\"T_88d6b_row1_col0\" class=\"data row1 col0\" >12</td>\n",
       "      <td id=\"T_88d6b_row1_col1\" class=\"data row1 col1\" >1352.67</td>\n",
       "      <td id=\"T_88d6b_row1_col2\" class=\"data row1 col2\" >1400.66</td>\n",
       "      <td id=\"T_88d6b_row1_col3\" class=\"data row1 col3\" >0.1374</td>\n",
       "      <td id=\"T_88d6b_row1_col4\" class=\"data row1 col4\" >0.1109</td>\n",
       "      <td id=\"T_88d6b_row1_col5\" class=\"data row1 col5\" >5.04</td>\n",
       "      <td id=\"T_88d6b_row1_col6\" class=\"data row1 col6\" >0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88d6b_level0_row2\" class=\"row_heading level0 row2\" >Poly3 (TCI/road_len)</th>\n",
       "      <td id=\"T_88d6b_row2_col0\" class=\"data row2 col0\" >364</td>\n",
       "      <td id=\"T_88d6b_row2_col1\" class=\"data row2 col1\" >1916.06</td>\n",
       "      <td id=\"T_88d6b_row2_col2\" class=\"data row2 col2\" >3371.68</td>\n",
       "      <td id=\"T_88d6b_row2_col3\" class=\"data row2 col3\" >0.8104</td>\n",
       "      <td id=\"T_88d6b_row2_col4\" class=\"data row2 col4\" >-1.0061</td>\n",
       "      <td id=\"T_88d6b_row2_col5\" class=\"data row2 col5\" >6.56</td>\n",
       "      <td id=\"T_88d6b_row2_col6\" class=\"data row2 col6\" >0.6310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88d6b_level0_row3\" class=\"row_heading level0 row3\" >OLS (TCI)</th>\n",
       "      <td id=\"T_88d6b_row3_col0\" class=\"data row3 col0\" >12</td>\n",
       "      <td id=\"T_88d6b_row3_col1\" class=\"data row3 col1\" >13017.25</td>\n",
       "      <td id=\"T_88d6b_row3_col2\" class=\"data row3 col2\" >13065.23</td>\n",
       "      <td id=\"T_88d6b_row3_col3\" class=\"data row3 col3\" >0.6511</td>\n",
       "      <td id=\"T_88d6b_row3_col4\" class=\"data row3 col4\" >0.6404</td>\n",
       "      <td id=\"T_88d6b_row3_col5\" class=\"data row3 col5\" >11332689.04</td>\n",
       "      <td id=\"T_88d6b_row3_col6\" class=\"data row3 col6\" >0.7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88d6b_level0_row4\" class=\"row_heading level0 row4\" >LogLog (TCI)</th>\n",
       "      <td id=\"T_88d6b_row4_col0\" class=\"data row4 col0\" >12</td>\n",
       "      <td id=\"T_88d6b_row4_col1\" class=\"data row4 col1\" >1458.35</td>\n",
       "      <td id=\"T_88d6b_row4_col2\" class=\"data row4 col2\" >1506.34</td>\n",
       "      <td id=\"T_88d6b_row4_col3\" class=\"data row4 col3\" >0.1163</td>\n",
       "      <td id=\"T_88d6b_row4_col4\" class=\"data row4 col4\" >0.0891</td>\n",
       "      <td id=\"T_88d6b_row4_col5\" class=\"data row4 col5\" >5.71</td>\n",
       "      <td id=\"T_88d6b_row4_col6\" class=\"data row4 col6\" >-0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88d6b_level0_row5\" class=\"row_heading level0 row5\" >Poly3 (TCI)</th>\n",
       "      <td id=\"T_88d6b_row5_col0\" class=\"data row5 col0\" >364</td>\n",
       "      <td id=\"T_88d6b_row5_col1\" class=\"data row5 col1\" >13333.87</td>\n",
       "      <td id=\"T_88d6b_row5_col2\" class=\"data row5 col2\" >14789.49</td>\n",
       "      <td id=\"T_88d6b_row5_col3\" class=\"data row5 col3\" >0.8666</td>\n",
       "      <td id=\"T_88d6b_row5_col4\" class=\"data row5 col4\" >-0.4115</td>\n",
       "      <td id=\"T_88d6b_row5_col5\" class=\"data row5 col5\" >14277190.14</td>\n",
       "      <td id=\"T_88d6b_row5_col6\" class=\"data row5 col6\" >0.5727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b4f91527d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_ic(model, X_tr, y_tr):\n",
    "    \"\"\"Return (p, AIC, BIC, R2_train, R2_adj).\"\"\"\n",
    "    y_hat = model.predict(X_tr)\n",
    "    rss   = ((y_tr - y_hat) ** 2).sum()\n",
    "    n     = len(y_tr)\n",
    "    p     = np.count_nonzero(model.coef_) + 1\n",
    "    aic   = n * np.log(rss / n) + 2 * p\n",
    "    bic   = n * np.log(rss / n) + p * np.log(n)\n",
    "    r2    = r2_score(y_tr, y_hat)\n",
    "    r2_adj= 1 - (1 - r2)*(n - 1)/(n - p - 1)\n",
    "    return p, aic, bic, r2, r2_adj\n",
    "\n",
    "def compute_test_metrics(model, X_te, y_te):\n",
    "    \"\"\"Return (RMSE_test, R2_test).\"\"\"\n",
    "    y_hat = model.predict(X_te)\n",
    "    return np.sqrt(mean_squared_error(y_te, y_hat)), r2_score(y_te, y_hat)\n",
    "\n",
    "# container\n",
    "rows = []\n",
    "\n",
    "# 1) OLS on TCI/road_len\n",
    "y1 = baghdad_df['TCI'] / baghdad_df['road_len']\n",
    "X1 = baghdad_df[features]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['OLS (TCI/road_len)',\n",
    "     *compute_ic(lr, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 2) Log–Log OLS on TCI/road_len\n",
    "X2, y2 = df_ll[features], df_ll['y2']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['LogLog (TCI/road_len)',\n",
    "     *compute_ic(lr_ll, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr_ll, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 3) Poly3 OLS on TCI/road_len\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X3 = poly.fit_transform(baghdad_df[features])\n",
    "y3 = baghdad_df['TCI'] / baghdad_df['road_len']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X3, y3, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['Poly3 (TCI/road_len)',\n",
    "     *compute_ic(poly_lr, X_tr, y_tr),\n",
    "     *compute_test_metrics(poly_lr, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 4) OLS on TCI\n",
    "X4, y4 = baghdad_df[features], baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X4, y4, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['OLS (TCI)',\n",
    "     *compute_ic(lr2, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr2, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 5) Log–Log OLS on TCI\n",
    "X5, y5 = df_ll2[features], df_ll2['y5']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X5, y5, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['LogLog (TCI)',\n",
    "     *compute_ic(lr_ll2, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr_ll2, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 6) Poly3 OLS on TCI\n",
    "poly2 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X6 = poly2.fit_transform(baghdad_df[features])\n",
    "y6 = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X6, y6, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['Poly3 (TCI)',\n",
    "     *compute_ic(poly2_lr, X_tr, y_tr),\n",
    "     *compute_test_metrics(poly2_lr, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# build DataFrame\n",
    "df_compare = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "      'Model', '#params', 'AIC', 'BIC', 'R2_train', 'R2_adj',\n",
    "      'RMSE_test', 'R2_test'\n",
    "    ]\n",
    ").set_index('Model')\n",
    "\n",
    "# display\n",
    "display(\n",
    "    df_compare.style\n",
    "      .format({\n",
    "         '#params':'{:.0f}',\n",
    "         'AIC':'{:.2f}','BIC':'{:.2f}',\n",
    "         'R2_train':'{:.4f}','R2_adj':'{:.4f}',\n",
    "         'RMSE_test':'{:.2f}','R2_test':'{:.4f}'\n",
    "      })\n",
    "      .set_caption(\"Complexity‐Penalized & Out‐of‐Sample Performance\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across both target definitions, the simple OLS approaches (12 parameters, no transforms or high-order terms) consistently give you the best out-of-sample R² while keeping model complexity and information‐criteria penalties low.\n",
    "\n",
    "The polynomial expansions over-fit, and the log–log specs under-fit—they never beat plain OLS on real-world generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimentation to improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal α        : 0.000100\n",
      "Train →  RMSE = 8528955.0585, R² = 0.7475\n",
      "Test  →  RMSE = 9837093.1042, R² = 0.7972\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# ── Suppress warnings ─────────────────────────────────────\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# ── (Re)define the full feature set & data split ─────────\n",
    "features_full = [\n",
    "    'no2_mean', 'no2_lag1', 'no2_neighbor_lag1',\n",
    "    'NTL_mean', 'pop_sum_m', 'road_len',\n",
    "    'poi_count', 'lu_industrial_area',\n",
    "    'lu_commercial_area', 'lu_residential_area',\n",
    "    'non_built_area', 'LST_day_mean', 'lu_retail_area',\n",
    "    'lu_farmland_area', 'lu_farmyard_area',\n",
    "    'road_primary_len', 'road_motorway_len',\n",
    "    'road_trunk_len', 'road_secondary_len',\n",
    "    'road_tertiary_len', 'road_residential_len',\n",
    "    'grassland_a', 'cropland_a', 'built_up_a',\n",
    "    'water_bod_a', 'wetland_a'\n",
    "    #,'snow_a'\n",
    "    # ,'sparse_veg_a', 'mangroves_a', 'moss_a',\n",
    "    # 'unclassified_a'\n",
    "]\n",
    "\n",
    "X_full = baghdad_df[features_full]\n",
    "y_full = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ── Pipeline & Fit ────────────────────────────────────────\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('scaler',  StandardScaler()),\n",
    "    ('lassocv', LassoCV(\n",
    "        alphas=np.logspace(-4, 1, 50),\n",
    "        cv=5, max_iter=20000, n_jobs=-1, random_state=42\n",
    "    ))\n",
    "])\n",
    "lasso_pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "# ── Metrics ───────────────────────────────────────────────\n",
    "y_tr_pred = lasso_pipeline.predict(X_tr)\n",
    "y_te_pred = lasso_pipeline.predict(X_te)\n",
    "\n",
    "rmse_tr = np.sqrt(mean_squared_error(y_tr, y_tr_pred))\n",
    "r2_tr   = r2_score(y_tr, y_tr_pred)\n",
    "rmse_te = np.sqrt(mean_squared_error(y_te, y_te_pred))\n",
    "r2_te   = r2_score(y_te, y_te_pred)\n",
    "alpha_opt = lasso_pipeline.named_steps['lassocv'].alpha_\n",
    "\n",
    "# ── Clean output ──────────────────────────────────────────\n",
    "print(f\"Optimal α        : {alpha_opt:.6f}\")\n",
    "print(f\"Train →  RMSE = {rmse_tr:.4f}, R² = {r2_tr:.4f}\")\n",
    "print(f\"Test  →  RMSE = {rmse_te:.4f}, R² = {r2_te:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       coefficient\n",
      "feature                           \n",
      "no2_lag1              4.757508e+08\n",
      "no2_neighbor_lag1    -2.840682e+08\n",
      "no2_mean             -1.338793e+08\n",
      "Intercept             1.704146e+07\n",
      "poi_count             9.290695e+05\n",
      "LST_day_mean         -4.622741e+05\n",
      "lu_retail_area        9.373272e+02\n",
      "NTL_mean             -6.388567e+02\n",
      "lu_farmyard_area     -5.730282e+02\n",
      "road_motorway_len    -3.758904e+02\n",
      "road_trunk_len       -2.648180e+02\n",
      "road_primary_len     -2.073009e+02\n",
      "road_len              5.797252e+01\n",
      "road_residential_len -5.014573e+01\n",
      "road_secondary_len   -4.675715e+01\n",
      "road_tertiary_len    -4.370022e+01\n",
      "wetland_a            -2.552886e+01\n",
      "pop_sum_m             2.223345e+01\n",
      "grassland_a          -3.277168e+00\n",
      "water_bod_a           3.898104e-01\n",
      "cropland_a           -3.261144e-01\n",
      "non_built_area        2.877204e-01\n",
      "lu_commercial_area    2.606185e-01\n",
      "built_up_a           -8.912455e-02\n",
      "lu_industrial_area   -8.243068e-02\n",
      "lu_residential_area  -7.556240e-02\n",
      "lu_farmland_area      5.355667e-03\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract fitted components\n",
    "scaler = lasso_pipeline.named_steps['scaler']\n",
    "lasso  = lasso_pipeline.named_steps['lassocv']\n",
    "\n",
    "# 2. Scaled-space coefficients\n",
    "coef_scaled = lasso.coef_\n",
    "\n",
    "# 3. Back-transform to original units\n",
    "coef_original = coef_scaled / scaler.scale_\n",
    "intercept_original = (\n",
    "    lasso.intercept_\n",
    "    - np.dot(scaler.mean_ / scaler.scale_, coef_scaled)\n",
    ")\n",
    "\n",
    "# 4. Assemble into DataFrame\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': features_full,\n",
    "    'coefficient': coef_original\n",
    "}).set_index('feature')\n",
    "\n",
    "# Add the intercept\n",
    "coef_df.loc['Intercept'] = intercept_original\n",
    "\n",
    "# 5. Sort by magnitude for readability\n",
    "coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "coef_df = coef_df.sort_values('abs_coef', ascending=False).drop(columns='abs_coef')\n",
    "\n",
    "# 6. Print to console\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.to_csv(r'C:\\Users\\Luis.ParraMorales\\OneDrive - Arup\\Documents\\coef.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LassoCV on degree-2 polynomials with pairwise interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶︎ Best configuration:\n",
      "    • degree = 1\n",
      "    • interactions_only = True\n",
      "    • α = 0.000100\n",
      "▶︎ Cross-val compromise R² (train/val) = 0.7510 / 0.6698\n",
      "▶︎ Final performance:\n",
      "    • Train → RMSE = 8528955.0585, R² = 0.7475\n",
      "    • Test  → RMSE = 9837093.1042, R² = 0.7972\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# ── suppress warnings ─────────────────────────────────────\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# ── re-split train/test ──────────────────────────────────\n",
    "X = baghdad_df[features_full]\n",
    "y = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ── pipeline skeleton ────────────────────────────────────\n",
    "pipe = Pipeline([\n",
    "    ('poly',  PolynomialFeatures(include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lasso', Lasso(max_iter=20000, random_state=42))\n",
    "])\n",
    "\n",
    "# ── hyperparameter grid ──────────────────────────────────\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'poly__interaction_only': [True, False],\n",
    "    'lasso__alpha': np.logspace(-4, 1, 20)\n",
    "}\n",
    "\n",
    "# ── grid-search with train & val scoring ─────────────────\n",
    "gs = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs.fit(X_tr, y_tr)\n",
    "\n",
    "# ── assemble CV results & compromise metric ──────────────\n",
    "cv_df = pd.DataFrame(gs.cv_results_)[[\n",
    "    'param_poly__degree',\n",
    "    'param_poly__interaction_only',\n",
    "    'param_lasso__alpha',\n",
    "    'mean_train_score',\n",
    "    'mean_test_score'\n",
    "]].rename(columns={\n",
    "    'param_poly__degree':'degree',\n",
    "    'param_poly__interaction_only':'interactions_only',\n",
    "    'param_lasso__alpha':'alpha',\n",
    "    'mean_train_score':'r2_train_cv',\n",
    "    'mean_test_score':'r2_val_cv'\n",
    "})\n",
    "cv_df['r2_min'] = cv_df[['r2_train_cv','r2_val_cv']].min(axis=1)\n",
    "\n",
    "best_idx   = cv_df['r2_min'].idxmax()\n",
    "best_cfg   = cv_df.loc[best_idx]\n",
    "best_deg   = best_cfg['degree']\n",
    "best_inter = best_cfg['interactions_only']\n",
    "best_alpha = best_cfg['alpha']\n",
    "\n",
    "# ── refit the compromise model ───────────────────────────\n",
    "best_pipe = Pipeline([\n",
    "    ('poly',  PolynomialFeatures(degree=int(best_deg),\n",
    "                                 interaction_only=bool(best_inter),\n",
    "                                 include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lasso', Lasso(alpha=best_alpha,\n",
    "                    max_iter=20000,\n",
    "                    random_state=42))\n",
    "])\n",
    "best_pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# ── final evaluation ─────────────────────────────────────\n",
    "y_tr_hat = best_pipe.predict(X_tr)\n",
    "y_te_hat = best_pipe.predict(X_te)\n",
    "\n",
    "rmse_tr = np.sqrt(mean_squared_error(y_tr, y_tr_hat))\n",
    "r2_tr   = r2_score(y_tr, y_tr_hat)\n",
    "rmse_te = np.sqrt(mean_squared_error(y_te, y_te_hat))\n",
    "r2_te   = r2_score(y_te, y_te_hat)\n",
    "\n",
    "print(\"▶︎ Best configuration:\")\n",
    "print(f\"    • degree = {best_deg}\")\n",
    "print(f\"    • interactions_only = {best_inter}\")\n",
    "print(f\"    • α = {best_alpha:.6f}\")\n",
    "print(\"▶︎ Cross-val compromise R² (train/val) = \"\n",
    "      f\"{best_cfg.r2_train_cv:.4f} / {best_cfg.r2_val_cv:.4f}\")\n",
    "print(\"▶︎ Final performance:\")\n",
    "print(f\"    • Train → RMSE = {rmse_tr:.4f}, R² = {r2_tr:.4f}\")\n",
    "print(f\"    • Test  → RMSE = {rmse_te:.4f}, R² = {r2_te:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF (TRAIN) → RMSE = 5791056.7935, R² = 0.8836\n",
      "RF (TEST) → RMSE = 10372546.1159, R² = 0.7745\n",
      "Selected RF params: {'max_depth': None, 'max_features': 0.9, 'min_samples_leaf': 5, 'n_estimators': 605}\n",
      "\n",
      "Top-10 SHAP drivers:\n",
      "road_primary_len       3846821.8062\n",
      "poi_count              2325672.0367\n",
      "non_built_area         1744155.0331\n",
      "cropland_a             1713642.4255\n",
      "LST_day_mean           1264173.3052\n",
      "road_len               1035734.7067\n",
      "road_residential_len    769925.1754\n",
      "lu_residential_area     764448.9765\n",
      "pop_sum_m               753231.9807\n",
      "road_motorway_len       668670.4499\n",
      "\n",
      "Local elasticities for one test sample (top-10):\n",
      "LST_day_mean        -0.9786\n",
      "built_up_a          -0.7177\n",
      "wetland_a           -0.5245\n",
      "water_bod_a          0.1236\n",
      "grassland_a          0.1188\n",
      "NTL_mean            -0.0563\n",
      "cropland_a          -0.0375\n",
      "road_trunk_len      -0.0000\n",
      "pop_sum_m           -0.0000\n",
      "road_motorway_len    0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap, warnings, scipy.stats as st\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── 1) Raw-unit feature matrix & target ────────────────────────────────\n",
    "X_rf = baghdad_df[features_full].copy()\n",
    "y_rf = baghdad_df['TCI'].copy()\n",
    "\n",
    "X_tr_rf, X_te_rf, y_tr_rf, y_te_rf = train_test_split(\n",
    "    X_rf, y_rf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ── 2) Randomised hyper-tuning (20 draws) ──────────────────────────────\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators'    : st.randint(400, 1001),        # 400–1000 trees\n",
    "    'max_depth'       : [None, 20, 40],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'max_features'    : ['sqrt', 0.7, 0.9]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions = param_dist,\n",
    "    n_iter      = 20,\n",
    "    cv          = 5,\n",
    "    scoring     = 'r2',\n",
    "    return_train_score = True,\n",
    "    n_jobs      = -1,\n",
    "    random_state= 42\n",
    ")\n",
    "rs.fit(X_tr_rf, y_tr_rf)\n",
    "best_rf = rs.best_estimator_\n",
    "\n",
    "# ── 3) Evaluation ───────────────────────────────────────────────────────\n",
    "for label, X_, y_ in [('TRAIN', X_tr_rf, y_tr_rf),\n",
    "                      ('TEST',  X_te_rf, y_te_rf)]:\n",
    "    pred = best_rf.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"RF ({label}) → RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "print(\"Selected RF params:\", rs.best_params_)\n",
    "\n",
    "# ── 4) SHAP global importances ─────────────────────────────────────────\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "shap_vals = explainer.shap_values(X_te_rf, check_additivity=False)\n",
    "shap_df   = pd.Series(np.abs(shap_vals).mean(axis=0),\n",
    "                      index=X_rf.columns,\n",
    "                      name='mean|SHAP|').sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop-10 SHAP drivers:\")\n",
    "print(shap_df.head(10).to_string(float_format='%.4f'))\n",
    "\n",
    "# ── 5) Elasticity approximation (raw units) ────────────────────────────\n",
    "def elasticity(model, x_row, feature, delta=0.01):\n",
    "    \"\"\"\n",
    "    %Δy / %Δx  using finite difference on raw-unit model.\n",
    "    \"\"\"\n",
    "    x_up = x_row.copy()\n",
    "    bump = x_up[feature] * delta if x_up[feature] != 0 else delta\n",
    "    x_up[feature] += bump\n",
    "    y0 = model.predict(x_row.values.reshape(1, -1))[0]\n",
    "    y1 = model.predict(x_up.values.reshape(1, -1))[0]\n",
    "    if y0 == 0:            # guard against div-by-zero\n",
    "        return np.nan\n",
    "    return ((y1 - y0) / y0) / delta   # elasticity formula\n",
    "\n",
    "sample = X_te_rf.iloc[0]\n",
    "elas = {f: elasticity(best_rf, sample, f) for f in X_rf.columns}\n",
    "elas_df = pd.Series(elas, name='elasticity').sort_values(\n",
    "              key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\nLocal elasticities for one test sample (top-10):\")\n",
    "print(elas_df.head(10).to_string(float_format='%.4f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global elasticity summary (mean, median, 10–90 % deciles)\n",
      "\n",
      "                      mean  median  <lambda>  <lambda>\n",
      "LST_day_mean        3.4976  0.0000   -1.9369   21.0597\n",
      "lu_industrial_area  0.5642  0.0000   -0.0000    0.0000\n",
      "cropland_a          0.5449  0.0000   -1.6741    2.3852\n",
      "water_bod_a         0.3887  0.0000   -0.7366    1.4454\n",
      "non_built_area      0.1529  0.0000   -4.1205    2.0252\n",
      "road_tertiary_len  -0.1477  0.0000   -0.0000    0.0000\n",
      "built_up_a          0.1431  0.0000   -0.6631    0.9639\n",
      "wetland_a           0.1165  0.0000   -0.1189    0.3125\n",
      "pop_sum_m           0.0527  0.0000   -1.2763    0.8835\n",
      "grassland_a         0.0415  0.0000   -0.4009    0.6188\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ##### Global elasticity distribution (RF, raw units)\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def elasticity_matrix(model, X, delta=0.01):\n",
    "    \"\"\"\n",
    "    Vectorised elasticity for an entire sample matrix X.\n",
    "    Returns: array (n_samples, n_features)\n",
    "    ε_ij = ((f(x_i⊕δ_j) – f(x_i)) / f(x_i)) / δ\n",
    "    \"\"\"\n",
    "    y_base = model.predict(X)\n",
    "    X_bump = X.copy()\n",
    "    elas = np.empty_like(X.values, dtype=float)\n",
    "\n",
    "    for j, col in enumerate(X.columns):\n",
    "        bump = X[col].values * delta\n",
    "        bump[bump == 0] = delta        # guard zeros\n",
    "        X_bump[col] = X[col] + bump\n",
    "        y_bump = model.predict(X_bump)\n",
    "        elas[:, j] = ((y_bump - y_base) / y_base) / delta\n",
    "        X_bump[col] = X[col]           # restore\n",
    "\n",
    "    return elas\n",
    "\n",
    "# 1) Compute matrix on the whole test fold\n",
    "E = elasticity_matrix(best_rf, X_te_rf, delta=0.01)   # shape (n_test, n_feat)\n",
    "\n",
    "# 2) Wrap in DataFrame\n",
    "elas_df = pd.DataFrame(E, columns=X_te_rf.columns)\n",
    "\n",
    "# 3) Aggregate statistics\n",
    "summary = (elas_df\n",
    "           .agg(['mean','median',lambda s: s.quantile(0.1),lambda s: s.quantile(0.9)])\n",
    "           .T.rename(columns={'<lambda_0>':'q10','<lambda_1>':'q90'}))\n",
    "\n",
    "# 4) Rank by |mean|\n",
    "summary['abs_mean'] = summary['mean'].abs()\n",
    "summary = summary.sort_values('abs_mean', ascending=False).drop(columns='abs_mean')\n",
    "\n",
    "print(\"Global elasticity summary (mean, median, 10–90 % deciles)\\n\")\n",
    "print(summary.head(10).to_string(float_format='{:.4f}'.format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LightGBM experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1752\n",
      "[LightGBM] [Info] Number of data points in the train set: 403, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 11221479.322727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1752\n",
      "[LightGBM] [Info] Number of data points in the train set: 403, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 11221479.322727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM (TRAIN) → RMSE = 6849906.7318, R² = 0.8371\n",
      "LightGBM (TEST) → RMSE = 9608374.3596, R² = 0.8065\n",
      "\n",
      "Best LightGBM parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 50, 'n_estimators': 100, 'min_child_samples': 50, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1) Prepare raw‐unit dataset\n",
    "X_lgb = baghdad_df[features_full].copy()\n",
    "y_lgb = baghdad_df['TCI'].copy()\n",
    "\n",
    "X_tr_lgb, X_te_lgb, y_tr_lgb, y_te_lgb = train_test_split(\n",
    "    X_lgb, y_lgb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2) Parameter distributions for RandomizedSearch\n",
    "param_dist = {\n",
    "    'num_leaves':        [31, 50, 100],\n",
    "    'max_depth':         [-1, 10, 20],\n",
    "    'learning_rate':     [0.01, 0.05, 0.1],\n",
    "    'n_estimators':      [100, 200, 500],\n",
    "    'min_child_samples': [10, 20, 50],\n",
    "    'subsample':         [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree':  [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgb_reg = lgb.LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "rs_lgb = RandomizedSearchCV(\n",
    "    estimator=lgb_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    return_train_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rs_lgb.fit(X_tr_lgb, y_tr_lgb)\n",
    "\n",
    "best_params_lgb = rs_lgb.best_params_\n",
    "\n",
    "# 3) Refit best model on full training set\n",
    "best_lgb = lgb.LGBMRegressor(**best_params_lgb, random_state=42)\n",
    "best_lgb.fit(X_tr_lgb, y_tr_lgb)\n",
    "\n",
    "# 4) Evaluate on train and test\n",
    "for label, X_e, y_e in [('TRAIN', X_tr_lgb, y_tr_lgb), ('TEST', X_te_lgb, y_te_lgb)]:\n",
    "    y_pred = best_lgb.predict(X_e)\n",
    "    rmse   = np.sqrt(mean_squared_error(y_e, y_pred))\n",
    "    r2     = r2_score(y_e, y_pred)\n",
    "    print(f\"LightGBM ({label}) → RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "print(\"\\nBest LightGBM parameters:\")\n",
    "print(best_params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Performance Metrics\n",
      "              RMSE     R2\n",
      "TRAIN 6849906.7318 0.8371\n",
      "TEST  9608374.3596 0.8065\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_tr_pred = best_lgb.predict(X_tr_lgb)\n",
    "y_te_pred = best_lgb.predict(X_te_lgb)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = pd.DataFrame({\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_tr_lgb, y_tr_pred)),\n",
    "        np.sqrt(mean_squared_error(y_te_lgb, y_te_pred))\n",
    "    ],\n",
    "    'R2': [\n",
    "        r2_score(y_tr_lgb, y_tr_pred),\n",
    "        r2_score(y_te_lgb, y_te_pred)\n",
    "    ]\n",
    "}, index=['TRAIN', 'TEST'])\n",
    "\n",
    "# Display nicely\n",
    "print(\"LightGBM Performance Metrics\")\n",
    "print(metrics.to_string(float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 global drivers by SHAP:\n",
      "road_primary_len      4618026.5990\n",
      "poi_count             3937523.5479\n",
      "LST_day_mean          2114417.4322\n",
      "road_motorway_len     1763763.9300\n",
      "non_built_area        1404821.6684\n",
      "pop_sum_m             1159875.8743\n",
      "lu_residential_area   1131794.4888\n",
      "water_bod_a           1121480.6243\n",
      "road_secondary_len    1039664.0811\n",
      "lu_commercial_area     890669.2143\n",
      "\n",
      "Elasticity summary (percent-response per 1% feature bump):\n",
      "                    mean_elas  median_elas     q10    q90\n",
      "LST_day_mean           1.1533       0.0000 -3.8567 7.1198\n",
      "grassland_a           -0.9425       0.0000  0.0000 0.0000\n",
      "road_primary_len      -0.6787       0.0000  0.0000 0.0000\n",
      "water_bod_a           -0.6063       0.0000  0.0000 0.0000\n",
      "cropland_a            -0.3080       0.0000  0.0000 0.0000\n",
      "built_up_a            -0.2184       0.0000  0.0000 0.0000\n",
      "non_built_area        -0.1665       0.0000  0.0000 0.0000\n",
      "lu_industrial_area    -0.1052       0.0000  0.0000 0.0000\n",
      "NTL_mean              -0.1023       0.0000  0.0000 0.0000\n",
      "road_tertiary_len     -0.0630       0.0000  0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 1) Global SHAP importances\n",
    "explainer = shap.TreeExplainer(best_lgb)\n",
    "shap_vals  = explainer.shap_values(X_te_lgb)\n",
    "# mean absolute impact on model output\n",
    "shap_imp   = pd.Series(np.abs(shap_vals).mean(axis=0),\n",
    "                       index=X_te_lgb.columns,\n",
    "                       name='mean|SHAP|').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top-10 global drivers by SHAP:\")\n",
    "print(shap_imp.head(10).to_string(float_format='%.4f'))\n",
    "\n",
    "# Optional: visual summary\n",
    "# shap.summary_plot(shap_vals, X_te_lgb, plot_type=\"bar\")\n",
    "\n",
    "# 2) Elasticity approximation (finite-difference)\n",
    "def elasticity_pct(model, X, feature, delta=0.01):\n",
    "    \"\"\"\n",
    "    Approximates ε = (%Δy) / (%Δx) for a raw-unit model:\n",
    "      For each row i: bump xi by xi*delta, compute new y,\n",
    "      then (Δy / y) / delta.\n",
    "    Returns vector of local elasticities for each observation.\n",
    "    \"\"\"\n",
    "    x_base = X[feature].values\n",
    "    bump   = np.where(x_base!=0, x_base*delta, delta)\n",
    "    X_up   = X.copy()\n",
    "    X_up[feature] = x_base + bump\n",
    "\n",
    "    y0 = model.predict(X)\n",
    "    y1 = model.predict(X_up)\n",
    "    # avoid /0\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        eps = ((y1 - y0) / y0) / delta\n",
    "    return eps\n",
    "\n",
    "# 3) Compute elasticities for each feature across the test set\n",
    "elas_dict = {}\n",
    "for feat in X_te_lgb.columns:\n",
    "    eps = elasticity_pct(best_lgb, X_te_lgb, feat, delta=0.01)\n",
    "    # summarise: mean, median, 10% & 90% deciles\n",
    "    elas_dict[feat] = [\n",
    "        np.nanmean(eps),\n",
    "        np.nanmedian(eps),\n",
    "        np.nanpercentile(eps, 10),\n",
    "        np.nanpercentile(eps, 90)\n",
    "    ]\n",
    "\n",
    "elas_df = pd.DataFrame.from_dict(\n",
    "    elas_dict,\n",
    "    orient='index',\n",
    "    columns=['mean_elas','median_elas','q10','q90']\n",
    ").sort_values('mean_elas', key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\nElasticity summary (percent-response per 1% feature bump):\")\n",
    "print(elas_df.head(10).to_string(float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dims': [32, 16], 'dropout': 0.1, 'lr': 0.001} -0.46893307931640615 -0.522192213517753\n",
      "{'hidden_dims': [64, 32], 'dropout': 0.2, 'lr': 0.0005} -0.46893307363275527 -0.5221922030759059\n",
      "\n",
      "Best NN Performance:\n",
      "Train R²=-0.4244, Val R²=-0.4689, Test R²=-0.5222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1) Prepare & scale data\n",
    "X = baghdad_df[features_full].values\n",
    "y = baghdad_df['TCI'].values.reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_te, y_val, y_te = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2) DataLoaders\n",
    "def make_loader(X, y, bs, shuffle=False):\n",
    "    ds = TensorDataset(torch.from_numpy(X).float(),\n",
    "                       torch.from_numpy(y).float())\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "# 3) Model with BatchNorm\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dims)-1):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 4) Training routine with early stopping\n",
    "def train_nn(hidden_dims=[32,16], dropout=0.2, lr=1e-3, wd=1e-4,\n",
    "             batch_size=64, epochs=100, patience=10):\n",
    "    dims = [X_tr.shape[1]] + hidden_dims + [1]\n",
    "    model = MLP(dims, dropout).to(device)\n",
    "    opt   = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    sched = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    tr_load = make_loader(X_tr, y_tr, batch_size, shuffle=True)\n",
    "    val_load= make_loader(X_val, y_val, batch_size)\n",
    "\n",
    "    best_val_loss, wait = np.inf, 0\n",
    "    for ep in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xb,yb in tr_load:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss_fn(model(xb), yb).backward()\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_load:\n",
    "                out = model(xb.to(device)).cpu().numpy()\n",
    "                preds.append(out); truths.append(yb.numpy())\n",
    "        val_loss = mean_squared_error(\n",
    "            np.vstack(truths), np.vstack(preds))\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, wait = val_loss, 0\n",
    "            best_weights = model.state_dict()\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(best_weights)\n",
    "    # evaluate on train/val/test\n",
    "    def eval_set(Xs, ys):\n",
    "        yhat = model(torch.from_numpy(Xs).float().to(device)).cpu().detach().numpy()\n",
    "        return (\n",
    "            np.sqrt(mean_squared_error(ys, yhat)),\n",
    "            r2_score(ys, yhat)\n",
    "        )\n",
    "\n",
    "    rmse_tr, r2_tr = eval_set(X_tr, y_tr)\n",
    "    rmse_val, r2_val = eval_set(X_val, y_val)\n",
    "    rmse_te, r2_te = eval_set(X_te, y_te)\n",
    "\n",
    "    return {\n",
    "      'model': model,\n",
    "      'rmse_tr': rmse_tr, 'r2_tr': r2_tr,\n",
    "      'rmse_val': rmse_val, 'r2_val': r2_val,\n",
    "      'rmse_te': rmse_te, 'r2_te': r2_te\n",
    "    }\n",
    "\n",
    "# 5) Quick grid\n",
    "configs = [\n",
    "    {'hidden_dims':[32,16], 'dropout':0.1, 'lr':1e-3},\n",
    "    {'hidden_dims':[64,32], 'dropout':0.2, 'lr':5e-4},\n",
    "]\n",
    "best = None\n",
    "for cfg in configs:\n",
    "    res = train_nn(**cfg)\n",
    "    print(cfg, res['r2_val'], res['r2_te'])\n",
    "    if best is None or res['r2_te']>best['r2_te']:\n",
    "        best = res\n",
    "\n",
    "print(\"\\nBest NN Performance:\")\n",
    "print(f\"Train R²={best['r2_tr']:.4f}, Val R²={best['r2_val']:.4f}, Test R²={best['r2_te']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
