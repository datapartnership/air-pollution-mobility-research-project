{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Code Assiting Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "TBC-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Initial Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell every time you start a new kernel to configure related parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "CURR_PATH = Path().resolve()            # current file path\n",
    "REPO_PATH = CURR_PATH.parent            # current repository path\n",
    "DATA_PATH = REPO_PATH / \"data\"          # path for saving the data\n",
    "DEMO_PATH = DATA_PATH / \"demo-data\"     # path for demo purpose \n",
    "\n",
    "SRC_PATH = REPO_PATH / \"src\"    # path for other sources\n",
    "sys.path.append(str(SRC_PATH))  # add src to system path to import custom functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 NO2 Data Download "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, NO2 pollution data from [Google Earth Engine Sentinel 5P](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_NRTI_L3_NO2) is downloaded, for both Ethiopia and Iraq in country level.\n",
    "\n",
    "From related literature and data quality, we finally decided to use **NO2_column_number_density** as the proxy for NO2 concentration level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function to generate desired time period of NOx data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=uYhqPiWJrMjR7Cv3L-H8pYXp27i9wnzj0zy570RVcJg&tc=mWMhPmg5Kx04rm2-yyI09eUzh8fY19ckcTRWH60rQCg&cc=e_1deqHlvKKFNmcQOdsZ4jbFnb_eyF32R6mnIDQ6rWU>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=uYhqPiWJrMjR7Cv3L-H8pYXp27i9wnzj0zy570RVcJg&tc=mWMhPmg5Kx04rm2-yyI09eUzh8fY19ckcTRWH60rQCg&cc=e_1deqHlvKKFNmcQOdsZ4jbFnb_eyF32R6mnIDQ6rWU</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import ee\n",
    "ee.Authenticate() # For the first Initialization, individual API is needed to log into Google Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Function: generate desired time period of NO2 data  \n",
    "def specific_date(start_date: str, end_date: str, time_resolution: str = 'D') -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of dates within specified time period and resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date: str\n",
    "        Start date, format: 'YYYY-MM-DD'.\n",
    "    - end_date: str\n",
    "        End date, format: 'YYYY-MM-DD'.\n",
    "    - time_resolution: str\n",
    "        Time resolution (e.g., 'D' for daily, 'W' for weekly, 'M' for monthly). Default is 'D'.\n",
    "    \n",
    "    Return:\n",
    "    - dates(list): List of date strings marking the ends of each time segment, format: 'YYYY-MM-DD'.\n",
    "    \n",
    "    \"\"\"\n",
    "    dates = (\n",
    "        pd.date_range(start_date, end_date, freq = time_resolution)\n",
    "        .strftime('%Y-%m-%d')\n",
    "        .tolist()\n",
    "    )\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request tasks to download in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download NO2 data\n",
    "def download_no2_country(country_name: str, dates: list):\n",
    "    \"\"\"\n",
    "    Request NO2 data download from Earth Engine for a specified country and time period\n",
    "\n",
    "    Parameters:\n",
    "    - country_name: str\n",
    "        Name of the target country. Must match the format used by Earth Engine.\n",
    "    - dates: list\n",
    "        List containing the desired time range, (e.g., [start_date, end_date]).\n",
    "\n",
    "    Return:\n",
    "    - None. Sends a/multiple request(s) to Earth Engine to initiate data download.\n",
    "        Exported files are saved under a folder named 'NO2_<country_name>' in first-level Google Drive directory.\n",
    "        Each exported .tiff file is named using its starting date.\n",
    "    \"\"\"\n",
    "    \n",
    "    countries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n",
    "    country = countries.filter(ee.Filter.eq('country_na', country_name)).geometry()\n",
    "\n",
    "    n_dates = len(dates)\n",
    "\n",
    "    for i in range(n_dates-1):\n",
    "\n",
    "        date_start, date_end = dates[i], dates[i+1]\n",
    "\n",
    "        no2 = (ee.ImageCollection('COPERNICUS/S5P/NRTI/L3_NO2')\n",
    "            .select('tropospheric_NO2_column_number_density')\n",
    "            .filterDate(date_start, date_end)\n",
    "            .mean())\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=no2,\n",
    "            description=f'{country_name}_NO2_{date_start}_{date_end}',\n",
    "            folder=f'NO2_{country_name}',\n",
    "            fileNamePrefix=f'{country_name}_NO2_{date_start}',\n",
    "            region=country,\n",
    "            scale=1000,\n",
    "            maxPixels=1e13\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            task.start()\n",
    "            print(f'{country_name}: The export task for {date_start} is ongoing, please check the results in Google Drive.')\n",
    "        except Exception as e:\n",
    "            print(f'Fail to submit task: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download EVI data\n",
    "def download_EVI_country(country_name: str, dates: list):\n",
    "    \"\"\"\n",
    "    Request NO2 data download from Earth Engine for a specified country and time period\n",
    "\n",
    "    Parameters:\n",
    "    - country_name: str\n",
    "        Name of the target country. Must match the format used by Earth Engine.\n",
    "    - dates: list\n",
    "        List containing the desired time range, (e.g., [start_date, end_date]).\n",
    "\n",
    "    Return:\n",
    "    - None. Sends a/multiple request(s) to Earth Engine to initiate data download.\n",
    "        Exported files are saved under a folder named 'NO2_<country_name>' in first-level Google Drive directory.\n",
    "        Each exported .tiff file is named using its starting date.\n",
    "    \"\"\"\n",
    "    \n",
    "    countries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n",
    "    country = countries.filter(ee.Filter.eq('country_na', country_name)).geometry()\n",
    "\n",
    "    n_dates = len(dates)\n",
    "\n",
    "    for i in range(n_dates-1):\n",
    "\n",
    "        date_start, date_end = dates[i], dates[i+1]\n",
    "\n",
    "        no2 = (ee.ImageCollection('MODIS/MOD09GA_006_EVI')\n",
    "            .select('EVI')\n",
    "            .filterDate(date_start, date_end)\n",
    "            .mean())\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=no2,\n",
    "            description=f'{country_name}_NO2_{date_start}_{date_end}',\n",
    "            folder=f'NO2_{country_name}',\n",
    "            fileNamePrefix=f'{country_name}_NO2_{date_start}',\n",
    "            region=country,\n",
    "            scale=1000,\n",
    "            maxPixels=1e13\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            task.start()\n",
    "            print(f'{country_name}: The export task for {date_start} is ongoing, please check the results in Google Drive.')\n",
    "        except Exception as e:\n",
    "            print(f'Fail to submit task: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Call and Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = specific_date('2023-01-01', '2024-12-31')\n",
    "len(dates) # 731\n",
    "\n",
    "# Download Ethiopia NO2 Data\n",
    "download_no2_country('Ethiopia', dates)\n",
    "\n",
    "# Download Iraq NO2 Data\n",
    "download_no2_country('Iraq', dates)\n",
    "\n",
    "# Download Ethiopia EVI Data\n",
    "download_EVI_country('Ethiopia', dates)\n",
    "\n",
    "# Download Iraq EVI Data\n",
    "download_EVI_country('Iraq', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 OSM Data Download\n",
    "\n",
    "Including code to download data from OpenStreetMap(OSM), [OSM Ethiopia](https://download.geofabrik.de/africa/ethiopia-latest-free.shp.zip) and [OSM Iraq](https://download.geofabrik.de/asia/iraq-latest-free.shp.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install & import libraries, define folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import osm2geojson\n",
    "import requests\n",
    "import urllib3\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base data directory and subfolders\n",
    "base_dir = Path(r\"C:\\Users\\Luis.ParraMorales\\OneDrive - Imperial College London\\Group Design Project\\Data\")\n",
    "folders = {\n",
    "    \"boundaries\": base_dir / \"boundaries\",\n",
    "    \"roads\":      base_dir / \"roads\",\n",
    "    \"industry\":   base_dir / \"industry\",\n",
    "    \"energy\":     base_dir / \"energy\",\n",
    "}\n",
    "for path in folders.values():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# OSMnx settings\n",
    "ox.settings.use_cache        = True\n",
    "ox.settings.log_console      = True\n",
    "ox.settings.requests_kwargs  = {\"verify\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country/city boundaries (Ethiopia and Baghdad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define queries\n",
    "areas = {\n",
    "    \"ethiopia\": \"Ethiopia, Africa\",\n",
    "    \"baghdad\":  \"Baghdad, Iraq\",\n",
    "}\n",
    "\n",
    "# Dictionary to hold geometry polygons\n",
    "boundaries = {}\n",
    "\n",
    "for name, query in areas.items():\n",
    "    print(f\"Fetching boundary for {name}...\")\n",
    "    gdf = ox.geocode_to_gdf(query)\n",
    "    poly = gdf.loc[0, \"geometry\"]\n",
    "    boundaries[name] = poly\n",
    "    # save as shapefile\n",
    "    out_fp = folders[\"boundaries\"] / f\"{name}_boundary.shp\"\n",
    "    gdf.to_file(out_fp)\n",
    "    print(f\"Saved boundary to {out_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Road networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Ethiopia subregions\n",
    "subregs = gpd.read_file(folders[\"boundaries\"] / \"ethiopia_subregions.gpkg\")\n",
    "\n",
    "# Define the road filter\n",
    "road_types = [\"motorway\",\"trunk\",\"primary\",\"secondary\",\"tertiary\"]\n",
    "filter_str = f'[\"highway\"~\"^({\"|\".join(road_types)})$\"]'\n",
    "\n",
    "ethi_roads_parts = []\n",
    "for _, row in subregs.iterrows():\n",
    "    region_name = row[\"region_name\"]\n",
    "    poly = row[\"geometry\"]\n",
    "    print(f\"Fetching roads for Ethiopia – {region_name}…\")\n",
    "    try:\n",
    "        G = ox.graph_from_polygon(poly, custom_filter=filter_str)\n",
    "        roads = ox.graph_to_gdfs(G, nodes=False, edges=True, fill_edge_geometry=True)\n",
    "        roads[\"region_name\"] = region_name\n",
    "        ethi_roads_parts.append(roads)\n",
    "    except Exception as e:\n",
    "        print(f\"   skipped {region_name}: {e}\")\n",
    "\n",
    "# Concatenate and save Ethiopia roads\n",
    "ethi_roads = pd.concat(ethi_roads_parts, ignore_index=True)\n",
    "out_fp_eth = folders[\"roads\"] / \"ethiopia_roads.shp\"\n",
    "ethi_roads.to_file(out_fp_eth)\n",
    "print(f\"Saved Ethiopia roads to {out_fp_eth}\")\n",
    "\n",
    "# Fetch Baghdad’s roads\n",
    "print(\"📥 Fetching roads for Baghdad…\")\n",
    "G_bag = ox.graph_from_place(\"Baghdad, Iraq\", custom_filter=filter_str)\n",
    "bag_roads = ox.graph_to_gdfs(G_bag, nodes=False, edges=True, fill_edge_geometry=True)\n",
    "out_fp_bag = folders[\"roads\"] / \"baghdad_roads.shp\"\n",
    "bag_roads.to_file(out_fp_bag)\n",
    "print(f\"Saved Baghdad roads to {out_fp_bag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industrial features & power plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) SSL off & HTTP endpoints\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "ox.settings.requests_kwargs    = {\"verify\": False}\n",
    "ox.settings.nominatim_endpoint = \"http://nominatim.openstreetmap.org/search\"\n",
    "ox.settings.overpass_endpoint  = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# 2) Ensure poi folder exists\n",
    "folders[\"poi\"] = folders.get(\"poi\", folders[\"industry\"].parent / \"poi\")\n",
    "folders[\"poi\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) Tags for POIs\n",
    "poi_tags = {\n",
    "    \"amenity\": [\n",
    "        \"bus_station\",\"bus_stop\",\"parking\",\"fuel\",\"marketplace\",\n",
    "        \"school\",\"college\",\"university\",\"hospital\",\"clinic\",\n",
    "        \"bank\",\"restaurant\",\"cafe\",\"fast_food\",\"bar\",\"police\",\"fire_station\"\n",
    "    ],\n",
    "    \"shop\": True,\n",
    "    \"highway\": [\"bus_stop\",\"bus_station\"],\n",
    "    \"railway\": [\"station\",\"halt\",\"tram_stop\"],\n",
    "    \"aeroway\": [\"aerodrome\",\"helipad\",\"terminal\"],\n",
    "    \"landuse\": [\"industrial\"],\n",
    "    \"man_made\": [\"works\",\"chimney\",\"storage_tank\"],\n",
    "    \"power\": [\"plant\",\"substation\",\"generator\",\"tower\",\"transformer\"],\n",
    "    \"office\": True,\n",
    "    \"craft\": True,\n",
    "    \"place\": [\"city\",\"town\",\"village\",\"suburb\",\"neighbourhood\",\"hamlet\"],\n",
    "}\n",
    "\n",
    "# 4) Ethiopia – loop per subregion\n",
    "subregs    = gpd.read_file(folders[\"boundaries\"] / \"ethiopia_subregions.gpkg\")\n",
    "ethi_parts = []\n",
    "\n",
    "for _, row in subregs.iterrows():\n",
    "    region = row[\"region_name\"]\n",
    "    poly   = row.geometry\n",
    "    print(f\"📥 Fetching POIs for Ethiopia – {region} …\")\n",
    "    try:\n",
    "        gdf = ox.features_from_polygon(poly, tags=poi_tags)\n",
    "        if gdf.empty:\n",
    "            continue\n",
    "        # convert all non-Points to centroids\n",
    "        gdf[\"geometry\"] = gdf.geometry.apply(\n",
    "            lambda g: g if isinstance(g, Point) else g.centroid\n",
    "        )\n",
    "        gdf[\"region_name\"] = region\n",
    "        ethi_parts.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipped {region}: {e}\")\n",
    "\n",
    "if ethi_parts:\n",
    "    ethi_pois = pd.concat(ethi_parts, ignore_index=True).set_crs(\"EPSG:4326\")\n",
    "    # drop duplicate columns\n",
    "    ethi_pois = ethi_pois.loc[:, ~ethi_pois.columns.duplicated()]\n",
    "    # drop any fixme column\n",
    "    for bad in [\"fixme\", \"FIXME\"]:\n",
    "        if bad in ethi_pois.columns:\n",
    "            ethi_pois = ethi_pois.drop(columns=bad)\n",
    "    out_eth = folders[\"poi\"] / \"ethiopia_pois.gpkg\"\n",
    "    ethi_pois.to_file(out_eth, driver=\"GPKG\")\n",
    "    print(f\"Saved Ethiopia POIs to {out_eth}\")\n",
    "else:\n",
    "    print(\"No Ethiopia POIs fetched.\")\n",
    "\n",
    "# 5) Baghdad – single call\n",
    "print(\"Fetching POIs for Baghdad …\")\n",
    "bag_poly = boundaries.get(\"baghdad\") or ox.geocode_to_gdf(\"Baghdad, Iraq\").geometry.iloc[0]\n",
    "\n",
    "try:\n",
    "    bag_pois = ox.features_from_polygon(bag_poly, tags=poi_tags)\n",
    "    bag_pois[\"geometry\"] = bag_pois.geometry.apply(\n",
    "        lambda g: g if isinstance(g, Point) else g.centroid\n",
    "    )\n",
    "    bag_pois[\"region_name\"] = \"Baghdad\"\n",
    "    # drop duplicate columns\n",
    "    bag_pois = bag_pois.loc[:, ~bag_pois.columns.duplicated()]\n",
    "    # drop any fixme column\n",
    "    if \"fixme\" in bag_pois.columns:\n",
    "        bag_pois = bag_pois.drop(columns=\"fixme\")\n",
    "    out_bag = folders[\"poi\"] / \"baghdad_pois.gpkg\"\n",
    "    bag_pois.to_file(out_bag, driver=\"GPKG\")\n",
    "    print(f\"Saved Baghdad POIs to {out_bag}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch Baghdad POIs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy-grid components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Disable SSL verification & warnings, force HTTP endpoints\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "ox.settings.requests_kwargs    = {\"verify\": False}\n",
    "ox.settings.nominatim_endpoint = \"http://nominatim.openstreetmap.org/search\"\n",
    "ox.settings.overpass_endpoint  = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# 2) Ensure energy folder exists\n",
    "folders[\"energy\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) Define tags for grid components\n",
    "grid_tags = {\n",
    "    \"power\": [\"line\", \"substation\", \"transformer\", \"tower\"]\n",
    "}\n",
    "\n",
    "# 4) Ethiopia – loop per admin_level=4 subregion\n",
    "subregs   = gpd.read_file(folders[\"boundaries\"] / \"ethiopia_subregions.gpkg\")\n",
    "eth_parts = []\n",
    "\n",
    "for _, row in subregs.iterrows():\n",
    "    region   = row[\"region_name\"]\n",
    "    poly     = row.geometry\n",
    "    print(f\"Fetching energy‐grid for Ethiopia – {region} …\")\n",
    "    try:\n",
    "        gdf = ox.features_from_polygon(poly, tags=grid_tags)\n",
    "        if gdf.empty:\n",
    "            continue\n",
    "        # tag the region\n",
    "        gdf[\"region_name\"] = region\n",
    "        # drop any duplicate columns\n",
    "        gdf = gdf.loc[:, ~gdf.columns.duplicated()]\n",
    "        # drop problematic 'fixme' field if present\n",
    "        for bad in [\"fixme\", \"FIXME\"]:\n",
    "            if bad in gdf.columns:\n",
    "                gdf = gdf.drop(columns=bad)\n",
    "        eth_parts.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipped {region}: {e}\")\n",
    "\n",
    "# 5) Save Ethiopia grid\n",
    "if eth_parts:\n",
    "    eth_grid = pd.concat(eth_parts, ignore_index=True).set_crs(\"EPSG:4326\")\n",
    "    out_eth = folders[\"energy\"] / \"ethiopia_energy_grid.gpkg\"\n",
    "    eth_grid.to_file(out_eth, driver=\"GPKG\")\n",
    "    print(f\"Saved Ethiopia energy grid to {out_eth}\")\n",
    "else:\n",
    "    print(\"No Ethiopia energy‐grid features fetched.\")\n",
    "\n",
    "# 6) Baghdad – single call\n",
    "print(\"Fetching energy‐grid for Baghdad …\")\n",
    "bag_poly = boundaries.get(\"baghdad\") or ox.geocode_to_gdf(\"Baghdad, Iraq\").geometry.iloc[0]\n",
    "\n",
    "try:\n",
    "    bag_gdf = ox.features_from_polygon(bag_poly, tags=grid_tags)\n",
    "    bag_gdf[\"region_name\"] = \"Baghdad\"\n",
    "    bag_gdf = bag_gdf.loc[:, ~bag_gdf.columns.duplicated()]\n",
    "    if \"fixme\" in bag_gdf.columns:\n",
    "        bag_gdf = bag_gdf.drop(columns=\"fixme\")\n",
    "    out_bag = folders[\"energy\"] / \"baghdad_energy_grid.gpkg\"\n",
    "    bag_gdf.to_file(out_bag, driver=\"GPKG\")\n",
    "    print(f\"Saved Baghdad energy grid to {out_bag}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch Baghdad energy‐grid: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Other Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate Meshes for filling features\n",
    "\n",
    "Generate meshes, from 2023-01-01 to 2024-12-31, one mesh for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Generating meshes for Addis Ababa!\n",
      "Complete Generating meshes for Baghdad!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import fiona\n",
    "\n",
    "mesh_addis = data_root / \"mesh-grid\" / \"grid_addis_ababa.gpkg\"\n",
    "mesh_baghdad = data_root / \"mesh-grid\" / \"grid_baghdad.gpkg\"\n",
    "\n",
    "lyr_addis_name = fiona.listlayers(mesh_addis)[0]  # control layer number\n",
    "lyr_baghdad_name = fiona.listlayers(mesh_baghdad)[0]\n",
    "\n",
    "# start and end date\n",
    "start_date = datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2024-12-31\", \"%Y-%m-%d\")\n",
    "\n",
    "addis_meshes_path = data_root / 'addis-empty-mesh-data'\n",
    "baghdad_meshes_path = data_root / 'baghdad-empty-mesh-data'\n",
    "\n",
    "addis_meshes_path.mkdir(exist_ok=True)\n",
    "baghdad_meshes_path.mkdir(exist_ok=True)\n",
    "\n",
    "delta = end_date - start_date\n",
    "days_count = delta.days + 1\n",
    "\n",
    "# For Addis Ababa\n",
    "for i in range(days_count):\n",
    "    current_date = start_date + timedelta(days=i)\n",
    "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"addis-ababa-{date_str}.gpkg\"\n",
    "    dest_path = addis_meshes_path / filename\n",
    "\n",
    "    shutil.copy(mesh_addis, dest_path)\n",
    "\n",
    "print(f\"Complete Generating meshes for Addis Ababa!\")\n",
    "\n",
    "# For Baghdad\n",
    "for i in range(days_count):\n",
    "    current_date = start_date + timedelta(days=i)\n",
    "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"baghdad-{date_str}.gpkg\"\n",
    "    dest_path = baghdad_meshes_path / filename\n",
    "\n",
    "    shutil.copy(mesh_baghdad, dest_path)\n",
    "\n",
    "\n",
    "print(f\"Complete Generating meshes for Baghdad!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate Date Tables for Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date    Weekday Ethiopia_Workday_Type Iraq_Workday_Type\n",
      "0 2023-01-01     Sunday              Weekends           Thu-Sun\n",
      "1 2023-01-02     Monday              Workdays       Mon-Tue-Wed\n",
      "2 2023-01-03    Tuesday              Workdays       Mon-Tue-Wed\n",
      "3 2023-01-04  Wednesday              Workdays       Mon-Tue-Wed\n",
      "4 2023-01-05   Thursday              Workdays           Thu-Sun\n",
      "5 2023-01-06     Friday              Workdays           Fri-Sat\n",
      "6 2023-01-07   Saturday              Weekends           Fri-Sat\n",
      "7 2023-01-08     Sunday              Weekends           Thu-Sun\n",
      "8 2023-01-09     Monday              Workdays       Mon-Tue-Wed\n",
      "9 2023-01-10    Tuesday              Workdays       Mon-Tue-Wed\n"
     ]
    }
   ],
   "source": [
    "# Generate Standard Date Table\n",
    "import pandas as pd\n",
    "\n",
    "# Generate date range\n",
    "date_range = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')\n",
    "df = pd.DataFrame({'Date': date_range})\n",
    "df['Weekday'] = df['Date'].dt.day_name()\n",
    "\n",
    "# Define Ethiopia workday type: \n",
    "# Mon-Fri -> \"Workdays\", Sat-Sun -> \"Weekends\"\n",
    "df['Ethiopia_Workday_Type'] = df['Weekday'].apply(\n",
    "    lambda x: \"Workdays\" if x in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] else \"Weekends\"\n",
    ")\n",
    "\n",
    "# Define Iraq workday type:\n",
    "# Mon-Tue-Wed -> 'Mon-Tue-Wed', Fri-Sat -> 'Fri-Sat', Thu-Sun -> 'Thu-Sun'\n",
    "iraq_workday_type = {\n",
    "    'Monday': 'Mon-Tue-Wed', 'Tuesday': 'Mon-Tue-Wed', 'Wednesday': 'Mon-Tue-Wed',\n",
    "    'Friday': 'Fri-Sat', 'Saturday': 'Fri-Sat',\n",
    "    'Thursday': 'Thu-Sun', 'Sunday': 'Thu-Sun'\n",
    "}\n",
    "df['Iraq_Workday_Type'] = df['Weekday'].map(iraq_workday_type)\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['Date', 'Weekday', 'Ethiopia_Workday_Type', 'Iraq_Workday_Type']]\n",
    "df.to_csv(data_root / 'helper-files' / 'workday_type_2023_2024.csv', index=False, encoding='utf-8-sig')  # save the data if needed\n",
    "\n",
    "# Preview first few rows\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mesh File to CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change gpkg format to csv format. \n",
    "\n",
    "Addis Ababa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Addis Ababa\n",
    "file_path = DATA_PATH / 'addis-mesh-data'\n",
    "output_path = DATA_PATH / 'addis-mesh-data-csv'\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "files = list(file_path.glob(\"*.gpkg\"))\n",
    "\n",
    "for file in files:\n",
    "    name = file.stem\n",
    "    print(name)\n",
    "    gdf = gpd.read_file(file)\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda geom: geom.wkt if geom else None)\n",
    "    gdf.to_csv(output_path / f\"{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baghdad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = DATA_PATH / 'baghdad-mesh-data'\n",
    "output_path = DATA_PATH / 'baghdad-mesh-data-csv'\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "files = list(file_path.glob(\"*.gpkg\"))\n",
    "# files\n",
    "\n",
    "for file in files:\n",
    "    name = file.stem\n",
    "    print(name)\n",
    "    gdf = gpd.read_file(file)\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda geom: geom.wkt if geom else None)\n",
    "    gdf.to_csv(output_path / f\"{name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
