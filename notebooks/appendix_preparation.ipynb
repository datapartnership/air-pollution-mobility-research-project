{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Code Assiting Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "TBC-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Initial Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell every time you start a new kernel to configure related parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "CURR_PATH = Path().resolve()            # current file path\n",
    "REPO_PATH = CURR_PATH.parent            # current repository path\n",
    "DATA_PATH = REPO_PATH / \"data\"          # path for saving the data\n",
    "DEMO_PATH = DATA_PATH / \"demo-data\"     # path for demo purpose \n",
    "\n",
    "SRC_PATH = REPO_PATH / \"src\"    # path for other sources\n",
    "sys.path.append(str(SRC_PATH))  # add src to system path to import custom functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 NO2 Data Download "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, NO2 pollution data from [Google Earth Engine Sentinel 5P](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_NRTI_L3_NO2) is downloaded, for both Ethiopia and Iraq in country level.\n",
    "\n",
    "From related literature and data quality, we finally decided to use **NO2_column_number_density** as the proxy for NO2 concentration level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function to generate desired time period of NOx data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=uYhqPiWJrMjR7Cv3L-H8pYXp27i9wnzj0zy570RVcJg&tc=mWMhPmg5Kx04rm2-yyI09eUzh8fY19ckcTRWH60rQCg&cc=e_1deqHlvKKFNmcQOdsZ4jbFnb_eyF32R6mnIDQ6rWU>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=uYhqPiWJrMjR7Cv3L-H8pYXp27i9wnzj0zy570RVcJg&tc=mWMhPmg5Kx04rm2-yyI09eUzh8fY19ckcTRWH60rQCg&cc=e_1deqHlvKKFNmcQOdsZ4jbFnb_eyF32R6mnIDQ6rWU</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import ee\n",
    "ee.Authenticate() # For the first Initialization, individual API is needed to log into Google Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Function: generate desired time period of NO2 data  \n",
    "def specific_date(start_date: str, end_date: str, time_resolution: str = 'D') -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of dates within specified time period and resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date: str\n",
    "        Start date, format: 'YYYY-MM-DD'.\n",
    "    - end_date: str\n",
    "        End date, format: 'YYYY-MM-DD'.\n",
    "    - time_resolution: str\n",
    "        Time resolution (e.g., 'D' for daily, 'W' for weekly, 'M' for monthly). Default is 'D'.\n",
    "    \n",
    "    Return:\n",
    "    - dates(list): List of date strings marking the ends of each time segment, format: 'YYYY-MM-DD'.\n",
    "    \n",
    "    \"\"\"\n",
    "    dates = (\n",
    "        pd.date_range(start_date, end_date, freq = time_resolution)\n",
    "        .strftime('%Y-%m-%d')\n",
    "        .tolist()\n",
    "    )\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request tasks to download in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download NO2 data\n",
    "def download_no2_country(country_name: str, dates: list):\n",
    "    \"\"\"\n",
    "    Request NO2 data download from Earth Engine for a specified country and time period\n",
    "\n",
    "    Parameters:\n",
    "    - country_name: str\n",
    "        Name of the target country. Must match the format used by Earth Engine.\n",
    "    - dates: list\n",
    "        List containing the desired time range, (e.g., [start_date, end_date]).\n",
    "\n",
    "    Return:\n",
    "    - None. Sends a/multiple request(s) to Earth Engine to initiate data download.\n",
    "        Exported files are saved under a folder named 'NO2_<country_name>' in first-level Google Drive directory.\n",
    "        Each exported .tiff file is named using its starting date.\n",
    "    \"\"\"\n",
    "    \n",
    "    countries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n",
    "    country = countries.filter(ee.Filter.eq('country_na', country_name)).geometry()\n",
    "\n",
    "    n_dates = len(dates)\n",
    "\n",
    "    for i in range(n_dates-1):\n",
    "\n",
    "        date_start, date_end = dates[i], dates[i+1]\n",
    "\n",
    "        no2 = (ee.ImageCollection('COPERNICUS/S5P/NRTI/L3_NO2')\n",
    "            .select('tropospheric_NO2_column_number_density')\n",
    "            .filterDate(date_start, date_end)\n",
    "            .mean())\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=no2,\n",
    "            description=f'{country_name}_NO2_{date_start}_{date_end}',\n",
    "            folder=f'NO2_{country_name}',\n",
    "            fileNamePrefix=f'{country_name}_NO2_{date_start}',\n",
    "            region=country,\n",
    "            scale=1000,\n",
    "            maxPixels=1e13\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            task.start()\n",
    "            print(f'{country_name}: The export task for {date_start} is ongoing, please check the results in Google Drive.')\n",
    "        except Exception as e:\n",
    "            print(f'Fail to submit task: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download EVI data\n",
    "def download_EVI_country(country_name: str, dates: list):\n",
    "    \"\"\"\n",
    "    Request NO2 data download from Earth Engine for a specified country and time period\n",
    "\n",
    "    Parameters:\n",
    "    - country_name: str\n",
    "        Name of the target country. Must match the format used by Earth Engine.\n",
    "    - dates: list\n",
    "        List containing the desired time range, (e.g., [start_date, end_date]).\n",
    "\n",
    "    Return:\n",
    "    - None. Sends a/multiple request(s) to Earth Engine to initiate data download.\n",
    "        Exported files are saved under a folder named 'NO2_<country_name>' in first-level Google Drive directory.\n",
    "        Each exported .tiff file is named using its starting date.\n",
    "    \"\"\"\n",
    "    \n",
    "    countries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n",
    "    country = countries.filter(ee.Filter.eq('country_na', country_name)).geometry()\n",
    "\n",
    "    n_dates = len(dates)\n",
    "\n",
    "    for i in range(n_dates-1):\n",
    "\n",
    "        date_start, date_end = dates[i], dates[i+1]\n",
    "\n",
    "        no2 = (ee.ImageCollection('MODIS/MOD09GA_006_EVI')\n",
    "            .select('EVI')\n",
    "            .filterDate(date_start, date_end)\n",
    "            .mean())\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=no2,\n",
    "            description=f'{country_name}_NO2_{date_start}_{date_end}',\n",
    "            folder=f'NO2_{country_name}',\n",
    "            fileNamePrefix=f'{country_name}_NO2_{date_start}',\n",
    "            region=country,\n",
    "            scale=1000,\n",
    "            maxPixels=1e13\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            task.start()\n",
    "            print(f'{country_name}: The export task for {date_start} is ongoing, please check the results in Google Drive.')\n",
    "        except Exception as e:\n",
    "            print(f'Fail to submit task: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Call and Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = specific_date('2023-01-01', '2024-12-31')\n",
    "len(dates) # 731\n",
    "\n",
    "# Download Ethiopia NO2 Data\n",
    "download_no2_country('Ethiopia', dates)\n",
    "\n",
    "# Download Iraq NO2 Data\n",
    "download_no2_country('Iraq', dates)\n",
    "\n",
    "# Download Ethiopia EVI Data\n",
    "download_EVI_country('Ethiopia', dates)\n",
    "\n",
    "# Download Iraq EVI Data\n",
    "download_EVI_country('Iraq', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 OSM Data Download\n",
    "\n",
    "Including code to download data from OpenStreetMap(OSM), [OSM Ethiopia](https://download.geofabrik.de/africa/ethiopia-latest-free.shp.zip) and [OSM Iraq](https://download.geofabrik.de/asia/iraq-latest-free.shp.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install & import libraries, define folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import osm2geojson\n",
    "import requests\n",
    "import urllib3\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base data directory and subfolders\n",
    "base_dir = Path(r\"C:\\Users\\Luis.ParraMorales\\OneDrive - Imperial College London\\Group Design Project\\Data\")\n",
    "folders = {\n",
    "    \"boundaries\": base_dir / \"boundaries\",\n",
    "    \"roads\":      base_dir / \"roads\",\n",
    "    \"industry\":   base_dir / \"industry\",\n",
    "    \"energy\":     base_dir / \"energy\",\n",
    "}\n",
    "for path in folders.values():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# OSMnx settings\n",
    "ox.settings.use_cache        = True\n",
    "ox.settings.log_console      = True\n",
    "ox.settings.requests_kwargs  = {\"verify\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country/city boundaries (Ethiopia and Baghdad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define queries\n",
    "areas = {\n",
    "    \"ethiopia\": \"Ethiopia, Africa\",\n",
    "    \"baghdad\":  \"Baghdad, Iraq\",\n",
    "}\n",
    "\n",
    "# Dictionary to hold geometry polygons\n",
    "boundaries = {}\n",
    "\n",
    "for name, query in areas.items():\n",
    "    print(f\"Fetching boundary for {name}...\")\n",
    "    gdf = ox.geocode_to_gdf(query)\n",
    "    poly = gdf.loc[0, \"geometry\"]\n",
    "    boundaries[name] = poly\n",
    "    # save as shapefile\n",
    "    out_fp = folders[\"boundaries\"] / f\"{name}_boundary.shp\"\n",
    "    gdf.to_file(out_fp)\n",
    "    print(f\"Saved boundary to {out_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Road networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Ethiopia subregions\n",
    "subregs = gpd.read_file(folders[\"boundaries\"] / \"ethiopia_subregions.gpkg\")\n",
    "\n",
    "# Define the road filter\n",
    "road_types = [\"motorway\",\"trunk\",\"primary\",\"secondary\",\"tertiary\"]\n",
    "filter_str = f'[\"highway\"~\"^({\"|\".join(road_types)})$\"]'\n",
    "\n",
    "ethi_roads_parts = []\n",
    "for _, row in subregs.iterrows():\n",
    "    region_name = row[\"region_name\"]\n",
    "    poly = row[\"geometry\"]\n",
    "    print(f\"Fetching roads for Ethiopia – {region_name}…\")\n",
    "    try:\n",
    "        G = ox.graph_from_polygon(poly, custom_filter=filter_str)\n",
    "        roads = ox.graph_to_gdfs(G, nodes=False, edges=True, fill_edge_geometry=True)\n",
    "        roads[\"region_name\"] = region_name\n",
    "        ethi_roads_parts.append(roads)\n",
    "    except Exception as e:\n",
    "        print(f\"   skipped {region_name}: {e}\")\n",
    "\n",
    "# Concatenate and save Ethiopia roads\n",
    "ethi_roads = pd.concat(ethi_roads_parts, ignore_index=True)\n",
    "out_fp_eth = folders[\"roads\"] / \"ethiopia_roads.shp\"\n",
    "ethi_roads.to_file(out_fp_eth)\n",
    "print(f\"Saved Ethiopia roads to {out_fp_eth}\")\n",
    "\n",
    "# Fetch Baghdad’s roads\n",
    "print(\"📥 Fetching roads for Baghdad…\")\n",
    "G_bag = ox.graph_from_place(\"Baghdad, Iraq\", custom_filter=filter_str)\n",
    "bag_roads = ox.graph_to_gdfs(G_bag, nodes=False, edges=True, fill_edge_geometry=True)\n",
    "out_fp_bag = folders[\"roads\"] / \"baghdad_roads.shp\"\n",
    "bag_roads.to_file(out_fp_bag)\n",
    "print(f\"Saved Baghdad roads to {out_fp_bag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industrial features & power plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) SSL off & HTTP endpoints\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "ox.settings.requests_kwargs    = {\"verify\": False}\n",
    "ox.settings.nominatim_endpoint = \"http://nominatim.openstreetmap.org/search\"\n",
    "ox.settings.overpass_endpoint  = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# 2) Ensure poi folder exists\n",
    "folders[\"poi\"] = folders.get(\"poi\", folders[\"industry\"].parent / \"poi\")\n",
    "folders[\"poi\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) Tags for POIs\n",
    "poi_tags = {\n",
    "    \"amenity\": [\n",
    "        \"bus_station\",\"bus_stop\",\"parking\",\"fuel\",\"marketplace\",\n",
    "        \"school\",\"college\",\"university\",\"hospital\",\"clinic\",\n",
    "        \"bank\",\"restaurant\",\"cafe\",\"fast_food\",\"bar\",\"police\",\"fire_station\"\n",
    "    ],\n",
    "    \"shop\": True,\n",
    "    \"highway\": [\"bus_stop\",\"bus_station\"],\n",
    "    \"railway\": [\"station\",\"halt\",\"tram_stop\"],\n",
    "    \"aeroway\": [\"aerodrome\",\"helipad\",\"terminal\"],\n",
    "    \"landuse\": [\"industrial\"],\n",
    "    \"man_made\": [\"works\",\"chimney\",\"storage_tank\"],\n",
    "    \"power\": [\"plant\",\"substation\",\"generator\",\"tower\",\"transformer\"],\n",
    "    \"office\": True,\n",
    "    \"craft\": True,\n",
    "    \"place\": [\"city\",\"town\",\"village\",\"suburb\",\"neighbourhood\",\"hamlet\"],\n",
    "}\n",
    "\n",
    "# 4) Ethiopia – loop per subregion\n",
    "subregs    = gpd.read_file(folders[\"boundaries\"] / \"ethiopia_subregions.gpkg\")\n",
    "ethi_parts = []\n",
    "\n",
    "for _, row in subregs.iterrows():\n",
    "    region = row[\"region_name\"]\n",
    "    poly   = row.geometry\n",
    "    print(f\"📥 Fetching POIs for Ethiopia – {region} …\")\n",
    "    try:\n",
    "        gdf = ox.features_from_polygon(poly, tags=poi_tags)\n",
    "        if gdf.empty:\n",
    "            continue\n",
    "        # convert all non-Points to centroids\n",
    "        gdf[\"geometry\"] = gdf.geometry.apply(\n",
    "            lambda g: g if isinstance(g, Point) else g.centroid\n",
    "        )\n",
    "        gdf[\"region_name\"] = region\n",
    "        ethi_parts.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipped {region}: {e}\")\n",
    "\n",
    "if ethi_parts:\n",
    "    ethi_pois = pd.concat(ethi_parts, ignore_index=True).set_crs(\"EPSG:4326\")\n",
    "    # drop duplicate columns\n",
    "    ethi_pois = ethi_pois.loc[:, ~ethi_pois.columns.duplicated()]\n",
    "    # drop any fixme column\n",
    "    for bad in [\"fixme\", \"FIXME\"]:\n",
    "        if bad in ethi_pois.columns:\n",
    "            ethi_pois = ethi_pois.drop(columns=bad)\n",
    "    out_eth = folders[\"poi\"] / \"ethiopia_pois.gpkg\"\n",
    "    ethi_pois.to_file(out_eth, driver=\"GPKG\")\n",
    "    print(f\"Saved Ethiopia POIs to {out_eth}\")\n",
    "else:\n",
    "    print(\"No Ethiopia POIs fetched.\")\n",
    "\n",
    "# 5) Baghdad – single call\n",
    "print(\"Fetching POIs for Baghdad …\")\n",
    "bag_poly = boundaries.get(\"baghdad\") or ox.geocode_to_gdf(\"Baghdad, Iraq\").geometry.iloc[0]\n",
    "\n",
    "try:\n",
    "    bag_pois = ox.features_from_polygon(bag_poly, tags=poi_tags)\n",
    "    bag_pois[\"geometry\"] = bag_pois.geometry.apply(\n",
    "        lambda g: g if isinstance(g, Point) else g.centroid\n",
    "    )\n",
    "    bag_pois[\"region_name\"] = \"Baghdad\"\n",
    "    # drop duplicate columns\n",
    "    bag_pois = bag_pois.loc[:, ~bag_pois.columns.duplicated()]\n",
    "    # drop any fixme column\n",
    "    if \"fixme\" in bag_pois.columns:\n",
    "        bag_pois = bag_pois.drop(columns=\"fixme\")\n",
    "    out_bag = folders[\"poi\"] / \"baghdad_pois.gpkg\"\n",
    "    bag_pois.to_file(out_bag, driver=\"GPKG\")\n",
    "    print(f\"Saved Baghdad POIs to {out_bag}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch Baghdad POIs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy-grid components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Disable SSL verification & warnings, force HTTP endpoints\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "ox.settings.requests_kwargs    = {\"verify\": False}\n",
    "ox.settings.nominatim_endpoint = \"http://nominatim.openstreetmap.org/search\"\n",
    "ox.settings.overpass_endpoint  = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# 2) Ensure energy folder exists\n",
    "folders[\"energy\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) Define tags for grid components\n",
    "grid_tags = {\n",
    "    \"power\": [\"line\", \"substation\", \"transformer\", \"tower\"]\n",
    "}\n",
    "\n",
    "# 4) Ethiopia – loop per admin_level=4 subregion\n",
    "subregs   = gpd.read_file(folders[\"boundaries\"] / \"ethiopia_subregions.gpkg\")\n",
    "eth_parts = []\n",
    "\n",
    "for _, row in subregs.iterrows():\n",
    "    region   = row[\"region_name\"]\n",
    "    poly     = row.geometry\n",
    "    print(f\"Fetching energy‐grid for Ethiopia – {region} …\")\n",
    "    try:\n",
    "        gdf = ox.features_from_polygon(poly, tags=grid_tags)\n",
    "        if gdf.empty:\n",
    "            continue\n",
    "        # tag the region\n",
    "        gdf[\"region_name\"] = region\n",
    "        # drop any duplicate columns\n",
    "        gdf = gdf.loc[:, ~gdf.columns.duplicated()]\n",
    "        # drop problematic 'fixme' field if present\n",
    "        for bad in [\"fixme\", \"FIXME\"]:\n",
    "            if bad in gdf.columns:\n",
    "                gdf = gdf.drop(columns=bad)\n",
    "        eth_parts.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipped {region}: {e}\")\n",
    "\n",
    "# 5) Save Ethiopia grid\n",
    "if eth_parts:\n",
    "    eth_grid = pd.concat(eth_parts, ignore_index=True).set_crs(\"EPSG:4326\")\n",
    "    out_eth = folders[\"energy\"] / \"ethiopia_energy_grid.gpkg\"\n",
    "    eth_grid.to_file(out_eth, driver=\"GPKG\")\n",
    "    print(f\"Saved Ethiopia energy grid to {out_eth}\")\n",
    "else:\n",
    "    print(\"No Ethiopia energy‐grid features fetched.\")\n",
    "\n",
    "# 6) Baghdad – single call\n",
    "print(\"Fetching energy‐grid for Baghdad …\")\n",
    "bag_poly = boundaries.get(\"baghdad\") or ox.geocode_to_gdf(\"Baghdad, Iraq\").geometry.iloc[0]\n",
    "\n",
    "try:\n",
    "    bag_gdf = ox.features_from_polygon(bag_poly, tags=grid_tags)\n",
    "    bag_gdf[\"region_name\"] = \"Baghdad\"\n",
    "    bag_gdf = bag_gdf.loc[:, ~bag_gdf.columns.duplicated()]\n",
    "    if \"fixme\" in bag_gdf.columns:\n",
    "        bag_gdf = bag_gdf.drop(columns=\"fixme\")\n",
    "    out_bag = folders[\"energy\"] / \"baghdad_energy_grid.gpkg\"\n",
    "    bag_gdf.to_file(out_bag, driver=\"GPKG\")\n",
    "    print(f\"Saved Baghdad energy grid to {out_bag}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch Baghdad energy‐grid: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM Power Plants Coal and Gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Downloading OSM power assets for ISO 'ET' …\n",
      "✅ ISO 'ET': 5 fossil‐fuel assets saved → 'ethiopia_power_fossil.gpkg'\n",
      "\n",
      "⏳ Downloading OSM power assets for ISO 'IQ' …\n",
      "✅ ISO 'IQ': 172 fossil‐fuel assets saved → 'iraq_power_fossil.gpkg'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib3\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Suppress SSL warnings (we are intentionally bypassing certificate verification)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Constants and output directory configuration\n",
    "OVERPASS_URL = \"https://overpass-api.de/api/interpreter\"\n",
    "OUT_DIR = Path(r\"C:\\Users\\Luis.ParraMorales\\OneDrive - Imperial College London\\Group Design Project\\Data\\energy\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Compile a regex pattern to identify any fossil fuel mention in relevant tags\n",
    "FOSSIL_PATTERN = re.compile(r\"\\b(coal|gas|oil|diesel|natural\\s?gas|hfo|petroleum)\\b\", re.IGNORECASE)\n",
    "\n",
    "# The Overpass‐QL template, using ISO3166-1 alpha-2 code to fetch the country’s admin boundary\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "[out:json][timeout:300];\n",
    "area[\"ISO3166-1\"=\"{iso2}\"][\"admin_level\"=\"2\"]->.country;\n",
    "(\n",
    "  nwr[\"power\"=\"plant\"](area.country);\n",
    "  nwr[\"power\"=\"generator\"](area.country);\n",
    ");\n",
    "out geom qt;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_geom(element: dict):\n",
    "    \"\"\"\n",
    "    Convert an Overpass element (node/way/relation) into a Shapely geometry.\n",
    "    Returns:\n",
    "      - Point for node records\n",
    "      - LineString or Polygon for way records, depending on closure\n",
    "      - Unary union of Polygons for relation records composed of closed ways\n",
    "      - None if geometry cannot be constructed\n",
    "    \"\"\"\n",
    "    elem_type = element.get(\"type\")\n",
    "\n",
    "    # --- Node: simple point geometry ---\n",
    "    if elem_type == \"node\":\n",
    "        return Point(element[\"lon\"], element[\"lat\"])\n",
    "\n",
    "    # For ways and relations, ensure a 'geometry' array exists\n",
    "    if \"geometry\" not in element:\n",
    "        return None\n",
    "    coords = [(pt[\"lon\"], pt[\"lat\"]) for pt in element[\"geometry\"]]\n",
    "\n",
    "    # --- Way: decide between LineString vs. closed Polygon ---\n",
    "    if elem_type == \"way\":\n",
    "        if len(coords) < 2:\n",
    "            return None\n",
    "        # If the first and last coordinates match, and there are at least 4 points → Polygon\n",
    "        if coords[0] == coords[-1] and len(coords) >= 4:\n",
    "            return Polygon(coords)\n",
    "        # Otherwise, interpret as a LineString\n",
    "        return LineString(coords)\n",
    "\n",
    "    # --- Relation: union of member polygons ---\n",
    "    if elem_type == \"relation\":\n",
    "        member_polygons = []\n",
    "        for member in element.get(\"members\", []):\n",
    "            if member.get(\"type\") == \"way\" and \"geometry\" in member:\n",
    "                member_coords = [(pt[\"lon\"], pt[\"lat\"]) for pt in member[\"geometry\"]]\n",
    "                if len(member_coords) >= 4 and member_coords[0] == member_coords[-1]:\n",
    "                    member_polygons.append(Polygon(member_coords))\n",
    "        if member_polygons:\n",
    "            try:\n",
    "                return unary_union(member_polygons)\n",
    "            except Exception:\n",
    "                return member_polygons[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_osm_power_assets(iso2: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Query Overpass for all 'power=plant' and 'power=generator' elements\n",
    "    within the specified country's admin boundary (ISO3166-1 alpha-2).\n",
    "    Returns a GeoDataFrame containing all elements’ tags and geometries.\n",
    "    \"\"\"\n",
    "    # Build the Overpass query\n",
    "    query = QUERY_TEMPLATE.format(iso2=iso2)\n",
    "    response = requests.post(OVERPASS_URL, data={\"data\": query}, verify=False)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    elements = response.json().get(\"elements\", [])\n",
    "    rows = []\n",
    "\n",
    "    for elem in elements:\n",
    "        geom = build_geom(elem)\n",
    "        if geom is None:\n",
    "            continue  # Skip elements with no valid geometry\n",
    "        tags = elem.get(\"tags\", {})\n",
    "        # Merge all tag key-value pairs with the geometry into a single row\n",
    "        row = {**tags, \"geometry\": geom}\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        # Return an empty GeoDataFrame if nothing was fetched\n",
    "        return gpd.GeoDataFrame([], geometry=[], crs=\"EPSG:4326\")\n",
    "\n",
    "    return gpd.GeoDataFrame(rows, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "def fossil_only(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Filter a GeoDataFrame of power assets to retain only those\n",
    "    whose relevant tag values match the fossil fuel pattern.\n",
    "    Checks 'plant:source', 'generator:source', 'generator:primary_fuel', and 'fuel'.\n",
    "    \"\"\"\n",
    "    def is_fossil(row):\n",
    "        for key in (\"plant:source\", \"generator:source\", \"generator:primary_fuel\", \"fuel\"):\n",
    "            value = str(row.get(key, \"\"))\n",
    "            if FOSSIL_PATTERN.search(value):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    mask = gdf.apply(is_fossil, axis=1)\n",
    "    return gdf[mask].copy()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Main loop: fetch, filter, and save for each country\n",
    "for iso2_code, output_filename in [\n",
    "    (\"ET\", \"ethiopia_power_fossil.gpkg\"),\n",
    "    (\"IQ\", \"iraq_power_fossil.gpkg\")\n",
    "]:\n",
    "    print(f\"⏳ Downloading OSM power assets for ISO '{iso2_code}' …\")\n",
    "    raw_gdf = fetch_osm_power_assets(iso2_code)\n",
    "    fossil_gdf = fossil_only(raw_gdf)\n",
    "\n",
    "    output_path = OUT_DIR / output_filename\n",
    "    fossil_gdf.to_file(output_path, driver=\"GPKG\")\n",
    "\n",
    "    print(f\"✅ ISO '{iso2_code}': {len(fossil_gdf)} fossil‐fuel assets saved → '{output_filename}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mesh File to CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change gpkg format to csv format. \n",
    "\n",
    "Addis Ababa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Addis Ababa\n",
    "file_path = DATA_PATH / 'addis-mesh-data'\n",
    "output_path = DATA_PATH / 'addis-mesh-data-csv'\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "files = list(file_path.glob(\"*.gpkg\"))\n",
    "\n",
    "for file in files:\n",
    "    name = file.stem\n",
    "    print(name)\n",
    "    gdf = gpd.read_file(file)\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda geom: geom.wkt if geom else None)\n",
    "    gdf.to_csv(output_path / f\"{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baghdad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = DATA_PATH / 'baghdad-mesh-data'\n",
    "output_path = DATA_PATH / 'baghdad-mesh-data-csv'\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "files = list(file_path.glob(\"*.gpkg\"))\n",
    "# files\n",
    "\n",
    "for file in files:\n",
    "    name = file.stem\n",
    "    print(name)\n",
    "    gdf = gpd.read_file(file)\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda geom: geom.wkt if geom else None)\n",
    "    gdf.to_csv(output_path / f\"{name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
