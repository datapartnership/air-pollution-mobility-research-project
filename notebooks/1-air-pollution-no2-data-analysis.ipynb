{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Pollution NO2 Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "### Overview\n",
    "This notebook conducts a comprehensive analysis of nitrogen dioxide (NO₂) pollution using Sentinel-5P data, with a focus on Ethiopia (Addis Ababa) and Iraq (Baghdad). It covers the full workflow, including data retrieval, preprocessing, aggregation, and visualisation.\n",
    "\n",
    "### Objective\n",
    "The aim is to assess spatial and temporal patterns in NO₂ levels as a proxy for air pollution and economic activity. \n",
    "\n",
    "### Workflow\n",
    "The notebook is structured into three main parts:\n",
    "1. **Data Download** – Retrieves NO₂ data from Google Earth Engine (Sentinel-5P).\n",
    "2. **Data Processing** – Fills missing values, clips to boundaries, and aggregates to mesh/grid.\n",
    "3. **Visualisation** – Produces spatial plots and animated GIFs for temporal dynamics.\n",
    "\n",
    "### Outputs\n",
    "- **Filled NO₂ Data**: Exported in TIFF format for spatial analyses.\n",
    "- **Aggregated NO₂ Values**: Saved in GeoParquet format by mesh for efficiency.\n",
    "- **Visual Animations**: NO₂ variation over time shown in GIF format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Prepare Packages and Path\n",
    "\n",
    "Cancel the comment to install all the packages and libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rasterio matplotlib rasterstats ipynbname imageio tqdm\n",
    "# ! pip install numpy==1.24.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Management\n",
    "\n",
    "Get current / repo / data path in local to make sure the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters",
     "init"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "curr_root = Path().resolve()    # current file path\n",
    "repo_root = curr_root.parent    # current repository path\n",
    "data_root = repo_root / \"data\"  # path for saving the data\n",
    "src_root = repo_root / \"src\"    # path for other sources\n",
    "sys.path.append(str(src_root))  # add src to system path to import custom functions\n",
    "\n",
    "# Import customised scripts\n",
    "from animation import*\n",
    "from aggregation import*\n",
    "\n",
    "# print(repo_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate meshes, from 2023-01-01 to 2024-12-31, one mesh for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Generating meshes for Addis Ababa!\n",
      "Complete Generating meshes for Baghdad!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import fiona\n",
    "\n",
    "mesh_addis = data_root / \"mesh-grid\" / \"grid_addis_ababa.gpkg\"\n",
    "mesh_baghdad = data_root / \"mesh-grid\" / \"grid_baghdad.gpkg\"\n",
    "\n",
    "lyr_addis_name = fiona.listlayers(mesh_addis)[0]  # control layer number\n",
    "lyr_baghdad_name = fiona.listlayers(mesh_baghdad)[0]\n",
    "\n",
    "# start and end date\n",
    "start_date = datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2024-12-31\", \"%Y-%m-%d\")\n",
    "\n",
    "addis_meshes_path = data_root / 'addis-mesh-data'\n",
    "baghdad_meshes_path = data_root / 'baghdad-mesh-data'\n",
    "\n",
    "addis_meshes_path.mkdir(exist_ok=True)\n",
    "baghdad_meshes_path.mkdir(exist_ok=True)\n",
    "\n",
    "delta = end_date - start_date\n",
    "days_count = delta.days + 1\n",
    "\n",
    "# For Addis Ababa\n",
    "for i in range(days_count):\n",
    "    current_date = start_date + timedelta(days=i)\n",
    "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"addis-ababa-{date_str}.gpkg\"\n",
    "    dest_path = addis_meshes_path / filename\n",
    "\n",
    "    shutil.copy(mesh_addis, dest_path)\n",
    "\n",
    "print(f\"Complete Generating meshes for Addis Ababa!\")\n",
    "\n",
    "# For Baghdad\n",
    "for i in range(days_count):\n",
    "    current_date = start_date + timedelta(days=i)\n",
    "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"baghdad-{date_str}.gpkg\"\n",
    "    dest_path = baghdad_meshes_path / filename\n",
    "\n",
    "    shutil.copy(mesh_baghdad, dest_path)\n",
    "\n",
    "\n",
    "print(f\"Complete Generating meshes for Baghdad!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_addis = data_root / \"mesh-grid\" / \"grid_addis_ababa.gpkg\"\n",
    "mesh_baghdad = data_root / \"mesh-grid\" / \"grid_baghdad.gpkg\"\n",
    "\n",
    "lyr_addis_name = fiona.listlayers(mesh_addis) # control layer number\n",
    "lyr_baghdad_name = fiona.listlayers(mesh_baghdad)\n",
    "\n",
    "# rd_name = fiona.listlayers(data_root / 'addis-mesh-data' / 'addis-ababa-2023-02-25.gpkg') # control layer number\n",
    "# rd1 = fiona.listlayers(data_root / 'baghdad-mesh-data' / 'baghdad-2023-02-25.gpkg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid_addis_ababa']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyr_addis_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid_badhdad']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyr_baghdad_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid_addis_ababa']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid_badhdad']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Download Data\n",
    "\n",
    "In this chapter, NO2 pollution data from [Google Earth Engine Sentinel 5P](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_NRTI_L3_NO2) is downloaded, for both Ethiopia and Iraq in country level.\n",
    "\n",
    "From related literature, we chose **tropospheric_NO2_column_number_density** as the proxy for NO2 concentration level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function to generate desired time period of NOx data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import ee\n",
    "ee.Authenticate() # For the first Initialization, individual API is needed to log into Google Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Function: generate desired time period of NO2 data  \n",
    "def specific_date(start_date: str, end_date: str, time_resolution: str = 'D') -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of dates within specified time period and resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date: str\n",
    "        Start date, format: 'YYYY-MM-DD'.\n",
    "    - end_date: str\n",
    "        End date, format: 'YYYY-MM-DD'.\n",
    "    - time_resolution: str\n",
    "        Time resolution (e.g., 'D' for daily, 'W' for weekly, 'M' for monthly). Default is 'D'.\n",
    "    \n",
    "    Return:\n",
    "    - dates(list): List of date strings marking the ends of each time segment, format: 'YYYY-MM-DD'.\n",
    "    \n",
    "    \"\"\"\n",
    "    dates = (\n",
    "        pd.date_range(start_date, end_date, freq = time_resolution)\n",
    "        .strftime('%Y-%m-%d')\n",
    "        .tolist()\n",
    "    )\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request tasks to download in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download NO2 data\n",
    "def download_no2_country(country_name: str, dates: list):\n",
    "    \"\"\"\n",
    "    Request NO2 data download from Earth Engine for a specified country and time period\n",
    "\n",
    "    Parameters:\n",
    "    - country_name: str\n",
    "        Name of the target country. Must match the format used by Earth Engine.\n",
    "    - dates: list\n",
    "        List containing the desired time range, (e.g., [start_date, end_date]).\n",
    "\n",
    "    Return:\n",
    "    - None. Sends a/multiple request(s) to Earth Engine to initiate data download.\n",
    "        Exported files are saved under a folder named 'NO2_<country_name>' in first-level Google Drive directory.\n",
    "        Each exported .tiff file is named using its starting date.\n",
    "    \"\"\"\n",
    "    \n",
    "    countries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n",
    "    country = countries.filter(ee.Filter.eq('country_na', country_name)).geometry()\n",
    "\n",
    "    n_dates = len(dates)\n",
    "\n",
    "    for i in range(n_dates-1):\n",
    "\n",
    "        date_start, date_end = dates[i], dates[i+1]\n",
    "\n",
    "        no2 = (ee.ImageCollection('COPERNICUS/S5P/NRTI/L3_NO2')\n",
    "            .select('tropospheric_NO2_column_number_density')\n",
    "            .filterDate(date_start, date_end)\n",
    "            .mean())\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=no2,\n",
    "            description=f'{country_name}_NO2_{date_start}_{date_end}',\n",
    "            folder=f'NO2_{country_name}',\n",
    "            fileNamePrefix=f'{country_name}_NO2_{date_start}',\n",
    "            region=country,\n",
    "            scale=1000,\n",
    "            maxPixels=1e13\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            task.start()\n",
    "            print(f'{country_name}: The export task for {date_start} is ongoing, please check the results in Google Drive.')\n",
    "        except Exception as e:\n",
    "            print(f'Fail to sumbit task: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Call and Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = specific_date('2023-01-01', '2025-01-01')\n",
    "len(dates) # 731\n",
    "\n",
    "# Download Ethiopia Data\n",
    "download_no2_country('Ethiopia', dates)\n",
    "\n",
    "# Download Iraq Data\n",
    "download_no2_country('Iraq', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Process Pipeline\n",
    "\n",
    "This chapter processes the NO2 data downloaded in Chapter 1 through the following steps:\n",
    "\n",
    "- **(1) Filling Missing Value**: Spot the missing values in raster and replenish them using iterative checking, using **mean** of the neighbour raster as the replenish value.\n",
    "\n",
    "- **(2) Clipping to Region**: Clipping the data to the interested area, and output the filled raster.\n",
    "\n",
    "- **(3) Aggregation**: Import the generated mesh and aggregate the raster to the mesh level.\n",
    "\n",
    "Step 2 and 3 are realised by selecting and aggregating the data within the (synthesised) mesh grid. \n",
    "\n",
    "Output at the end of the process:\n",
    "\n",
    "- The processed data will be exported in format of GeoParquet (*.gpq*), which is an open, efficient and modern file format designed for storing geospatial vector data.\n",
    "\n",
    "*Note: Currently since the official mesh for the two regions (Baghdad and Addis Ababa) are not provided, so we synthesised the mesh to establish the workflow.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fill Missing Data\n",
    "\n",
    "In this chapter, missing data in each raster is filled using the neighbour data, and the filled raster is saved in a new seperate folder: *Ethiopia_NO2_filled* and *Iraq_NO2_filled*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Define Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to read and iteratively fill missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function: read tiff files\n",
    "def read_tiff(filename):\n",
    "    with rasterio.open(filename) as src:\n",
    "        band = src.read(1)          # first band\n",
    "        profile = src.profile       # meta data\n",
    "        nodata_value = src.nodata   # get nodata (missing)\n",
    "\n",
    "    return src, band, profile, nodata_value\n",
    "\n",
    "\n",
    "# iterative missing data interpolate\n",
    "from scipy.ndimage import generic_filter\n",
    "\n",
    "# Function: using neighbour average mean as interpolation value\n",
    "def fill_nan_with_mean(arr):\n",
    "    center = arr[len(arr) // 2]\n",
    "    if np.isnan(center):\n",
    "        mean = np.nanmean(arr)\n",
    "        return mean if not np.isnan(mean) else np.nan\n",
    "    return center\n",
    "\n",
    "# Function: iteratively interpolate missing values in single tiff file\n",
    "def iterative_fill(data, max_iter=10, window_size=9):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    max_iteration: control the max iteration value\n",
    "    window_size: the smoothing window of the moving average, set as odd number to avoid raster shifting\n",
    "    \n",
    "    \"\"\"\n",
    "    filled = data.copy()\n",
    "    for i in range(max_iter):\n",
    "        prev_nan = np.isnan(filled).sum()\n",
    "        filled = generic_filter(filled, function=fill_nan_with_mean, size=window_size, mode='nearest')\n",
    "        new_nan = np.isnan(filled).sum()\n",
    "        # print(f\"Iteration {i+1}: remaining NaNs = {new_nan}\")\n",
    "        if new_nan == 0 or new_nan == prev_nan:\n",
    "            break\n",
    "    return filled\n",
    "\n",
    "# Function: fill missing values in all the tiff files under same path\n",
    "def fill_missing_data(country, data_tiff_path):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    country: str\n",
    "        - Name of the country, initial should be uppercase, such as 'Iraq'.\n",
    "\n",
    "    data_tiff_path:\n",
    "        - Path of the tiff files to be processed.\n",
    "\n",
    "    Output:\n",
    "    --------\n",
    "    Return tiff files under a new created folder in data/countryname_NO2_filled\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Get the paths of the to be processed tiff files\n",
    "    tiffs = [f for f in os.listdir(data_tiff_path) if f.lower().endswith('.tif')]\n",
    "    abs_tiff_paths = [os.path.join(data_tiff_path, f) for f in tiffs]  # abosolute path\n",
    "    n_task = len(tiffs)\n",
    "\n",
    "    # Create a folder to save filled data\n",
    "    output_dir = data_root / f'{country}-no2-filled'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # Loop to process each tiff file\n",
    "    for index, tiff_path in enumerate(abs_tiff_paths):\n",
    "\n",
    "        # Get the tiff date from the file name\n",
    "        date = tiffs[index].split('_')[2].split('.')[0]\n",
    "        \n",
    "        # Trace progress\n",
    "        print(f\"currently working on: {index+1}/{n_task}, {date}\")\n",
    "\n",
    "        # Detect non-valid data\n",
    "        file_size_mb = os.path.getsize(tiff_path) / (1024 * 1024)\n",
    "        if file_size_mb < 1:\n",
    "            print(f\"File size {file_size_mb:.2f}KB < 1MB, skipping {date} file.\")\n",
    "            continue \n",
    "\n",
    "        # Read raster data\n",
    "        src, band, profile, nodata_value = read_tiff(tiff_path)\n",
    "        if nodata_value is not None:    # replace no_data as np.nan\n",
    "            band = np.where(band == nodata_value, np.nan, band)\n",
    "\n",
    "        # Missing Value replacement\n",
    "        band_filled = iterative_fill(band, max_iter=10, window_size=9)\n",
    "\n",
    "        # Save filled data\n",
    "        output_file = output_dir / f'{country}_NO2_{date}_filled.tif'\n",
    "\n",
    "        with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "            filled_band = np.where(np.isnan(band_filled), nodata_value, band_filled) # replace np.nan with nodata\n",
    "            dst.write(filled_band.astype(profile['dtype']), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fill Missing Data in Ethiopia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took over 8 hours to run the following cell. So after processed the following cell, comment it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_tiff_path = data_root / 'Ethiopia_NO2'\n",
    "fill_missing_data('Ethiopia', eth_tiff_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fill Missing Data in Iraq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processed the following cell, comment it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iraq_tiff_path = data_root / 'Iraq_NO2'\n",
    "# fill_missing_data('Iraq', iraq_tiff_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Demonstrate Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *Ethiopia_NO2_2018-07-12.tif* file as an exmaple to show what this missing data process loop do in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get demo working directory\n",
    "demo_path = data_root / \"demo-data\"\n",
    "\n",
    "# original image\n",
    "src, band, profile, nodata_value = read_tiff(demo_path / 'Ethiopia_NO2_2018-07-12.tif')\n",
    "\n",
    "plt.imshow(band, cmap='gray') # 'gray'\n",
    "plt.title(\"Original Image\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled image\n",
    "band_filled = iterative_fill(band, max_iter=10, window_size=9)\n",
    "\n",
    "plt.imshow(band_filled, cmap='gray')\n",
    "plt.title(\"Filled Image\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aggregate Based on Mesh Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aggregation import*\n",
    "\n",
    "addis_meshes_path = data_root / 'addis-mesh-data'\n",
    "baghdad_meshes_path = data_root / 'baghdad-mesh-data'\n",
    "\n",
    "mesh_addis = data_root / \"mesh-grid\" / \"grid_addis_ababa.gpkg\"\n",
    "mesh_baghdad = data_root / \"mesh-grid\" / \"grid_baghdad.gpkg\"\n",
    "\n",
    "lyr_addis_name = fiona.listlayers(mesh_addis)[0] # control layer number\n",
    "lyr_baghdad_name = fiona.listlayers(mesh_baghdad)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Aggregate Ethiopia - Addis Ababa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently working on: 1/711, 2023-01-01\n",
      "currently working on: 2/711, 2023-01-02\n",
      "currently working on: 3/711, 2023-01-03\n",
      "currently working on: 4/711, 2023-01-04\n",
      "currently working on: 5/711, 2023-01-05\n",
      "currently working on: 6/711, 2023-01-06\n",
      "currently working on: 7/711, 2023-01-07\n",
      "currently working on: 8/711, 2023-01-08\n",
      "currently working on: 9/711, 2023-01-09\n",
      "currently working on: 10/711, 2023-01-10\n",
      "currently working on: 11/711, 2023-01-11\n",
      "currently working on: 12/711, 2023-01-12\n",
      "currently working on: 13/711, 2023-01-13\n",
      "currently working on: 14/711, 2023-01-14\n",
      "currently working on: 15/711, 2023-01-15\n",
      "currently working on: 16/711, 2023-01-16\n",
      "currently working on: 17/711, 2023-01-17\n",
      "currently working on: 18/711, 2023-01-18\n",
      "currently working on: 19/711, 2023-01-19\n",
      "currently working on: 20/711, 2023-01-20\n",
      "currently working on: 21/711, 2023-01-21\n",
      "currently working on: 22/711, 2023-01-22\n",
      "currently working on: 23/711, 2023-01-23\n",
      "currently working on: 24/711, 2023-01-24\n",
      "currently working on: 25/711, 2023-01-25\n",
      "currently working on: 26/711, 2023-01-26\n",
      "currently working on: 27/711, 2023-01-27\n",
      "currently working on: 28/711, 2023-01-28\n",
      "currently working on: 29/711, 2023-01-29\n",
      "currently working on: 30/711, 2023-01-30\n",
      "currently working on: 31/711, 2023-01-31\n",
      "currently working on: 32/711, 2023-02-01\n",
      "currently working on: 33/711, 2023-02-02\n",
      "currently working on: 34/711, 2023-02-03\n",
      "currently working on: 35/711, 2023-02-04\n",
      "currently working on: 36/711, 2023-02-05\n",
      "currently working on: 37/711, 2023-02-06\n",
      "currently working on: 38/711, 2023-02-07\n",
      "currently working on: 39/711, 2023-02-08\n",
      "currently working on: 40/711, 2023-02-09\n",
      "currently working on: 41/711, 2023-02-10\n",
      "currently working on: 42/711, 2023-02-11\n",
      "currently working on: 43/711, 2023-02-12\n",
      "currently working on: 44/711, 2023-02-13\n",
      "currently working on: 45/711, 2023-02-14\n",
      "currently working on: 46/711, 2023-02-15\n",
      "currently working on: 47/711, 2023-02-16\n",
      "currently working on: 48/711, 2023-02-17\n",
      "currently working on: 49/711, 2023-02-18\n",
      "currently working on: 50/711, 2023-02-19\n",
      "currently working on: 51/711, 2023-02-20\n",
      "currently working on: 52/711, 2023-02-21\n",
      "currently working on: 53/711, 2023-02-22\n",
      "currently working on: 54/711, 2023-02-23\n",
      "currently working on: 55/711, 2023-02-24\n",
      "currently working on: 56/711, 2023-02-25\n",
      "currently working on: 57/711, 2023-02-26\n",
      "currently working on: 58/711, 2023-02-27\n",
      "currently working on: 59/711, 2023-02-28\n",
      "currently working on: 60/711, 2023-03-01\n",
      "currently working on: 61/711, 2023-03-02\n",
      "currently working on: 62/711, 2023-03-03\n",
      "currently working on: 63/711, 2023-03-04\n",
      "currently working on: 64/711, 2023-03-05\n",
      "currently working on: 65/711, 2023-03-06\n",
      "currently working on: 66/711, 2023-03-07\n",
      "currently working on: 67/711, 2023-03-08\n",
      "currently working on: 68/711, 2023-03-09\n",
      "currently working on: 69/711, 2023-03-10\n",
      "currently working on: 70/711, 2023-03-11\n",
      "currently working on: 71/711, 2023-03-12\n",
      "currently working on: 72/711, 2023-03-13\n",
      "currently working on: 73/711, 2023-03-14\n",
      "currently working on: 74/711, 2023-03-15\n",
      "currently working on: 75/711, 2023-03-16\n",
      "currently working on: 76/711, 2023-03-17\n",
      "currently working on: 77/711, 2023-03-19\n",
      "currently working on: 78/711, 2023-03-20\n",
      "currently working on: 79/711, 2023-03-21\n",
      "currently working on: 80/711, 2023-03-22\n",
      "currently working on: 81/711, 2023-03-23\n",
      "currently working on: 82/711, 2023-03-24\n",
      "currently working on: 83/711, 2023-03-25\n",
      "currently working on: 84/711, 2023-03-26\n",
      "currently working on: 85/711, 2023-03-27\n",
      "currently working on: 86/711, 2023-03-28\n",
      "currently working on: 87/711, 2023-03-29\n",
      "currently working on: 88/711, 2023-03-30\n",
      "currently working on: 89/711, 2023-03-31\n",
      "currently working on: 90/711, 2023-04-01\n",
      "currently working on: 91/711, 2023-04-02\n",
      "currently working on: 92/711, 2023-04-03\n",
      "currently working on: 93/711, 2023-04-04\n",
      "currently working on: 94/711, 2023-04-05\n",
      "currently working on: 95/711, 2023-04-06\n",
      "currently working on: 96/711, 2023-04-07\n",
      "currently working on: 97/711, 2023-04-08\n",
      "currently working on: 98/711, 2023-04-09\n",
      "currently working on: 99/711, 2023-04-10\n",
      "currently working on: 100/711, 2023-04-11\n",
      "currently working on: 101/711, 2023-04-12\n",
      "currently working on: 102/711, 2023-04-13\n",
      "currently working on: 103/711, 2023-04-14\n",
      "currently working on: 104/711, 2023-04-15\n",
      "currently working on: 105/711, 2023-04-16\n",
      "currently working on: 106/711, 2023-04-17\n",
      "currently working on: 107/711, 2023-04-18\n",
      "currently working on: 108/711, 2023-04-19\n",
      "currently working on: 109/711, 2023-04-20\n",
      "currently working on: 110/711, 2023-04-21\n",
      "currently working on: 111/711, 2023-04-22\n",
      "currently working on: 112/711, 2023-04-23\n",
      "currently working on: 113/711, 2023-04-24\n",
      "currently working on: 114/711, 2023-04-25\n",
      "currently working on: 115/711, 2023-04-26\n",
      "currently working on: 116/711, 2023-04-27\n",
      "currently working on: 117/711, 2023-04-28\n",
      "currently working on: 118/711, 2023-04-29\n",
      "currently working on: 119/711, 2023-04-30\n",
      "currently working on: 120/711, 2023-05-01\n",
      "currently working on: 121/711, 2023-05-02\n",
      "currently working on: 122/711, 2023-05-03\n",
      "currently working on: 123/711, 2023-05-04\n",
      "currently working on: 124/711, 2023-05-05\n",
      "currently working on: 125/711, 2023-05-06\n",
      "currently working on: 126/711, 2023-05-07\n",
      "currently working on: 127/711, 2023-05-08\n",
      "currently working on: 128/711, 2023-05-09\n",
      "currently working on: 129/711, 2023-05-10\n",
      "currently working on: 130/711, 2023-05-11\n",
      "currently working on: 131/711, 2023-05-12\n",
      "currently working on: 132/711, 2023-05-13\n",
      "currently working on: 133/711, 2023-05-14\n",
      "currently working on: 134/711, 2023-05-15\n",
      "currently working on: 135/711, 2023-05-16\n",
      "currently working on: 136/711, 2023-05-17\n",
      "currently working on: 137/711, 2023-05-18\n",
      "currently working on: 138/711, 2023-05-19\n",
      "currently working on: 139/711, 2023-05-20\n",
      "currently working on: 140/711, 2023-05-21\n"
     ]
    }
   ],
   "source": [
    "# Aggregate Ethiopia - Addis Ababa\n",
    "eth_no2_filled_path = data_root / 'eth-no2-filled'\n",
    "aggregate_data(\n",
    "    data_tiff_path=eth_no2_filled_path, \n",
    "    mesh_path=addis_meshes_path, \n",
    "    layer_name=lyr_addis_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrate Purpose\n",
    "Show aggregated result in 2023-01-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Aggregate Iraq - Baghdad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Iraq - Baghdad\n",
    "iraq_no2_filled_path = data_root / 'iraq-no2-filled'\n",
    "aggregate_data(\n",
    "    data_tiff_path=iraq_no2_filled_path, \n",
    "    mesh_path=baghdad_meshes_path, \n",
    "    layer_name=lyr_baghdad_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrate Purpose\n",
    "\n",
    "Show aggregated value in Addis Ababa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the mesh and file path\n",
    "demo_mesh = gpd.read_file(data_root / 'mesh-grid' / 'grid_addis_ababa.gpkg')\n",
    "raster_path = data_root / 'demo-data' / 'Ethiopia_NO2_2018-07-12_filled.tif'\n",
    "\n",
    "# using mean as representitive value\n",
    "stats = zonal_stats(demo_mesh, raster_path, stats=[\"mean\"], nodata=np.nan)  # other alternatives: \"std\", \"max\", \"min\", \"sum\"\n",
    "demo_mesh[\"mean\"] = [s[\"mean\"] for s in stats]\n",
    "\n",
    "# visual\n",
    "demo_mesh.plot(column=\"mean\", edgecolor=\"grey\", legend=True)\n",
    "plt.title(\"Addis Ababa 5km$^2$ Hexagon Aggregated NO2\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show aggregated value in Baghdad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the mesh and file path\n",
    "demo_mesh = gpd.read_file(data_root / 'mesh_grid' / 'grid_baghdad.gpkg')\n",
    "raster_path = data_root / 'demo-data' / 'Iraq_NO2_2018-07-12_filled.tif'\n",
    "\n",
    "# using mean as representitive value\n",
    "stats = zonal_stats(demo_mesh, raster_path, stats=[\"mean\"], nodata=np.nan)  # other alternatives: \"std\", \"max\", \"min\", \"sum\"\n",
    "demo_mesh[\"mean\"] = [s[\"mean\"] for s in stats]\n",
    "\n",
    "# visual\n",
    "demo_mesh.plot(column=\"mean\", edgecolor='gray', legend=True)\n",
    "plt.title(\"Addis Ababa 5km$^2$ Hexagon Aggregated NO2\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Visualisation\n",
    "\n",
    "This chapter is used to generate a dynatmic figure, to show how the feature distribution change with time.\n",
    "\n",
    "Note:\n",
    "\n",
    "- In the coloration system, percentile clipping and contrast streching method is uesd to imporve the visual effects of the image.\n",
    "\n",
    "- In this chapter, the dynamic distribution of NO2 is generated, in format of GIF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages in src/animation.py\n",
    "from animation import tiff_2_gif, mesh_2_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) NO2 Distribution in Ethiopia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_eth_tif_dir= data_root / 'Ethiopia_NO2_filled'  \n",
    "# tiff_2_gif(no2_eth_tif_dir, output_path=data_root, output_name=\"ethiopia-no2-animation\", fps = 8)\n",
    "\n",
    "# total NO2 distribution animation\n",
    "total_no2_eth_tif_dir = data_root / 'eth-total-no2-filled'  \n",
    "tiff_2_gif(total_no2_eth_tif_dir, output_path=data_root, output_name=\"ethiopia-total-no2-animation\", fps = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) NO2 Distribution in Iraq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_iraq_tif_dir= data_root / 'iraq-no2-filled'  \n",
    "tiff_2_gif(no2_iraq_tif_dir, output_path=data_root, output_name=\"iraq-no2-animation\", fps = 8)\n",
    "\n",
    "# total NO2 distribution animation\n",
    "total_no2_eth_tif_dir = data_root / 'iraq-total-no2-filled'  \n",
    "tiff_2_gif(total_no2_eth_tif_dir, output_path=data_root, output_name=\"iraq-total-no2-animation\", fps = 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
